<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://hemimorphite.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://hemimorphite.github.io/" rel="alternate" type="text/html" /><updated>2024-04-10T13:41:05+00:00</updated><id>https://hemimorphite.github.io/feed.xml</id><title type="html">Hemimorphite</title><subtitle>Gain a better understanding of mathematics, chemistry, physics, and computer science. Our tutorials provide clear explanations and practical examples to help you learn and master these subjects. Helping learners of all levels to grasp fundamental concepts and hone their skills.</subtitle><entry xml:lang="en"><title type="html">Deploy a Jekyll Site to Github Pages</title><link href="https://hemimorphite.github.io/2024/04/10/deploy-a-jekyll-site-to-github-pages-Copy/" rel="alternate" type="text/html" title="Deploy a Jekyll Site to Github Pages" /><published>2024-04-10T13:30:00+00:00</published><updated>2024-04-10T13:30:00+00:00</updated><id>https://hemimorphite.github.io/2024/04/10/deploy-a-jekyll-site-to-github-pages-%20Copy</id><content type="html" xml:base="https://hemimorphite.github.io/2024/04/10/deploy-a-jekyll-site-to-github-pages-Copy/"><![CDATA[<div class="blog-post">
    <h2 class="post-title">Deploy a Jekyll Site to Github Pages</h2>
<div class="post-author">
    <span class="avatar"></span>
    <span class="info"><span class="date">Published April 10, 2024</span><br><span class="name">By Samuel Yang</span></span>
</div>

<figure class="post-image">
    <img src="/assets/images/githubpages.png" alt="Blog Cover">
</figure>

    <article class="post-content">
        <p>Create a new repository on Github by clicking the new button</p>
        
        <figure class="post-figure">
            <img src="/assets/images/githubpages-01.png" alt="Create a new repository">
        </figure>

        <p>Your repository must be named <code>&lt;username&gt;.github.io</code> and must be public. Then click Create Repository button.</p>

        <figure class="post-figure">
            <img src="/assets/images/githubpages-02.png" alt="repository name">
        </figure>

        <p>Fork the <a href="https://github.com/daattali/beautiful-jekyll" target="_blank">Beautiful Jekyll repo</a>:</p>

        <figure class="post-figure">
            <img src="/assets/images/githubpages-03.png" alt="Fork repository">
        </figure>

        <p>Rename the repository name to <code>&lt;username&gt;.github.io</code> and click Create Fork button.</p>

        <figure class="post-figure">
            <img src="/assets/images/githubpages-04.png" alt="Create Fork">
        </figure>

        <p>Click on the repository settings tab.</p>

        <figure class="post-figure">
            <img src="/assets/images/githubpages-05.png" alt="Setting repository">
        </figure>

        <p>From the left-hand side menu, click Pages under the Code and Automation section.</p>

        <figure class="post-figure">
            <img src="/assets/images/githubpages-06.png" alt="Pages menu">
        </figure>

        <p>In the Build and Deployment section, choose the branch (i.e. <code>master</code>) from which you would like GitHub Pages to deploy your website and click save button.</p>

        <figure class="post-figure">
            <img src="/assets/images/githubpages-07.png" alt="Choose branch to publish Github Pages">
        </figure>

        <p>Edit the <code>_config.yml</code> file to change any settings you want. To edit the file, first click on it to view the file, and on the next page click on the pencil icon to edit it. After changing the settings, click the green Commit changes button to save these edits.</p>
        
        <figure class="post-figure">
            <img src="/assets/images/githubpages-08.png" alt="Edit the _config.yml">
        </figure>

        <figure class="post-figure">
            <img src="/assets/images/githubpages-09.png" alt="Edit button">
        </figure>

        <figure class="post-figure">
            <img src="/assets/images/githubpages-10.png" alt="Commit Changes">
        </figure>

        <figure class="post-figure">
            <img src="/assets/images/githubpages-11.png" alt="Commit Changes">
        </figure>

        <p>You can now view your site at https://chonkarexsaurus.github.io.</p>
    </article>

    <div class="post-tags">
	<div class="title">Tags</div>
	<ul class="tags">
		
		<li><a href="https://hemimorphite.github.io/tag/jekyll" class="tag">jekyll</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/github-pages" class="tag">github pages</a></li>
		
	</ul>
</div>


    <div class="post-share">
    <div class="title">Share this post</div>
    <ul class="rounded-social-buttons">
        <li><a href="https://www.facebook.com/sharer/sharer.php?u=https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/" class="social-button facebook"><i class="fab fa-facebook-f"></i></a></li>
        <li><a href="http://twitter.com/share?text=Hey+guys%2c+check+this+out!&amp;url=https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/&amp;hashtags=qemu,linux,fedora 38,firewalld,DHCP" class="social-button twitter"><i class="fab fa-twitter"></i></a></li>
        <li><a href="https://www.linkedin.com/sharing/share-offsite/?url=https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/" class="social-button linkedin"><i class="fab fa-linkedin"></i></a></li>
    </ul>
</div>
</div>]]></content><author><name>Samuel Yang</name></author><category term="Tutorials" /><category term="year-2024" /><category term="month-04" /><category term="day-10" /><category term="jekyll" /><category term="github pages" /><summary type="html"><![CDATA[Learn how to deploy your Jekyll site to GitHub Pages with our step-by-step guide. Easy instructions for hosting your site on GitHub Pages.]]></summary></entry><entry xml:lang="en"><title type="html">Install Jekyll on Ubuntu 22.04</title><link href="https://hemimorphite.github.io/2024/04/09/install-jekyll-on-ubuntu-22.04/" rel="alternate" type="text/html" title="Install Jekyll on Ubuntu 22.04" /><published>2024-04-09T15:10:00+00:00</published><updated>2024-04-09T15:10:00+00:00</updated><id>https://hemimorphite.github.io/2024/04/09/install-jekyll-on-ubuntu-22.04</id><content type="html" xml:base="https://hemimorphite.github.io/2024/04/09/install-jekyll-on-ubuntu-22.04/"><![CDATA[<div class="blog-post">
    <h2 class="post-title">Install Jekyll on Ubuntu 22.04</h2>
<div class="post-author">
    <span class="avatar"></span>
    <span class="info"><span class="date">Published April 09, 2024</span><br><span class="name">By Samuel Yang</span></span>
</div>

<figure class="post-image">
    <img src="/assets/images/jekyll.jpg" alt="Blog Cover">
</figure>

    <article class="post-content">
        <p>Install Ruby:</p>
        
        <pre><code class="language-bash hljs">sudo apt-get install -y build-essential ruby-dev git</code></pre>        

        <p>Avoid installing RubyGems packages (called gems) as the root user. Set up a gem installation directory for your user account. The following commands will add environment variables to your <code>~/.bashrc</code> file to configure the gem installation path:</p>

        <pre><code class="language-bash hljs">echo 'export GEM_HOME="$HOME/gems"' >> ~/.bashrc
echo 'export PATH="$HOME/gems/bin:$PATH"' >> ~/.bashrc
source ~/.bashrc</code></pre>

        <p>Install Jekyll and Bundler:</p>

        <pre><code class="language-bash hljs">gem install jekyll bundler</code></pre>

        <p>Clone the <a href="https://github.com/daattali/beautiful-jekyll" target="_blank">Beautiful Jekyll repo</a>:</p>

        <pre><code class="language-bash hljs">git clone https://github.com/daattali/beautiful-jekyll.git</code></pre>

        <p>Navigate to your blog directory and install all dependencies:</p>

        <pre><code class="language-bash hljs">cd beautiful-jekyll
bundle install</code></pre>

        <p>Run your site locally:</p>

        <pre><code class="language-bash hljs">bundle exec jekyll serve</code></pre>
        
        <figure class="post-figure">
            <img src="/assets/images/jekyll-01.png" alt="Run Jekyll">
        </figure>

        <p>You can now view your site at http://localhost:4000.</p>        
    </article>

    <div class="post-tags">
	<div class="title">Tags</div>
	<ul class="tags">
		
		<li><a href="https://hemimorphite.github.io/tag/linux" class="tag">linux</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/ubuntu-22-04" class="tag">ubuntu 22.04</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/jekyll" class="tag">jekyll</a></li>
		
	</ul>
</div>


    <div class="post-share">
    <div class="title">Share this post</div>
    <ul class="rounded-social-buttons">
        <li><a href="https://www.facebook.com/sharer/sharer.php?u=https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/" class="social-button facebook"><i class="fab fa-facebook-f"></i></a></li>
        <li><a href="http://twitter.com/share?text=Hey+guys%2c+check+this+out!&amp;url=https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/&amp;hashtags=qemu,linux,fedora 38,firewalld,DHCP" class="social-button twitter"><i class="fab fa-twitter"></i></a></li>
        <li><a href="https://www.linkedin.com/sharing/share-offsite/?url=https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/" class="social-button linkedin"><i class="fab fa-linkedin"></i></a></li>
    </ul>
</div>
</div>]]></content><author><name>Samuel Yang</name></author><category term="Tutorials" /><category term="year-2024" /><category term="month-04" /><category term="day-09" /><category term="linux" /><category term="ubuntu 22.04" /><category term="jekyll" /><summary type="html"><![CDATA[Learn how to install Jekyll on Ubuntu 22.04 with step-by-step instructions in this comprehensive guide. Explore the process of setting up Jekyll to build static websites on your Ubuntu system.]]></summary></entry><entry xml:lang="en"><title type="html">Setup Fedora Server 38 as a DNS Server with Qemu/KVM, Part 3</title><link href="https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dns-server/" rel="alternate" type="text/html" title="Setup Fedora Server 38 as a DNS Server with Qemu/KVM, Part 3" /><published>2023-08-16T15:00:59+00:00</published><updated>2023-08-16T15:00:59+00:00</updated><id>https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dns-server</id><content type="html" xml:base="https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dns-server/"><![CDATA[<div class="blog-post">
    <h2 class="post-title">Setup Fedora Server 38 as a DNS Server with Qemu/KVM, Part 3</h2>
<div class="post-author">
    <span class="avatar"></span>
    <span class="info"><span class="date">Published August 16, 2023</span><br><span class="name">By Samuel Yang</span></span>
</div>

<figure class="post-image">
    <img src="/assets/images/fedora38.jpg" alt="Blog Cover">
</figure>

    <article class="post-content">
        <p>Welcome to configuring Fedora Server 38 as a router tutorial series!</p>

        <ol>
            <li><a href="https://hemimorphite.github.io/2023/08/15/setup-fedora-server-38-as-a-nat-router-with-qemu-kvm/">Setup Fedora Server 38 as a NAT Router with Qemu/KVM, Part 1</a></li>
            <li><a href="https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/">Setup Fedora Server 38 as a DHCP Server with Qemu/KVM, Part 2</a></li>
            <li><a href="https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dns-server/">Setup Fedora Server 38 as a DNS Server with Qemu/KVM, Part 3</a></li>
        </ol>

        <p>One of the most popular programs for Linux operating systems providing DNS forwarder service is dnsmasq.</p>
        
        <p>Fedora Server 38 comes with <code>systemd-resolved</code> which you need to disable since it binds to port 53 which will conflict with dnsmasq port.</p>

        <p>Disable the systemd-resolved DNS listeners to free up port 53 by uncommenting <code>DNSStubListener</code> and setting it to <code>no</code> in <code>/etc/systemd/resolved.conf</code>.</p>

        <pre><code class="language-bash hljs">DNSStubListener=no</code></pre>        

        <p>Disable the systemd-resolved DNSSEC, DNSOverTLS, LLMNR, and MulticastDNS by uncommenting them and setting it to <code>no</code> in <code>/etc/systemd/resolved.conf</code>.</p>

        <pre><code class="language-bash hljs">DNSSEC=no
DNSOverTLS=no
MulticastDNS=no
LLMNR=no</code></pre>

        <p>In order to configure dnsmasq to act as cache for the host on which it is running, uncomment <code>DNS</code> and set it to <code>127.0.0.1</code> to force host to perform local DNS lookup.</p>

        <pre><code class="language-bash hljs">DNS=127.0.0.1</code></pre>

        <p>Restart the <code>systemd-resolved</code> service to apply your changes:</p>

        <pre><code class="language-bash hljs">sudo systemctl restart systemd-resolved</code></pre>

        <p>Once that is done, install dnsmasq:</p>

        <pre><code class="language-bash hljs">sudo apt -y install dnsmasq</code></pre>

        <p>To enable DNS forwarder service, you need to configure <code>/etc/dnsmasq.conf</code>.</p>

        <p>Set the port on which dnsmasq will listen for DNS requests. This default to UDP port 53.</p>

        <pre><code class="language-bash hljs">port=53</code></pre>

        <p>Disable forwarding of names without a dot or domain part by uncommenting:</p>

        <pre><code class="language-bash hljs">domain-needed</code></pre>

        <p>Add a local-only domain:</p>

        <pre><code class="language-bash hljs">local=/app/</code></pre>

        <p>Restart the dnsmasq service to apply your changes:</p>

        <pre><code class="language-bash hljs">sudo systemctl restart dnsmasq</code></pre>

        <p>By default, DNS uses TCP and UDP port 53. Open the DNS port, run:</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --zone=public --permanent --add-port=53/tcp
sudo firewall-cmd --zone=public --permanent --add-port=53/udp</code></pre>

        <p>To apply the configuration, run:</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --reload</code></pre>

        <p>Next, you will need to edit your <code>/etc/hosts</code> file and add the local DNS server entries.</p>

        <pre><code class="language-bash hljs">172.16.0.100    bulbasaur.app
172.16.0.101    ivysaur.app
172.16.0.102    venusaur.app</code></pre>        

        <p>Restart the dnsmasq service to apply your changes:</p>

        <pre><code class="language-bash hljs">sudo systemctl restart dnsmasq</code></pre>
        
        <p>Install <code>dnsutils</code> package:</p>

        <pre><code class="language-bash hljs">sudo dnf -y install dnsutils</code></pre>

        <p>To test if the local DNS server is working or not, run:</p>

        <pre><code class="language-bash hljs">dig bulbasaur.app
dig ivysaur.app
dig venusaur.app</code></pre>
        
        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">; &lt;&lt;&gt;&gt; DiG 9.18.17 &lt;&lt;&gt;&gt; bulbasaur.app
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 39436
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 1232
;; QUESTION SECTION:
;bulbasaur.app.                 IN      A

;; ANSWER SECTION:
bulbasaur.app.          0       IN      A       172.16.0.100

;; Query time: 0 msec
;; SERVER: 127.0.0.1#53(127.0.0.1) (UDP)
;; WHEN: Wed Aug 16 21:51:03 WIB 2023
;; MSG SIZE  rcvd: 58


; &lt;&lt;&gt;&gt; DiG 9.18.17 &lt;&lt;&gt;&gt; ivysaur.app
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 28024
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 1232
;; QUESTION SECTION:
;ivysaur.app.                   IN      A

;; ANSWER SECTION:
ivysaur.app.            0       IN      A       172.16.0.101

;; Query time: 0 msec
;; SERVER: 127.0.0.1#53(127.0.0.1) (UDP)
;; WHEN: Wed Aug 16 21:51:03 WIB 2023
;; MSG SIZE  rcvd: 56


; &lt;&lt;&gt;&gt; DiG 9.18.17 &lt;&lt;&gt;&gt; venusaur.app
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 7234
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 1232
;; QUESTION SECTION:
;venusaur.app.                  IN      A

;; ANSWER SECTION:
venusaur.app.           0       IN      A       172.16.0.102

;; Query time: 1 msec
;; SERVER: 127.0.0.1#53(127.0.0.1) (UDP)
;; WHEN: Wed Aug 16 21:51:03 WIB 2023
;; MSG SIZE  rcvd: 57</code></pre>

        <p>To test a reverse IP lookup, run:</p>

        <pre><code class="language-bash hljs">dig -x 172.16.0.100
dig -x 172.16.0.101
dig -x 172.16.0.102</code></pre>        
        
        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">; &lt;&lt;&gt;&gt; DiG 9.18.17 &lt;&lt;&gt;&gt; -x 172.16.0.100
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 24505
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 1232
;; QUESTION SECTION:
;100.0.16.172.in-addr.arpa.     IN      PTR

;; ANSWER SECTION:
100.0.16.172.in-addr.arpa. 0    IN      PTR     bulbasaur.app.

;; Query time: 0 msec
;; SERVER: 127.0.0.1#53(127.0.0.1) (UDP)
;; WHEN: Wed Aug 16 21:52:03 WIB 2023
;; MSG SIZE  rcvd: 81


; &lt;&lt;&gt;&gt; DiG 9.18.17 &lt;&lt;&gt;&gt; -x 172.16.0.101
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 45987
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 1232
;; QUESTION SECTION:
;101.0.16.172.in-addr.arpa.     IN      PTR

;; ANSWER SECTION:
101.0.16.172.in-addr.arpa. 0    IN      PTR     ivysaur.app.

;; Query time: 0 msec
;; SERVER: 127.0.0.1#53(127.0.0.1) (UDP)
;; WHEN: Wed Aug 16 21:52:03 WIB 2023
;; MSG SIZE  rcvd: 79


; &lt;&lt;&gt;&gt; DiG 9.18.17 &lt;&lt;&gt;&gt; -x 172.16.0.102
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 32116
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 1232
;; QUESTION SECTION:
;102.0.16.172.in-addr.arpa.     IN      PTR

;; ANSWER SECTION:
102.0.16.172.in-addr.arpa. 0    IN      PTR     venusaur.app.

;; Query time: 1 msec
;; SERVER: 127.0.0.1#53(127.0.0.1) (UDP)
;; WHEN: Wed Aug 16 21:52:03 WIB 2023
;; MSG SIZE  rcvd: 80</code></pre>

        <p>Run the tests on the client computers as well to check if the local DNS server is working or not.</p>        
    </article>

    <div class="post-tags">
	<div class="title">Tags</div>
	<ul class="tags">
		
		<li><a href="https://hemimorphite.github.io/tag/qemu" class="tag">qemu</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/linux" class="tag">linux</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/fedora-38" class="tag">fedora 38</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/ufw" class="tag">ufw</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/dns" class="tag">DNS</a></li>
		
	</ul>
</div>


    <div class="post-share">
    <div class="title">Share this post</div>
    <ul class="rounded-social-buttons">
        <li><a href="https://www.facebook.com/sharer/sharer.php?u=https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/" class="social-button facebook"><i class="fab fa-facebook-f"></i></a></li>
        <li><a href="http://twitter.com/share?text=Hey+guys%2c+check+this+out!&amp;url=https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/&amp;hashtags=qemu,linux,fedora 38,firewalld,DHCP" class="social-button twitter"><i class="fab fa-twitter"></i></a></li>
        <li><a href="https://www.linkedin.com/sharing/share-offsite/?url=https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/" class="social-button linkedin"><i class="fab fa-linkedin"></i></a></li>
    </ul>
</div>
</div>]]></content><author><name>Samuel Yang</name></author><category term="Tutorials" /><category term="year-2023" /><category term="month-08" /><category term="day-16" /><category term="qemu" /><category term="linux" /><category term="fedora 38" /><category term="ufw" /><category term="DNS" /><summary type="html"><![CDATA[This tutorial provides step-by-step instructions on how to set up Fedora Server 38 as a DNS Server using Qemu/KVM. Learn how to configure network settings and create virtual machines to build your own virtual network infrastructure.]]></summary></entry><entry xml:lang="en"><title type="html">Setup Fedora Server 38 as a DHCP Server with Qemu/KVM, Part 2</title><link href="https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/" rel="alternate" type="text/html" title="Setup Fedora Server 38 as a DHCP Server with Qemu/KVM, Part 2" /><published>2023-08-16T09:38:59+00:00</published><updated>2023-08-16T09:38:59+00:00</updated><id>https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server</id><content type="html" xml:base="https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/"><![CDATA[<div class="blog-post">
    <h2 class="post-title">Setup Fedora Server 38 as a DHCP Server with Qemu/KVM, Part 2</h2>
<div class="post-author">
    <span class="avatar"></span>
    <span class="info"><span class="date">Published August 16, 2023</span><br><span class="name">By Samuel Yang</span></span>
</div>

<figure class="post-image">
    <img src="/assets/images/fedora38.jpg" alt="Blog Cover">
</figure>

    <article class="post-content">
        <p>Welcome to configuring Fedora Server 38 as a router tutorial series!</p>

        <ol>
            <li><a href="https://hemimorphite.github.io/2023/08/15/setup-fedora-server-38-as-a-nat-router-with-qemu-kvm/">Setup Fedora Server 38 as a NAT Router with Qemu/KVM, Part 1</a></li>
            <li><a href="https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/">Setup Fedora Server 38 as a DHCP Server with Qemu/KVM, Part 2</a></li>
            <li><a href="https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dns-server/">Setup Fedora Server 38 as a DNS Server with Qemu/KVM, Part 3</a></li>
        </ol>

        <p>One of the most popular programs for Linux operating systems providing DHCP service is dnsmasq.</p>
        
        <p>Install dnsmasq:</p>

        <pre><code class="language-bash hljs">sudo dnf -y install dnsmasq</code></pre>

        <p>The response should look similar to this:</p>
        
        <pre><code class="language-bash hljs">Fedora 38 - x86_64                                                                        1.3 MB/s |  83 MB     01:02    
Fedora 38 openh264 (From Cisco) - x86_64                                                  504  B/s | 2.5 kB     00:05    
Fedora Modular 38 - x86_64                                                                376 kB/s | 2.8 MB     00:07    
Fedora 38 - x86_64 - Updates                                                              1.4 MB/s |  30 MB     00:21    
Fedora Modular 38 - x86_64 - Updates                                                      402 kB/s | 2.1 MB     00:05    
Dependencies resolved.
==========================================================================================================================
 Package                     Architecture               Version                         Repository                   Size
==========================================================================================================================
Installing:
 dnsmasq                     x86_64                     2.89-5.fc38                     updates                     357 k

Transaction Summary
==========================================================================================================================
Install  1 Package

Total download size: 357 k
Installed size: 768 k
Downloading Packages:
dnsmasq-2.89-5.fc38.x86_64.rpm                                                            253 kB/s | 357 kB     00:01    
--------------------------------------------------------------------------------------------------------------------------
Total                                                                                     126 kB/s | 357 kB     00:02     
Fedora 38 - x86_64 - Updates                                                              1.2 MB/s | 1.6 kB     00:00    
Importing GPG key 0xEB10B464:
 Userid     : "Fedora (38) &lt;fedora-38-primary@fedoraproject.org&gt;"
 Fingerprint: 6A51 BBAB BA3D 5467 B617 1221 809A 8D7C EB10 B464
 From       : /etc/pki/rpm-gpg/RPM-GPG-KEY-fedora-38-x86_64
Key imported successfully
Running transaction check
Transaction check succeeded.
Running transaction test
Transaction test succeeded.
Running transaction
  Preparing        :                                                                                                  1/1 
  Running scriptlet: dnsmasq-2.89-5.fc38.x86_64                                                                       1/1 
  Installing       : dnsmasq-2.89-5.fc38.x86_64                                                                       1/1 
  Running scriptlet: dnsmasq-2.89-5.fc38.x86_64                                                                       1/1 
  Verifying        : dnsmasq-2.89-5.fc38.x86_64                                                                       1/1 

Installed:
  dnsmasq-2.89-5.fc38.x86_64                                                                                              

Complete!</code></pre>

        <p>To enable DHCP service, you need to configure <code>/etc/dnsmasq.conf</code>.</p>

        <p>By default dnsmasq enables DNS service. You can turn it off by changing the port to 0:</p>

        <pre><code class="language-bash hljs">port=0</code></pre>
        
        <p>The DHCPv4 server is activated by specifying an IPv4 address range and a router:</p>

        <pre><code class="language-bash hljs">dhcp-range=set:enp0s5v4,172.16.0.2,172.16.0.254,255.255.255.0,12h

dhcp-option=tag:enp0s5v4,option:router,172.16.0.1</code></pre>

        <p>The above configuration instructs dnsmasq to offer IPv4 addresses between <code>172.16.0.2</code> and <code>172.16.0.254</code> with a subnet <code>255.255.255.0</code> on the interface <code>enp0s5</code>. Issued IPs will have a lease lifetime of twelve hours, after which clients will need to request a renewed lease.</p>

        <p>Make sure the following options and other options are commented out:</p>

        <pre><code class="language-bash hljs">#interface=lo

#bind-interfaces</code></pre>

        <pre><code class="language-bash hljs">dhcp-range=set:enp0s5v4,172.16.0.2,172.16.0.254,255.255.255.0,12h

dhcp-option=tag:enp0s5v4,option:router,172.16.0.1</code></pre>

        <p>Restart the dnsmasq service to apply your changes:</p>

        <pre><code class="language-bash hljs">sudo systemctl restart dnsmasq</code></pre>        

        <p>By default DHCP uses UDP ports 68 and 67 to initiate communication between the IPv4 client and server. If port 67 is in use by another process, DHCP server cannot communicate with DHCPv4 clients.</p>

        <p>To open UDP port 67 in ufw, run:</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --zone=public --permanent --add-port=67/udp</code></pre>

        <p>To apply the configuration, run:</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --reload</code></pre>

        <p>Dnsmasq also provides full IPv6 support.</p>
                
        <p>The DHCPv6 server is activated by specifying an IPv6 address range and enabling IPv6 Router Advertisement feature:</p>

        <pre><code class="language-bash hljs">dhcp-range=set:enp0s5v6,::2,::ffff,constructor:enp0s5,slaac,64,12h

enable-ra</code></pre>
        
        <p>The above configuration instructs dnsmasq to offer IPv6 addresses between <code>fd62:bf06:3a25:7670::2</code> and <code>fd62:bf06:3a25:7670::ffff</code> with prefix length 64 bits on the interface <code>enp0s5</code>.</p>

        <p>Restart the dnsmasq service to apply your changes:</p>

        <pre><code class="language-bash hljs">sudo systemctl restart dnsmasq</code></pre>

        <p>By default DHCP uses UDP ports 546 and 547 to initiate communication between the IPv6 client and server. If port 547 is in use by another process, DHCP server cannot communicate with DHCPv6 clients.</p>

        <p>To open UDP port 547 in ufw, run:</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --zone=public --permanent --add-port=547/udp</code></pre>

        <p>To apply the configuration, run:</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --reload</code></pre>

        <p>Now the client computers should obtain IPv4 address and IPv6 address automatically.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoradhcpserver-01.jpg" alt="configuring Fedora Server 38 as DHCP server">
        </figure>
    </article>

    <div class="post-tags">
	<div class="title">Tags</div>
	<ul class="tags">
		
		<li><a href="https://hemimorphite.github.io/tag/qemu" class="tag">qemu</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/linux" class="tag">linux</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/fedora-38" class="tag">fedora 38</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/firewalld" class="tag">firewalld</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/dhcp" class="tag">DHCP</a></li>
		
	</ul>
</div>


    <div class="post-share">
    <div class="title">Share this post</div>
    <ul class="rounded-social-buttons">
        <li><a href="https://www.facebook.com/sharer/sharer.php?u=https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/" class="social-button facebook"><i class="fab fa-facebook-f"></i></a></li>
        <li><a href="http://twitter.com/share?text=Hey+guys%2c+check+this+out!&amp;url=https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/&amp;hashtags=qemu,linux,fedora 38,firewalld,DHCP" class="social-button twitter"><i class="fab fa-twitter"></i></a></li>
        <li><a href="https://www.linkedin.com/sharing/share-offsite/?url=https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/" class="social-button linkedin"><i class="fab fa-linkedin"></i></a></li>
    </ul>
</div>
</div>]]></content><author><name>Samuel Yang</name></author><category term="Tutorials" /><category term="year-2023" /><category term="month-08" /><category term="day-16" /><category term="qemu" /><category term="linux" /><category term="fedora 38" /><category term="firewalld" /><category term="DHCP" /><summary type="html"><![CDATA[This tutorial provides step-by-step instructions on how to set up Fedora Server 38 as a DHCP Server using Qemu/KVM. Learn how to configure network settings and create virtual machines to build your own virtual network infrastructure.]]></summary></entry><entry xml:lang="en"><title type="html">Setup Fedora Server 38 as a NAT Router with Qemu/KVM, Part 1</title><link href="https://hemimorphite.github.io/2023/08/15/setup-fedora-server-38-as-a-nat-router-with-qemu-kvm/" rel="alternate" type="text/html" title="Setup Fedora Server 38 as a NAT Router with Qemu/KVM, Part 1" /><published>2023-08-15T13:10:20+00:00</published><updated>2023-08-15T13:10:20+00:00</updated><id>https://hemimorphite.github.io/2023/08/15/setup-fedora-server-38-as-a-nat-router-with-qemu-kvm</id><content type="html" xml:base="https://hemimorphite.github.io/2023/08/15/setup-fedora-server-38-as-a-nat-router-with-qemu-kvm/"><![CDATA[<div class="blog-post">
    <h2 class="post-title">Setup Fedora Server 38 as a NAT Router with Qemu/KVM, Part 1</h2>
<div class="post-author">
    <span class="avatar"></span>
    <span class="info"><span class="date">Published August 15, 2023</span><br><span class="name">By Samuel Yang</span></span>
</div>

<figure class="post-image">
    <img src="/assets/images/fedora38.jpg" alt="Blog Cover">
</figure>

    <article class="post-content">
        <p>Welcome to configuring Fedora Server 38 as a router tutorial series!</p>

        <ol>
            <li><a href="https://hemimorphite.github.io/2023/08/15/setup-fedora-server-38-as-a-nat-router-with-qemu-kvm/">Setup Fedora Server 38 as a NAT Router with Qemu/KVM, Part 1</a></li>
            <li><a href="https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/">Setup Fedora Server 38 as a DHCP Server with Qemu/KVM, Part 2</a></li>
            <li><a href="https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dns-server/">Setup Fedora Server 38 as a DNS Server with Qemu/KVM, Part 3</a></li>
        </ol>

        <p>This is how it will look like the virtual network we are going to build:</p>
        
        <figure class="post-figure">
            <img src="/assets/images/fedoranat-01.jpg" alt="configuring Fedora Server 38 as a NAT router">
        </figure>

        <ul>
            <li><b>enp0s4</b>: WAN interface</li>
            <li><b>enp0s5</b>: LAN interface with a ipv4 subnet prefix address 172.16.0.0/24</li>
            <li><b>192.168.0.15</b>: the first IPv4 address of Fedora Server 38 as WAN IPv4 address</li>
            <li><b>172.16.0.1</b>: the second IPv4 address of Fedora Server 38 as LAN IPv4 Gateway</li>
            <li><b>172.16.0.11</b>: the IPv4 address of the first client</li>
            <li><b>172.16.0.12</b>: the IPv4 address of the second client</li>
        </ul>

        <p>First, we need to have three qemu images:</p>

        <ul>
            <li>Fedora Server 38 distro as a router gateway</li>
            <li>Fedora Desktop 38 distro as a client 1</li>
            <li>Fedora Desktop 38 distro as a client 2</li>
        </ul>

        <p>Then, we need set up a linux bridge (which will act as a layer 2 switch) and three tap interfaces on the host computer to connect three qemu images to each other.</p>

        <p>We can create a new tap interface using <code>tunctl</code> command. In order to use <code>tunctl</code> command, you need to install <code>uml-utilities</code> package on Ubuntu:</p>

        <pre><code class="language-bash hljs">sudo apt install uml-utilities</code></pre>

        <p>Or <code>tunctl</code> package on Fedora:</p>

        <pre><code class="language-bash hljs">sudo dnf install tunctl</code></pre>

        <p>We need to create a tap interface for each of the virtual machines, then we will need to create three different tap interfaces:</p>

        <pre><code class="language-bash hljs">sudo tunctl -u $USER -t tap1
sudo tunctl -u $USER -t tap2
sudo tunctl -u $USER -t tap3</code></pre>

        <p>Next, we bring the tap interfaces up:</p>

        <pre><code class="language-bash hljs">sudo ip link set dev tap1 up 
sudo ip link set dev tap2 up
sudo ip link set dev tap3 up</code></pre>
        
        <p>Then, we create a network bridge using <code>brctl</code> command:</p>

        <pre><code class="language-bash hljs">sudo brctl addbr br0</code></pre>

        <p>Bring up the network bridge:</p>

        <pre><code class="language-bash hljs">sudo ip link set dev br0 up</code></pre>

        <p>Then we attach the tap interfaces to the bridge interface:</p>

        <pre><code class="language-bash hljs">sudo brctl addif br0 tap1
sudo brctl addif br0 tap2
sudo brctl addif br0 tap3</code></pre>
        
        <p>Start the Fedora Server 38 virtual machine by specifying two network interfaces (WAN interface and LAN interface) with unique mac addresses:</p>
        
        <pre><code class="language-bash hljs">qemu-system-x86_64 -name "Fedora Server 38 Router" \
-machine type=pc-q35-2.12 -accel kvm \
-m 4G -cpu host \
-display sdl \
-bios /usr/share/ovmf/OVMF.fd \
-device virtio-vga,addr=01.0 \
-drive file=fedorarouter.img,if=none,id=drive0 \
-device nvme,serial=364740043439,addr=02.0,bus=pcie.0,drive=drive0 \
-netdev user,id=net0,ipv4=on,net=192.168.0.0/24,ipv6=on,ipv6-net=fd65:9513:8ed6:5dc7::/64,dns=192.168.0.1,ipv6-dns=fd65:9513:8ed6:5dc7::1 \
-device e1000-82545em,addr=04.0,bus=pcie.0,mac=46:34:84:53:93:78,netdev=net0 \
-netdev tap,id=net1,ifname="tap1",script=no,downscript=no \
-device e1000-82545em,addr=05.0,bus=pcie.0,mac=35:93:59:28:34:55,netdev=net1</code></pre>
        
        <p>List all the available devices/network interfaces using <code>nmcli</code> command:</p>

        <pre><code class="language-bash hljs">sudo nmcli device status</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">enp0s4  ethernet  connected               enp0s4     
lo      loopback  connected (externally)  lo         
enp0s5  ethernet  disconnected            --</code></pre>
        
        <p>WAN interface <code>enp0s4</code> sets to DHCP and LAN interface <code>enp0s5</code> sets to static IP.</p>

        <p>Create a LAN interface connection <code>enp0s5</code>:</p>

        <pre><code class="language-bash hljs">sudo nmcli connection add type ethernet con-name enp0s5</code></pre>

        <p>Attach the <code>enp0s5</code> device to the connection <code>enp0s5</code>:</p>
        
        <pre><code class="language-bash hljs">sudo nmcli connection modify enp0s5 connection.interface-name enp0s5</code></pre>

        <p>Modify the connection <code>enp0s5</code> to use a static IP.</p>

        <pre><code class="language-bash hljs">sudo nmcli connection modify enp0s5 ipv4.addresses 172.16.0.1/24
sudo nmcli connection modify enp0s5 ipv6.addresses fd62:bf06:3a25:7670::1/64</code></pre>
        
sudo nmcli connection modify enp0s6 ipv4.addresses 172.17.0.1/24
sudo nmcli connection modify enp0s6 ipv6.addresses fdf4:1106:0655:089f::1/64

        <p>Then, change the connection addressing method from <code>auto</code> to <code>manual</code>.</p>
        
        <pre><code class="language-bash hljs">sudo nmcli connection modify enp0s5 ipv4.method manual
sudo nmcli connection modify enp0s5 ipv6.method manual</code></pre>
        
        <p>To apply the configuration, run:</p>

        <pre><code class="language-bash hljs">sudo systemctl restart NetworkManager</code></pre>

        <p>Next, we need to enable IP Masquerading. The purpose of IP Masquerading is to allow machines with private IP addresses on your network to access the Internet through the machine doing the masquerading.</p>

        <p>To enable IP Masquerading in firewalld, run:</p>
        
        <pre><code class="language-bash hljs">sudo firewall-cmd --zone=public --permanent --add-masquerade</code></pre>

        <p>IP forwarding plays a fundamental role on a router. This is the functionality that allows a router to forward traffic from one network interface to another network interface.</p>

        <p>To enable IP forwarding in firewalld, run:</p>
        
        <pre><code class="language-bash hljs">sudo firewall-cmd --zone=public --permanent --add-forward</code></pre>    

        <p>To apply the configuration, run:</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --reload</code></pre>

        <p>Then, we also need to IP forwarding on the Fedora Server system by creating sysctl configuration file <code>90-override.conf</code> in <code>/etc/sysctl.d/</code> and add the following line:.</p>        

        <pre><code class="language-bash hljs">net.ipv4.ip_forward=1

net.ipv6.conf.default.forwarding=1</code></pre>        

        <p>Next, execute the <code>sysctl</code> command to enable the new settings in the configuration file:</p>

        <pre><code class="language-bash hljs">sysctl -p /etc/sysctl.d/90-override.conf</code></pre>

        <!--<p>Dynamic Host Configuration Protocol or DHCP is an IP network protocol that relies on client-server architecture to automatically set IP addresses and other attributes to an IP host to enable information transfer between network nodes.</p>

        <p>DHCPv4 servers have a UDP port number of 67, so listen for messages addressed to this port number. On the other hand, DHCPv4 clients have the UDP port number 68 and only respond to messages sent to number 68.</p>

        <p>DHCPv4 and DHCPv6 UDP port numbers are different. DHCPv6 servers have a UDP port number of 547 and DHCPv6 clients have the UDP port number 546.</p>

        <p>In order to enable communication between the built-in DHCPv4 server of <code>systemd-networkd</code> on UDP port 67 and the built-in DHCPv4 client of <code>systemd-networkd</code> on UDP port 68, you need to open UDP port 67 in ufw:</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --zone=public --permanent --add-port=67/udp</code></pre>

        <p>The same as DHCPv6 server of <code>systemd-networkd</code> on UDP port 547 and the built-in DHCPv6 client of <code>systemd-networkd</code> on UDP port 546, you need to open UDP port 547 in ufw:</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --zone=public --permanent --add-port=547/udp</code></pre>-->

        <p>Start the Ubuntu Desktop 22.04 virtual machines by specifying one network interface (only LAN interface) with unique mac addresses:</p>

        <pre><code class="language-bash hljs">qemu-system-x86_64 -name "Fedora Desktop 38 Client 1" \
-machine type=pc-q35-2.12 -accel kvm \
-m 4G -cpu host \
-display sdl \
-bios /usr/share/ovmf/OVMF.fd \
-device virtio-vga,addr=01.0 \
-drive file=fedoraclient1.img,if=none,id=drive0 \
-device nvme,serial=364740043439,addr=02.0,bus=pcie.0,drive=drive0 \
-netdev tap,id=net0,ifname="tap2",script=no,downscript=no \
-device e1000-82545em,addr=04.0,bus=pcie.0,mac=68:98:35:90:34:56,netdev=net0</code></pre>
        
        <pre><code class="language-bash hljs">qemu-system-x86_64 -name "Fedora Desktop 38 Client 2" \
-machine type=pc-q35-2.12 -accel kvm \
-m 4G -cpu host \
-display sdl \
-bios /usr/share/ovmf/OVMF.fd \
-device virtio-vga,addr=01.0 \
-drive file=fedoraclient2.img,if=none,id=drive0 \
-device nvme,serial=364740043439,addr=02.0,bus=pcie.0,drive=drive0 \
-netdev tap,id=net0,ifname="tap3",script=no,downscript=no \
-device e1000-82545em,addr=04.0,bus=pcie.0,mac=82:54:65:76:38:28,netdev=net0</code></pre>
        
        <p>List all the available devices/network interfaces using <code>nmcli</code> command:</p>

        <pre><code class="language-bash hljs">sudo nmcli device status</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">lo      loopback  connected (externally)  lo         
enp0s4  ethernet  disconnected            --</code></pre>
        
        <p>Create a LAN interface connection <code>enp0s4</code>:</p>

        <pre><code class="language-bash hljs">sudo nmcli connection add type ethernet con-name enp0s4</code></pre>

        <p>Attach the <code>enp0s4</code> device to the connection <code>enp0s4</code>:</p>
        
        <pre><code class="language-bash hljs">sudo nmcli connection modify enp0s4 connection.interface-name enp0s4</code></pre>

        <p>On the first client:</p>

        <p>Modify the connection <code>enp0s4</code> to use a static IP.</p>

        <pre><code class="language-bash hljs">sudo nmcli connection modify enp0s4 ipv4.addresses 172.16.0.11/24
sudo nmcli connection modify enp0s4 ipv6.addresses fd62:bf06:3a25:7670::11/64</code></pre>
        
        <p>Configure the default gateway.</p>

        <pre><code class="language-bash hljs">sudo nmcli connection modify enp0s4 ipv4.gateway 172.16.0.1
sudo nmcli connection modify enp0s4 ipv6.gateway fd62:bf06:3a25:7670::1</code></pre>

        <p>Then, change the connection addressing method from <code>auto</code> to <code>manual</code>.</p>
        
        <pre><code class="language-bash hljs">sudo nmcli connection modify enp0s4 ipv4.method manual
sudo nmcli connection modify enp0s4 ipv6.method manual</code></pre>
        
        <p>To apply the configuration, run:</p>

        <pre><code class="language-bash hljs">sudo systemctl restart NetworkManager</code></pre>

        <p>On the second client:</p>

        <p>Modify the connection <code>enp0s4</code> to use a static IP.</p>

        <pre><code class="language-bash hljs">sudo nmcli connection modify enp0s4 ipv4.addresses 172.16.0.12/24
sudo nmcli connection mod enp0s4 ipv6.addresses fd62:bf06:3a25:7670::12/64</code></pre>
        
        <p>Configure the default gateway.</p>

        <pre><code class="language-bash hljs">sudo nmcli connection modify enp0s4 ipv4.gateway 172.16.0.1
sudo nmcli connection mod enp0s4 ipv6.addresses fd62:bf06:3a25:7670::1</code></pre>

        <p>Then, change the connection addressing method from <code>auto</code> to <code>manual</code>.</p>
        
        <pre><code class="language-bash hljs">sudo nmcli connection mod enp0s4 ipv4.method manual
sudo nmcli connection mod enp0s4 ipv6.method manual</code></pre>

        <p>To apply the configuration, run:</p>

        <pre><code class="language-bash hljs">sudo systemctl restart NetworkManager</code></pre>        
        
        <p>Configure <code>systemd-resolved</code> to use Google Public DNS so your system can have internet access. Edit <code>/etc/systemd/resolved.conf</code>, uncomment and change the <code>DNS</code> to <code>8.8.8.8</code></p>

        <pre><code class="language-bash hljs">DNS=8.8.8.8</code></pre>        

        <p>To apply the configuration, run:</p>

        <pre><code class="language-bash hljs">sudo systemctl restart systemd-resolved</code></pre>

        <p>Try to ping google.com to check internet connectivity. You should have internet access now.</p>
    </article>

    <div class="post-tags">
	<div class="title">Tags</div>
	<ul class="tags">
		
		<li><a href="https://hemimorphite.github.io/tag/qemu" class="tag">qemu</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/linux" class="tag">linux</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/fedora-38" class="tag">fedora 38</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/firewalld" class="tag">firewalld</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/nat" class="tag">NAT</a></li>
		
	</ul>
</div>


    <div class="post-share">
    <div class="title">Share this post</div>
    <ul class="rounded-social-buttons">
        <li><a href="https://www.facebook.com/sharer/sharer.php?u=https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/" class="social-button facebook"><i class="fab fa-facebook-f"></i></a></li>
        <li><a href="http://twitter.com/share?text=Hey+guys%2c+check+this+out!&amp;url=https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/&amp;hashtags=qemu,linux,fedora 38,firewalld,DHCP" class="social-button twitter"><i class="fab fa-twitter"></i></a></li>
        <li><a href="https://www.linkedin.com/sharing/share-offsite/?url=https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/" class="social-button linkedin"><i class="fab fa-linkedin"></i></a></li>
    </ul>
</div>
</div>]]></content><author><name>Samuel Yang</name></author><category term="Tutorials" /><category term="year-2023" /><category term="month-08" /><category term="day-15" /><category term="qemu" /><category term="linux" /><category term="fedora 38" /><category term="firewalld" /><category term="NAT" /><summary type="html"><![CDATA[This tutorial provides step-by-step instructions on how to set up Fedora Server 38 as a NAT router using Qemu/KVM. Learn how to configure network settings and create virtual machines to build your own virtual network infrastructure.]]></summary></entry><entry xml:lang="en"><title type="html">Setup Ubuntu Server 22.04 as a DNS Server with Qemu/KVM, Part 3</title><link href="https://hemimorphite.github.io/2023/08/13/setup-ubuntu-server-22.04-as-a-dns-server/" rel="alternate" type="text/html" title="Setup Ubuntu Server 22.04 as a DNS Server with Qemu/KVM, Part 3" /><published>2023-08-13T15:48:59+00:00</published><updated>2023-08-13T15:48:59+00:00</updated><id>https://hemimorphite.github.io/2023/08/13/setup-ubuntu-server-22.04-as-a-dns-server</id><content type="html" xml:base="https://hemimorphite.github.io/2023/08/13/setup-ubuntu-server-22.04-as-a-dns-server/"><![CDATA[<div class="blog-post">
    <h2 class="post-title">Setup Ubuntu Server 22.04 as a DNS Server with Qemu/KVM, Part 3</h2>
<div class="post-author">
    <span class="avatar"></span>
    <span class="info"><span class="date">Published August 13, 2023</span><br><span class="name">By Samuel Yang</span></span>
</div>

<figure class="post-image">
    <img src="/assets/images/ubuntu2204.jpg" alt="Blog Cover">
</figure>

    <article class="post-content">
        <p>Welcome to configuring Ubuntu Server 22.04 as a router tutorial series!</p>

        <ol>
            <li><a href="https://hemimorphite.github.io/2023/08/08/setup-ubuntu-server-22-04-as-a-nat-router-with-qemu-kvm/">Setup Ubuntu Server 22.04 as a NAT Router with Qemu/KVM, Part 1</a></li>
            <li><a href="https://hemimorphite.github.io/2023/08/12/setup-ubuntu-server-22.04-as-a-dhcp-server/">Setup Ubuntu Server 22.04 as a DHCP Server with Qemu/KVM, Part 2</a></li>
            <li class="active"><a href="https://hemimorphite.github.io/2023/08/13/setup-ubuntu-server-22.04-as-a-dns-server/">Setup Ubuntu Server 22.04 as a DNS Server with Qemu/KVM, Part 3</a></li>
        </ol>

        <p>One of the most popular programs for Linux operating systems providing DNS forwarder service is dnsmasq.</p>
        
        <p>Ubuntu 22.04 comes with <code>systemd-resolved</code> which you need to disable since it binds to port 53 which will conflict with dnsmasq port.</p>

        <p>Disable the systemd-resolved DNS listeners to free up port 53 by uncommenting <code>DNSStubListener</code> and setting it to <code>no</code> in <code>/etc/systemd/resolved.conf</code>.</p>

        <pre><code class="language-bash hljs">DNSStubListener=no</code></pre>        

        <p>Disable the systemd-resolved DNSSEC, DNSOverTLS, LLMNR, and MulticastDNS by uncommenting them and setting it to <code>no</code> in <code>/etc/systemd/resolved.conf</code>.</p>

        <pre><code class="language-bash hljs">DNSSEC=no
DNSOverTLS=no
MulticastDNS=no
LLMNR=no</code></pre>

        <p>In order to configure dnsmasq to act as cache for the host on which it is running, uncomment <code>DNS</code> and set it to <code>127.0.0.1</code> to force host to perform local DNS lookup.</p>

        <pre><code class="language-bash hljs">DNS=127.0.0.1</code></pre>

        <p>Restart the <code>systemd-resolved</code> service to apply your changes:</p>

        <pre><code class="language-bash hljs">sudo systemctl restart systemd-resolved</code></pre>

        <p>Once that is done, install dnsmasq:</p>

        <pre><code class="language-bash hljs">sudo apt -y install dnsmasq</code></pre>

        <p>To enable DNS forwarder service, you need to configure <code>/etc/dnsmasq.conf</code>.</p>

        <p>Set the port on which dnsmasq will listen for DNS requests. This default to UDP port 53.</p>

        <pre><code class="language-bash hljs">port=53</code></pre>

        <p>Disable forwarding of names without a dot or domain part by uncommenting:</p>

        <pre><code class="language-bash hljs">domain-needed</code></pre>

        <p>Add a local-only domain:</p>

        <pre><code class="language-bash hljs">local=/app/</code></pre>

        <p>Restart the dnsmasq service to apply your changes:</p>

        <pre><code class="language-bash hljs">sudo systemctl restart dnsmasq</code></pre>

        <p>By default, DNS uses TCP and UDP port 53. Open the DNS port, run:</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --zone=public --permanent --add-port=53/tcp
sudo firewall-cmd --zone=public --permanent --add-port=53/udp</code></pre>

        <p>Next, you will need to edit your <code>/etc/hosts</code> file and add the local DNS server entries.</p>

        <pre><code class="language-bash hljs">172.16.0.100    bulbasaur.app
172.16.0.101    ivysaur.app
172.16.0.102    venusaur.app</code></pre>        

        <p>Restart the dnsmasq service to apply your changes:</p>

        <pre><code class="language-bash hljs">sudo systemctl restart dnsmasq</code></pre>
        
        <p>Install <code>dnsutils</code> package:</p>

        <pre><code class="language-bash hljs">sudo apt -y install dnsutils</code></pre>

        <p>To test if the local DNS server is working or not, run:</p>

        <pre><code class="language-bash hljs">dig bulbasaur.app
dig ivysaur.app
dig venusaur.app</code></pre>
        
        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">; &lt;&lt;&gt;&gt; DiG 9.18.12-0ubuntu0.22.04.2-Ubuntu &lt;&lt;&gt;&gt; bulbasaur.app
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 62416
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 1232
;; QUESTION SECTION:
;bulbasaur.app.         IN  A

;; ANSWER SECTION:
bulbasaur.app.      0   IN  A   172.16.0.100

;; Query time: 0 msec
;; SERVER: fe80::9a73:37ff:fe90:3457%3#53(fe80::9a73:37ff:fe90:3457%3%3) (UDP)
;; WHEN: Mon Aug 14 13:24:05 UTC 2023
;; MSG SIZE  rcvd: 58


; &lt;&lt;&gt;&gt; DiG 9.18.12-0ubuntu0.22.04.2-Ubuntu &lt;&lt;&gt;&gt; ivysaur.app
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 33069
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 1232
;; QUESTION SECTION:
;ivysaur.app.           IN  A

;; ANSWER SECTION:
ivysaur.app.        0   IN  A   172.16.0.101

;; Query time: 0 msec
;; SERVER: fe80::9a73:37ff:fe90:3457%3#53(fe80::9a73:37ff:fe90:3457%3%3) (UDP)
;; WHEN: Mon Aug 14 13:24:05 UTC 2023
;; MSG SIZE  rcvd: 56


; &lt;&lt;&gt;&gt; DiG 9.18.12-0ubuntu0.22.04.2-Ubuntu &lt;&lt;&gt;&gt; venusaur.app
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 50282
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 1232
;; QUESTION SECTION:
;venusaur.app.          IN  A

;; ANSWER SECTION:
venusaur.app.       0   IN  A   172.16.0.102

;; Query time: 0 msec
;; SERVER: fe80::9a73:37ff:fe90:3457%3#53(fe80::9a73:37ff:fe90:3457%3%3) (UDP)
;; WHEN: Mon Aug 14 13:24:05 UTC 2023
;; MSG SIZE  rcvd: 57</code></pre>

        <p>To test a reverse IP lookup, run:</p>

        <pre><code class="language-bash hljs">dig -x 172.16.0.100
dig -x 172.16.0.101
dig -x 172.16.0.102</code></pre>        
        
        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">; &lt;&lt;&gt;&gt; DiG 9.18.12-0ubuntu0.22.04.2-Ubuntu &lt;&lt;&gt;&gt; -x 172.16.0.100
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 13755
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 1232
;; QUESTION SECTION:
;100.0.16.172.in-addr.arpa. IN  PTR

;; ANSWER SECTION:
100.0.16.172.in-addr.arpa. 0    IN  PTR bulbasaur.app.

;; Query time: 0 msec
;; SERVER: fe80::9a73:37ff:fe90:3457%3#53(fe80::9a73:37ff:fe90:3457%3%3) (UDP)
;; WHEN: Mon Aug 14 13:26:19 UTC 2023
;; MSG SIZE  rcvd: 81


; &lt;&lt;&gt;&gt; DiG 9.18.12-0ubuntu0.22.04.2-Ubuntu &lt;&lt;&gt;&gt; -x 172.16.0.101
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 15516
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 1232
;; QUESTION SECTION:
;101.0.16.172.in-addr.arpa. IN  PTR

;; ANSWER SECTION:
101.0.16.172.in-addr.arpa. 0    IN  PTR ivysaur.app.

;; Query time: 0 msec
;; SERVER: fe80::9a73:37ff:fe90:3457%3#53(fe80::9a73:37ff:fe90:3457%3%3) (UDP)
;; WHEN: Mon Aug 14 13:26:19 UTC 2023
;; MSG SIZE  rcvd: 79


; &lt;&lt;&gt;&gt; DiG 9.18.12-0ubuntu0.22.04.2-Ubuntu &lt;&lt;&gt;&gt; -x 172.16.0.102
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 47090
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 1232
;; QUESTION SECTION:
;102.0.16.172.in-addr.arpa. IN  PTR

;; ANSWER SECTION:
102.0.16.172.in-addr.arpa. 0    IN  PTR venusaur.app.

;; Query time: 0 msec
;; SERVER: fe80::9a73:37ff:fe90:3457%3#53(fe80::9a73:37ff:fe90:3457%3%3) (UDP)
;; WHEN: Mon Aug 14 13:26:19 UTC 2023
;; MSG SIZE  rcvd: 80</code></pre>

        <p>Run the tests on the client computers as well to check if the local DNS server is working or not.</p>        
    </article>

    <div class="post-tags">
	<div class="title">Tags</div>
	<ul class="tags">
		
		<li><a href="https://hemimorphite.github.io/tag/qemu" class="tag">qemu</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/linux" class="tag">linux</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/ubuntu-22-04" class="tag">ubuntu 22.04</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/ufw" class="tag">ufw</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/dns" class="tag">DNS</a></li>
		
	</ul>
</div>


    <div class="post-share">
    <div class="title">Share this post</div>
    <ul class="rounded-social-buttons">
        <li><a href="https://www.facebook.com/sharer/sharer.php?u=https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/" class="social-button facebook"><i class="fab fa-facebook-f"></i></a></li>
        <li><a href="http://twitter.com/share?text=Hey+guys%2c+check+this+out!&amp;url=https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/&amp;hashtags=qemu,linux,fedora 38,firewalld,DHCP" class="social-button twitter"><i class="fab fa-twitter"></i></a></li>
        <li><a href="https://www.linkedin.com/sharing/share-offsite/?url=https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/" class="social-button linkedin"><i class="fab fa-linkedin"></i></a></li>
    </ul>
</div>
</div>]]></content><author><name>Samuel Yang</name></author><category term="Tutorials" /><category term="year-2023" /><category term="month-08" /><category term="day-13" /><category term="qemu" /><category term="linux" /><category term="ubuntu 22.04" /><category term="ufw" /><category term="DNS" /><summary type="html"><![CDATA[This tutorial will guide you through the process of configuring Ubuntu Server 22.04 to function as a DNS server. Learn step-by-step instructions to set up and optimize your Ubuntu server as a DNS server.]]></summary></entry><entry xml:lang="en"><title type="html">Setup Ubuntu Server 22.04 as a DHCP Server with Qemu/KVM, Part 2</title><link href="https://hemimorphite.github.io/2023/08/12/setup-ubuntu-server-22.04-as-a-dhcp-server/" rel="alternate" type="text/html" title="Setup Ubuntu Server 22.04 as a DHCP Server with Qemu/KVM, Part 2" /><published>2023-08-12T15:38:59+00:00</published><updated>2023-08-12T15:38:59+00:00</updated><id>https://hemimorphite.github.io/2023/08/12/setup-ubuntu-server-22.04-as-a-dhcp-server</id><content type="html" xml:base="https://hemimorphite.github.io/2023/08/12/setup-ubuntu-server-22.04-as-a-dhcp-server/"><![CDATA[<div class="blog-post">
    <h2 class="post-title">Setup Ubuntu Server 22.04 as a DHCP Server with Qemu/KVM, Part 2</h2>
<div class="post-author">
    <span class="avatar"></span>
    <span class="info"><span class="date">Published August 12, 2023</span><br><span class="name">By Samuel Yang</span></span>
</div>

<figure class="post-image">
    <img src="/assets/images/ubuntu2204.jpg" alt="Blog Cover">
</figure>

    <article class="post-content">
        <p>Welcome to configuring Ubuntu Server 22.04 as a router tutorial series!</p>

        <ol>
            <li><a href="https://hemimorphite.github.io/2023/08/08/setup-ubuntu-server-22-04-as-a-nat-router-with-qemu-kvm/">Setup Ubuntu Server 22.04 as a NAT Router with Qemu/KVM, Part 1</a></li>
            <li class="active"><a href="https://hemimorphite.github.io/2023/08/12/setup-ubuntu-server-22.04-as-a-dhcp-server/">Setup Ubuntu Server 22.04 as a DHCP Server with Qemu/KVM, Part 2</a></li>
            <li><a href="https://hemimorphite.github.io/2023/08/13/setup-ubuntu-server-22.04-as-a-dns-server/">Setup Ubuntu Server 22.04 as a DNS Server with Qemu/KVM, Part 3</a></li>
        </ol>

        <p>One of the most popular programs for Linux operating systems providing DHCP service is dnsmasq.</p>
        
        <p>Install dnsmasq:</p>

        <pre><code class="language-bash hljs">sudo apt -y install dnsmasq</code></pre>

        <p>The response should look similar to this:</p>
        
        <pre><code class="language-bash hljs">Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
The following additional packages will be installed:
  dns-root-data dnsmasq-base
Suggested packages:
  resolvconf
The following NEW packages will be installed:
  dns-root-data dnsmasq dnsmasq-base
0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.
Need to get 379 kB of archives.
After this operation, 974 kB of additional disk space will be used.
Get:1 http://id.archive.ubuntu.com/ubuntu jammy/main amd64 dns-root-data all 2021011101 [5256 B]
Get:2 http://id.archive.ubuntu.com/ubuntu jammy-updates/main amd64 dnsmasq-base amd64 2.86-1.1ubuntu0.3 [354 kB]
Get:3 http://id.archive.ubuntu.com/ubuntu jammy-updates/universe amd64 dnsmasq all 2.86-1.1ubuntu0.3 [19.2 kB]
Fetched 379 kB in 1s (575 kB/s)
debconf: delaying package configuration, since apt-utils is not installed
Selecting previously unselected package dns-root-data.
(Reading database ... 67590 files and directories currently installed.)
Preparing to unpack .../dns-root-data_2021011101_all.deb ...
Unpacking dns-root-data (2021011101) ...
Selecting previously unselected package dnsmasq-base.
Preparing to unpack .../dnsmasq-base_2.86-1.1ubuntu0.3_amd64.deb ...
Unpacking dnsmasq-base (2.86-1.1ubuntu0.3) ...
Selecting previously unselected package dnsmasq.
Preparing to unpack .../dnsmasq_2.86-1.1ubuntu0.3_all.deb ...
Unpacking dnsmasq (2.86-1.1ubuntu0.3) ...
Setting up dnsmasq-base (2.86-1.1ubuntu0.3) ...
Setting up dns-root-data (2021011101) ...
Setting up dnsmasq (2.86-1.1ubuntu0.3) ...
Created symlink /etc/systemd/system/multi-user.target.wants/dnsmasq.service  /lib/systemd/system/dnsmasq.service.
Job for dnsmasq.service failed because the control process exited with error code.
See "systemctl status dnsmasq.service" and "journalctl -xeu dnsmasq.service" for details.
invoke-rc.d: initscript dnsmasq, action "start" failed.
 dnsmasq.service - dnsmasq - A lightweight DHCP and caching DNS server
     Loaded: loaded (/lib/systemd/system/dnsmasq.service; enabled; vendor preset: enabled)
     Active: failed (Result: exit-code) since Fri 2023-08-11 16:02:30 UTC; 12ms ago
    Process: 1011 ExecStartPre=/etc/init.d/dnsmasq checkconfig (code=exited, status=0/SUCCESS)
    Process: 1032 ExecStart=/etc/init.d/dnsmasq systemd-exec (code=exited, status=2)
        CPU: 30ms

Aug 11 16:02:30 metapod systemd[1]: Starting dnsmasq - A lightweight DHCP and caching DNS server...
Aug 11 16:02:30 metapod dnsmasq[1032]: dnsmasq: failed to create listening socket for port 53: Address already in use
Aug 11 16:02:30 metapod dnsmasq[1032]: failed to create listening socket for port 53: Address already in use
Aug 11 16:02:30 metapod dnsmasq[1032]: FAILED to start up
Aug 11 16:02:30 metapod systemd[1]: dnsmasq.service: Control process exited, code=exited, status=2/INVALIDARGUMENT
Aug 11 16:02:30 metapod systemd[1]: dnsmasq.service: Failed with result 'exit-code'.
Aug 11 16:02:30 metapod systemd[1]: Failed to start dnsmasq - A lightweight DHCP and caching DNS server.
Processing triggers for dbus (1.12.20-2ubuntu4.1) ...
debconf: unable to initialize frontend: Dialog
debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)
debconf: falling back to frontend: Readline
Scanning processes...                                                                                                     
Scanning linux images...                                                                                                  

Running kernel seems to be up-to-date.

No services need to be restarted.

No containers need to be restarted.

No user sessions are running outdated binaries.

No VM guests are running outdated hypervisor (qemu) binaries on this host.</code></pre>

        <p>If it complain about "dnsmasq: failed to create listening socket for port 53: Address already in use", you can ignore it.</p>

        <p>To enable DHCP service, you need to configure <code>/etc/dnsmasq.conf</code>.</p>

        <p>By default dnsmasq enables DNS service. You can turn it off by changing the port to 0:</p>

        <pre><code class="language-bash hljs">port=0</code></pre>
        
        <p>The DHCPv4 server is activated by specifying an IPv4 address range and a router:</p>

        <pre><code class="language-bash hljs">dhcp-range=set:enp0s5v4,172.16.0.2,172.16.0.254,255.255.255.0,12h

dhcp-option=tag:enp0s5v4,option:router,172.16.0.1</code></pre>

        <p>The above configuration instructs dnsmasq to offer IPv4 addresses between <code>172.16.0.2</code> and <code>172.16.0.254</code> with a subnet <code>255.255.255.0</code> on the interface <code>enp0s5</code>. Issued IPs will have a lease lifetime of twelve hours, after which clients will need to request a renewed lease.</p>

        <p>Restart the dnsmasq service to apply your changes:</p>

        <pre><code class="language-bash hljs">sudo systemctl restart dnsmasq</code></pre>        

        <p>By default DHCP uses UDP ports 68 and 67 to initiate communication between the IPv4 client and server. If port 67 is in use by another process, DHCP server cannot communicate with DHCPv4 clients.</p>

        <p>To open UDP port 67 in ufw, run:</p>

        <pre><code class="language-bash hljs">sudo ufw allow 67/udp</code></pre>

        <p>Dnsmasq also provides full IPv6 support.</p>
                
        <p>The DHCPv6 server is activated by specifying an IPv6 address range and enabling IPv6 Router Advertisement feature:</p>

        <pre><code class="language-bash hljs">dhcp-range=set:enp0s5v6,::2,::ffff,constructor:enp0s5,slaac,64,12h

enable-ra</code></pre>
        
        <p>The above configuration instructs dnsmasq to offer IPv6 addresses between <code>fde0:fa74:a7a2:87e4::2</code> and <code>fde0:fa74:a7a2:87e4::ffff</code> with prefix length 64 bits on the interface <code>enp0s5</code>.</p>

        <p>Restart the dnsmasq service to apply your changes:</p>

        <pre><code class="language-bash hljs">sudo systemctl restart dnsmasq</code></pre>

        <p>By default DHCP uses UDP ports 546 and 547 to initiate communication between the IPv6 client and server. If port 547 is in use by another process, DHCP server cannot communicate with DHCPv6 clients.</p>

        <p>To open UDP port 547 in ufw, run:</p>

        <pre><code class="language-bash hljs">sudo ufw allow 547/udp</code></pre>

        <p>Now the client computers should obtain IPv4 address and IPv6 address automatically.</p>

        <figure class="post-figure">
            <img src="/assets/images/dhcpserver-01.jpg" alt="configuring Ubuntu Server 22.04 as DHCP server">
        </figure>

    </article>

    <div class="post-tags">
	<div class="title">Tags</div>
	<ul class="tags">
		
		<li><a href="https://hemimorphite.github.io/tag/qemu" class="tag">qemu</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/linux" class="tag">linux</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/ubuntu-22-04" class="tag">ubuntu 22.04</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/ufw" class="tag">ufw</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/dhcp" class="tag">DHCP</a></li>
		
	</ul>
</div>


    <div class="post-share">
    <div class="title">Share this post</div>
    <ul class="rounded-social-buttons">
        <li><a href="https://www.facebook.com/sharer/sharer.php?u=https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/" class="social-button facebook"><i class="fab fa-facebook-f"></i></a></li>
        <li><a href="http://twitter.com/share?text=Hey+guys%2c+check+this+out!&amp;url=https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/&amp;hashtags=qemu,linux,fedora 38,firewalld,DHCP" class="social-button twitter"><i class="fab fa-twitter"></i></a></li>
        <li><a href="https://www.linkedin.com/sharing/share-offsite/?url=https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/" class="social-button linkedin"><i class="fab fa-linkedin"></i></a></li>
    </ul>
</div>
</div>]]></content><author><name>Samuel Yang</name></author><category term="Tutorials" /><category term="year-2023" /><category term="month-08" /><category term="day-12" /><category term="qemu" /><category term="linux" /><category term="ubuntu 22.04" /><category term="ufw" /><category term="DHCP" /><summary type="html"><![CDATA[This tutorial will guide you through the process of configuring Ubuntu Server 22.04 to function as a DHCP server. Learn step-by-step instructions to set up and optimize your Ubuntu server as a DHCP server.]]></summary></entry><entry xml:lang="en"><title type="html">Setup Ubuntu Server 22.04 as a NAT Router with Qemu/KVM, Part 1</title><link href="https://hemimorphite.github.io/2023/08/08/setup-ubuntu-server-22-04-as-a-nat-router-with-qemu-kvm/" rel="alternate" type="text/html" title="Setup Ubuntu Server 22.04 as a NAT Router with Qemu/KVM, Part 1" /><published>2023-08-08T15:38:59+00:00</published><updated>2023-08-08T15:38:59+00:00</updated><id>https://hemimorphite.github.io/2023/08/08/setup-ubuntu-server-22-04-as-a-nat-router-with-qemu-kvm</id><content type="html" xml:base="https://hemimorphite.github.io/2023/08/08/setup-ubuntu-server-22-04-as-a-nat-router-with-qemu-kvm/"><![CDATA[<div class="blog-post">
    <h2 class="post-title">Setup Ubuntu Server 22.04 as a NAT Router with Qemu/KVM, Part 1</h2>
<div class="post-author">
    <span class="avatar"></span>
    <span class="info"><span class="date">Published August 08, 2023</span><br><span class="name">By Samuel Yang</span></span>
</div>

<figure class="post-image">
    <img src="/assets/images/ubuntu2204.jpg" alt="Blog Cover">
</figure>

    <article class="post-content">
        <p>Welcome to configuring Ubuntu Server 22.04 as a router tutorial series!</p>

        <ol>
            <li class="active"><a href="https://hemimorphite.github.io/2023/08/08/setup-ubuntu-server-22-04-as-a-nat-router-with-qemu-kvm/">Setup Ubuntu Server 22.04 as a NAT Router with Qemu/KVM, Part 1</a></li>
            <li><a href="https://hemimorphite.github.io/2023/08/12/setup-ubuntu-server-22.04-as-a-dhcp-server/">Setup Ubuntu Server 22.04 as a DHCP Server with Qemu/KVM, Part 2</a></li>
            <li><a href="https://hemimorphite.github.io/2023/08/13/setup-ubuntu-server-22.04-as-a-dns-server/">Setup Ubuntu Server 22.04 as a DNS Server with Qemu/KVM, Part 3</a></li>
        </ol>

        <p>This is how it will look like the virtual network we are going to build:</p>
        
        <figure class="post-figure">
            <img src="/assets/images/nat-01.jpg" alt="configuring Ubuntu Server 22.04 as a NAT router">
        </figure>

        <ul>
            <li><b>enp0s4</b>: WAN interface</li>
            <li><b>enp0s5</b>: LAN interface with a ipv4 subnet prefix address 172.16.0.0/24</li>
            <li><b>192.168.0.15</b>: the first IPv4 address of Ubuntu Server 22.04 as WAN IPv4 address</li>
            <li><b>172.16.0.1</b>: the second IPv4 address of Ubuntu Server 22.04 as LAN IPv4 Gateway</li>
            <li><b>172.16.0.11</b>: the IPv4 address of the first client</li>
            <li><b>172.16.0.12</b>: the IPv4 address of the second client</li>
        </ul>

        <p>First, we need to have three qemu images:</p>

        <ul>
            <li>Ubuntu Server 22.04 distro as a router gateway</li>
            <li>Ubuntu Desktop 22.04 distro as a client 1</li>
            <li>Ubuntu Desktop 22.04 distro as a client 2</li>
        </ul>

        <p>Then, we need set up a linux bridge (which will act as a layer 2 switch) and three tap interfaces on the host computer to connect three qemu images to each other.</p>

        <p>We can create a new tap interface using <code>tunctl</code> command. In order to use <code>tunctl</code> command, you need to install <code>uml-utilities</code> package on Ubuntu:</p>

        <pre><code class="language-bash hljs">sudo apt install uml-utilities</code></pre>

        <p>Or <code>tunctl</code> package on Fedora:</p>

        <pre><code class="language-bash hljs">sudo dnf install tunctl</code></pre>

        <p>We need to create a tap interface for each of the virtual machines, then we will need to create three different tap interfaces:</p>

        <pre><code class="language-bash hljs">sudo tunctl -u $USER -t tap1
sudo tunctl -u $USER -t tap2
sudo tunctl -u $USER -t tap3</code></pre>

        <p>Next, we bring the tap interfaces up:</p>

        <pre><code class="language-bash hljs">sudo ip link set dev tap1 up 
sudo ip link set dev tap2 up
sudo ip link set dev tap3 up</code></pre>
        
        <p>Then, we create a network bridge using <code>brctl</code> command:</p>

        <pre><code class="language-bash hljs">sudo brctl addbr br0</code></pre>

        <p>Bring up the network bridge:</p>

        <pre><code class="language-bash hljs">sudo ip link set dev br0 up</code></pre>

        <p>Then we attach the tap interfaces to the bridge interface:</p>

        <pre><code class="language-bash hljs">sudo brctl addif br0 tap1
sudo brctl addif br0 tap2
sudo brctl addif br0 tap3</code></pre>
        
        <p>Start the Ubuntu Server 22.04 virtual machine by specifying two network interfaces (WAN interface and LAN interface) with unique mac addresses:</p>
        
        <pre><code class="language-bash hljs">qemu-system-x86_64 -name "Ubuntu Server 22.04 Router Gateway" \
-machine type=pc-q35-2.12 -accel kvm \
-m 4G -cpu host \
-display sdl \
-bios /usr/share/ovmf/OVMF.fd \
-device virtio-vga,addr=01.0 \
-drive file=ubunturouter.img,if=none,id=drive0 \
-device nvme,serial=364740043439,addr=02.0,bus=pcie.0,drive=drive0 \
-netdev user,id=net0,ipv4=on,net=192.168.0.0/24,ipv6=on,ipv6-net=fdc8:45c7:c72b:b1e5::/64,dns=192.168.0.1,ipv6-dns=fdc8:45c7:c72b:b1e5::1 \
-device e1000-82545em,addr=04.0,bus=pcie.0,mac=38:24:10:62:34:78,netdev=net0 \
-netdev tap,id=net1,ifname="tap1",script=no,downscript=no \
-device e1000-82545em,addr=05.0,bus=pcie.0,mac=98:73:37:90:34:57,netdev=net1</code></pre>
        
        <p>List the network interfaces using <code>ip</code> command:</p>

        <pre><code class="language-bash hljs">ip addr show</code></pre>

        <figure class="post-figure">
            <img src="/assets/images/nat-02.jpg" alt="configuring Ubuntu Server 22.04 as a NAT router">
        </figure>

        <p>WAN interface <code>enp0s4</code> sets to DHCP and LAN interface <code>enp0s5</code> sets to static IP.</p>

        <p>Create a network configuration file for WAN interface, <code>10-enp0s4.network</code>, in the <code>/etc/systemd/network/</code> directory.</p>

        <pre><code class="language-bash hljs">[Match]
Name=enp0s4

[Network]
DHCP=yes</code></pre>

        <p>And create a network configuration file for LAN interface, <code>10-enp0s5.network</code>, in the <code>/etc/systemd/network/</code> directory.</p>

        <pre><code class="language-bash hljs">[Match]
Name=enp0s5

[Network]
DHCP=no
Address=172.16.0.1/24
Address=fde0:fa74:a7a2:87e4::1/64</code></pre>        
        
        <p>To apply the configuration, run:</p>

        <pre><code class="language-bash hljs">sudo systemctl restart systemd-networkd</code></pre>

        <p>Then, we install <code>ufw</code> firewall:</p>

        <pre><code class="language-bash hljs">sudo apt install ufw</code></pre>

        <p>To enable UFW, use this command:</p>

        <pre><code class="language-bash hljs">sudo ufw enable</code></pre>

        <p>Next, we need to enable IP Masquerading. The purpose of IP Masquerading is to allow machines with private IP addresses on your network to access the Internet through the machine doing the masquerading.</p>

        <p>Now add rules to the <code>/etc/ufw/before.rules</code> file. The default rules only configure the filter table, and to enable masquerading the nat table will need to be configured. Add the following to the end of the file just after the <code>COMMIT</code>:</p>

        <pre><code class="language-bash hljs"># nat Table rules
*nat
:POSTROUTING ACCEPT [0:0]

# Forward traffic from eth1 through enp0s4.
-A POSTROUTING -o enp0s4 -j MASQUERADE

# don't delete the 'COMMIT' line or these nat table rules won't be processed
COMMIT</code></pre> 
        
        <p>IP forwarding plays a fundamental role on a router. This is the functionality that allows a router to forward traffic from one network interface to another network interface.</p>
        
        <p>Packet forwarding needs to be enabled in ufw and ensure that ufw is configured to support IPv6 so that it will manage firewall rules for IPv6 in addition to IPv4. Edit <code>/etc/default/ufw</code> change the <code>DEFAULT_FORWARD_POLICY</code> to <code>"ACCEPT"</code> and <code>IPV6</code> to <code>yes</code></p>

        <p>Finally, disable and re-enable ufw to apply the changes:</p>

        <pre><code class="language-bash hljs">sudo ufw disable && sudo ufw enable</code></pre>               

        <p>Then, we also need to IP forwarding on the Ubuntu Server system by editing <code>/etc/sysctl.conf</code> and uncomment the following line:.</p>        

        <pre><code class="language-bash hljs">net.ipv4.ip_forward=1

net.ipv6.conf.default.forwarding=1</code></pre>        

        <p>Next, execute the <code>sysctl</code> command to enable the new settings in the configuration file:</p>

        <pre><code class="language-bash hljs">sudo sysctl -p /etc/sysctl.conf</code></pre>

        <p>Dynamic Host Configuration Protocol or DHCP is an IP network protocol that relies on client-server architecture to automatically set IP addresses and other attributes to an IP host to enable information transfer between network nodes.</p>

        <p>DHCPv4 servers have a UDP port number of 67, so listen for messages addressed to this port number. On the other hand, DHCPv4 clients have the UDP port number 68 and only respond to messages sent to number 68.</p>

        <p>DHCPv4 and DHCPv6 UDP port numbers are different. DHCPv6 servers have a UDP port number of 547 and DHCPv6 clients have the UDP port number 546.</p>

        <p>In order to enable communication between the built-in DHCPv4 server of <code>systemd-networkd</code> on UDP port 67 and the built-in DHCPv4 client of <code>systemd-networkd</code> on UDP port 68, you need to open UDP port 67 in ufw:</p>

        <pre><code class="language-bash hljs">sudo ufw allow 67/udp</code></pre>

        <p>The same as DHCPv6 server of <code>systemd-networkd</code> on UDP port 547 and the built-in DHCPv6 client of <code>systemd-networkd</code> on UDP port 546, you need to open UDP port 547 in ufw:</p>

        <pre><code class="language-bash hljs">sudo ufw allow 547/udp</code></pre>

        <p>Start the Ubuntu Desktop 22.04 virtual machines by specifying one network interface (only LAN interface) with unique mac addresses:</p>

        <pre><code class="language-bash hljs">qemu-system-x86_64 -name "Ubuntu Desktop 22.04 Client 1" \
-machine type=pc-q35-2.12 -accel kvm \
-m 4G -cpu host \
-display sdl \
-bios /usr/share/ovmf/OVMF.fd \
-device virtio-vga,addr=01.0 \
-drive file=ubuntuclient1.img,if=none,id=drive0 \
-device nvme,serial=364740043439,addr=02.0,bus=pcie.0,drive=drive0 \
-netdev tap,id=net0,ifname="tap2",script=no,downscript=no \
-device e1000-82545em,addr=04.0,bus=pcie.0,mac=68:98:35:90:34:56,netdev=net0</code></pre>
        
        <pre><code class="language-bash hljs">qemu-system-x86_64 -name "Ubuntu Desktop 22.04 Client 2" \
-machine type=pc-q35-2.12 -accel kvm \
-m 4G -cpu host \
-display sdl \
-bios /usr/share/ovmf/OVMF.fd \
-device virtio-vga,addr=01.0 \
-drive file=ubuntuclient1.img,if=none,id=drive0 \
-device nvme,serial=364740043439,addr=02.0,bus=pcie.0,drive=drive0 \
-netdev tap,id=net0,ifname="tap3",script=no,downscript=no \
-device e1000-82545em,addr=04.0,bus=pcie.0,mac=82:54:65:76:38:28,netdev=net0</code></pre>
        
        <p>Create a network configuration file for LAN interface, <code>10-enp0s4.network</code>, in the <code>/etc/systemd/network/</code> directory.</p>

        <p>On the first client:</p>

        <pre><code class="language-bash hljs">[Match]
Name=enp0s4

[Network]
DHCP=no
Address=172.16.0.11/24
Gateway=172.16.0.1
Address=fde0:fa74:a7a2:87e4::11/64
Gateway=fde0:fa74:a7a2:87e4::1</code></pre>
        
        <p>On the second client:</p>

        <pre><code class="language-bash hljs">[Match]
Name=enp0s4

[Network]
DHCP=no
Address=172.16.0.12/24
Gateway=172.16.0.1
Address=fde0:fa74:a7a2:87e4::12/64
Gateway=fde0:fa74:a7a2:87e4::1</code></pre>
        
        <p>To apply the configuration, run:</p>

        <pre><code class="language-bash hljs">sudo systemctl restart systemd-networkd</code></pre>

        <p>Configure <code>systemd-resolved</code> to use Google Public DNS so your system can have internet access. Edit <code>/etc/systemd/resolved.conf</code>, uncomment and change the <code>DNS</code> to <code>8.8.8.8</code></p>

        <pre><code class="language-bash hljs">DNS=8.8.8.8</code></pre>        

        <p>To apply the configuration, run:</p>

        <pre><code class="language-bash hljs">sudo systemctl restart systemd-resolved</code></pre>

        <p>Try to ping google.com to check internet connectivity. You should have internet access now.</p>
    </article>

    <div class="post-tags">
	<div class="title">Tags</div>
	<ul class="tags">
		
		<li><a href="https://hemimorphite.github.io/tag/qemu" class="tag">qemu</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/linux" class="tag">linux</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/ubuntu-22-04" class="tag">ubuntu 22.04</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/ufw" class="tag">ufw</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/nat" class="tag">NAT</a></li>
		
	</ul>
</div>


    <div class="post-share">
    <div class="title">Share this post</div>
    <ul class="rounded-social-buttons">
        <li><a href="https://www.facebook.com/sharer/sharer.php?u=https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/" class="social-button facebook"><i class="fab fa-facebook-f"></i></a></li>
        <li><a href="http://twitter.com/share?text=Hey+guys%2c+check+this+out!&amp;url=https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/&amp;hashtags=qemu,linux,fedora 38,firewalld,DHCP" class="social-button twitter"><i class="fab fa-twitter"></i></a></li>
        <li><a href="https://www.linkedin.com/sharing/share-offsite/?url=https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/" class="social-button linkedin"><i class="fab fa-linkedin"></i></a></li>
    </ul>
</div>
</div>]]></content><author><name>Samuel Yang</name></author><category term="Tutorials" /><category term="year-2023" /><category term="month-08" /><category term="day-08" /><category term="qemu" /><category term="linux" /><category term="ubuntu 22.04" /><category term="ufw" /><category term="NAT" /><summary type="html"><![CDATA[This tutorial will guide you through the process of configuring Ubuntu Server 22.04 to function as a NAT router. Learn step-by-step instructions to set up and optimize your Ubuntu server as a NAT router.]]></summary></entry><entry xml:lang="en"><title type="html">Getting Started with Qemu amd64 on Ubuntu 22.04</title><link href="https://hemimorphite.github.io/2023/07/31/getting-started-with-qemu-on-ubuntu-22.04/" rel="alternate" type="text/html" title="Getting Started with Qemu amd64 on Ubuntu 22.04" /><published>2023-07-31T07:38:59+00:00</published><updated>2023-07-31T07:38:59+00:00</updated><id>https://hemimorphite.github.io/2023/07/31/getting-started-with-qemu-on-ubuntu-22.04</id><content type="html" xml:base="https://hemimorphite.github.io/2023/07/31/getting-started-with-qemu-on-ubuntu-22.04/"><![CDATA[<div class="blog-post">
    <h2 class="post-title">Getting Started with Qemu amd64 on Ubuntu 22.04</h2>
<div class="post-author">
    <span class="avatar"></span>
    <span class="info"><span class="date">Published July 31, 2023</span><br><span class="name">By Samuel Yang</span></span>
</div>

<figure class="post-image">
    <img src="/assets/images/qemu.jpg" alt="Blog Cover">
</figure>

    <article class="post-content">
        <p>Installing QEMU on Ubuntu is pretty straightforward, run:</p>
        
        <pre><code class="language-bash hljs">sudo apt install qemu qemu-kvm</code></pre>

        <p>Run the following command to create an qcow2 image file 100GB in size in the current directory:</p>

        <pre><code class="language-bash hljs">qemu-img create -f qcow2 ubuntu.img 100G</code></pre>

        <p>Next, run the following command to start the Ubuntu installation:</p>

        <pre><code class="language-bash hljs">qemu-system-x86_64 -name "Ubuntu Server 22.04" \
-machine type=pc-q35-2.12 -accel kvm \
-m 4G -cpu host \
-display sdl \
-bios /usr/share/ovmf/OVMF.fd \
-boot menu=on \
-device virtio-vga,addr=01.0 \
-drive file=ubuntu.img,if=none,id=drive0 \
-device nvme,serial=694740036639,addr=02.0,bus=pcie.0,drive=drive0 \
-drive file=ubuntu-22.04.2-live-server-amd64.iso,format=raw,if=none,id=drive1 \
-device ich9-ahci,id=ahci0,addr=03.0,bus=pcie.0 \
-device ide-cd,serial=278674617506,drive=drive1,bus=ahci0.0 \
-netdev user,id=net0,ipv4=on,net=172.16.0.0/24,ipv6=on,ipv6-net=fdcc:7647:d52a:2cff::/64,dns=172.16.0.1,ipv6-dns=fdcc:7647:d52a:2cff::1 \
-device e1000-82545em,addr=04.0,bus=pcie.0,mac=b6:94:1b:12:34:56,netdev=net0</code></pre>

        <ul>
            <li><code>-name "Ubuntu Server 22.04"</code> specifies the name of the running guest system. The name is displayed in the window caption</li>
            <li>
                <span>
                <code>-machine type=pc-q35-2.12</code> specifies the type of the emulated machine.<br>
                Run <code>qemu-system-x86_64 -machine help</code> to view a list of supported machine types.<br>
                Run <code>qemu-system-x86_64 -machine type=pc-q35-2.12,help</code> to list all available properties of <code>pc-q35-2.12</code> machine.
                </span>     
            </li>
            <li><code>-accel kvm</code> specifies to use the hardware acceleration provided by kvm</li>
            <li><code>-boot menu=on</code> specifies manually to select boot drive</li>
            <li><code>-bios /usr/share/ovmf/OVMF.fd</code> specifies to boot with the TianoCore UEFI firmware</li>
            <li>
                <span>
                <code>-cpu host</code> specifies to emulate the host processor<br>
                Run <code>qemu-system-x86_64 -cpu help</code> to view a list of supported processors
                </span>
            </li> 
            <li><code>-smp 2</code> specifies how many CPUs will be emulated</li>
            
            <li><code>-m 4G</code> specifies the amount of memory (M stands for Megabyte, G for Gigabyte)</li>
            
            <li><code>-display sdl</code> specifies to display video output via SDL (usually in a separate graphics window)</li>

            <li>
                <span>
                <code>-device VGA,addr=01.0</code> specifies the type of the video card.<br>
                Run <code>qemu-system-x86_64 -device help</code> to view a list of video card types.<br>
                Run <code>qemu-system-x86_64 -device VGA,help</code> to list all available properties of <code>VGA</code> video card.
                </span>
            </li>
            
            <li><code>-drive file=ubuntu.img,if=none,id=drive0</code> specifies a virtual hard drive and use <code>ubuntu.img</code> image file</li>

            <li><code>-device nvme,serial=694740036639,addr=02.0,bus=pcie.0,drive=drive0</code> specifies to add a nvme storage device with serial number 694740036639 and PCIe address 02.0 (device number 02 and function number 0) on <code>pcie.0</code> bus (Root Bus)</li>
            
            <li><code>-drive file=ubuntu-22.04.2-live-server-amd64.iso,format=raw,if=none,id=drive1</code> specifies a virtual hard drive and use <code>ubuntu-22.04.2-live-server-amd64.iso</code> image file</li>

            <li><code>-device ich9-ahci,id=ahci0,addr=03.0,bus=pcie.0</code> specifies a virtual hard drive and use <code>ubuntu-22.04.2-live-server-amd64.iso</code> image file</li>

            <li><code>-device ide-cd,serial=278674617506,drive=drive1,bus=ahci0.0</code> specifies a virtual ide disk with serial number 278674617506 and ahci bus</li>

            <li><code>-netdev user,id=net0,ipv4=on,net=172.16.0.0/24,ipv6=on,ipv6-net=fdcc:7647:d52a:2cff::/64,dns=172.16.0.1,ipv6-dns=fdcc:7647:d52a:2cff::1</code> specifies to use user-mode networking with a built-in DHCPv4 server in the address range 172.16.0.0/24, a built-in DHCPv6 server in the address range fdcc:7647:d52a:2cff::/64, a IPv4 DNS Server 172.16.0.1 and a IPv6 DNS Server fdcc:7647:d52a:2cff::1</li>

            <li><code>-device e1000-82545em,addr=04.0,bus=pcie.0,mac=b6:94:1b:12:34:56,netdev=net0</code> specifies a virtual network device with mac address b6:94:1b:12:34:56 and PCIe address 04.0 (device number 04 and function number 0) on <code>pcie.0</code> bus (Root Bus)</li>
        </ul>

        <p>If you stopped the virtual machine after the installation or rebooted your system, you can start the existing virtual machine again with almost the same command as for the installation. We just don't need to specify the ISO image file this time.</p>

        <pre><code class="language-bash hljs">qemu-system-x86_64 -name "Ubuntu Server 22.04" \
-machine type=pc-q35-2.12 -accel kvm \
-m 4G -cpu host \
-display sdl \
-bios /usr/share/ovmf/OVMF.fd \
-device VGA,addr=01.0 \
-drive file=ubuntus01.img,if=none,id=drive0 \
-device nvme,serial=694740036639,addr=02.0,bus=pcie.0,drive=drive0 \
-netdev user,id=net0,ipv4=on,net=172.16.0.0/24,ipv6=on,ipv6-net=fdcc:7647:d52a:2cff::/64,dns=172.16.0.1,ipv6-dns=fdcc:7647:d52a:2cff::1 \
-device e1000-82545em,addr=04.0,bus=pcie.0,mac=b6:94:1b:12:34:56,netdev=net0</code></pre>

    </article>

    <div class="post-tags">
	<div class="title">Tags</div>
	<ul class="tags">
		
		<li><a href="https://hemimorphite.github.io/tag/qemu" class="tag">qemu</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/linux" class="tag">linux</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/ubuntu-22-04" class="tag">ubuntu 22.04</a></li>
		
	</ul>
</div>


    <div class="post-share">
    <div class="title">Share this post</div>
    <ul class="rounded-social-buttons">
        <li><a href="https://www.facebook.com/sharer/sharer.php?u=https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/" class="social-button facebook"><i class="fab fa-facebook-f"></i></a></li>
        <li><a href="http://twitter.com/share?text=Hey+guys%2c+check+this+out!&amp;url=https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/&amp;hashtags=qemu,linux,fedora 38,firewalld,DHCP" class="social-button twitter"><i class="fab fa-twitter"></i></a></li>
        <li><a href="https://www.linkedin.com/sharing/share-offsite/?url=https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/" class="social-button linkedin"><i class="fab fa-linkedin"></i></a></li>
    </ul>
</div>
</div>]]></content><author><name>Samuel Yang</name></author><category term="Tutorials" /><category term="year-2023" /><category term="month-07" /><category term="day-31" /><category term="qemu" /><category term="linux" /><category term="ubuntu 22.04" /><summary type="html"><![CDATA[This tutorial provides a step-by-step guide on how to get started with Qemu amd64 on Ubuntu 22.04. Learn how to set up and use Qemu to run virtual machines on your Ubuntu system.]]></summary></entry><entry xml:lang="en"><title type="html">Create a Kubernetes Cluster with Kubeadm, Containerd, Calico, MetalLB, and NGINX Ingress Controller on Bare Metal Machine Fedora 38</title><link href="https://hemimorphite.github.io/2023/07/27/create-a-kubernetes-cluster-with-kubeadm-containerd-calico-metallb-and-nginx-ingress-controller-on-bare-metal-machine-fedora-38/" rel="alternate" type="text/html" title="Create a Kubernetes Cluster with Kubeadm, Containerd, Calico, MetalLB, and NGINX Ingress Controller on Bare Metal Machine Fedora 38" /><published>2023-07-27T09:30:50+00:00</published><updated>2023-07-27T09:30:50+00:00</updated><id>https://hemimorphite.github.io/2023/07/27/create-a-kubernetes-cluster-with-kubeadm-containerd-calico-metallb-and-nginx-ingress-controller-on-bare-metal-machine-fedora-38</id><content type="html" xml:base="https://hemimorphite.github.io/2023/07/27/create-a-kubernetes-cluster-with-kubeadm-containerd-calico-metallb-and-nginx-ingress-controller-on-bare-metal-machine-fedora-38/"><![CDATA[<div class="blog-post">
    <h2 class="post-title">Create a Kubernetes Cluster with Kubeadm, Containerd, Calico, MetalLB, and NGINX Ingress Controller on Bare Metal Machine Fedora 38</h2>
<div class="post-author">
    <span class="avatar"></span>
    <span class="info"><span class="date">Published July 27, 2023</span><br><span class="name">By Samuel Yang</span></span>
</div>

<figure class="post-image">
    <img src="/assets/images/kubernetes.jpg" alt="Blog Cover">
</figure>

    <article class="post-content">
        <p>A Kubernetes cluster has two main components: the control plane (master node) and data plane (worker node).</p>

        <p>So our cluster will have 1 master node and 1 worker node. The Operating system used for the master node and worker node are Fedora 38.</p>

        <p>We'll assume:</p>

        <ul>
            <li>The first LAN subnet of 172.16.0.0/24</li>
            <li>The second LAN subnet of 114.122.37.0/24</li>
            <li>The first LAN Gateway IP address is 172.16.0.1</li>
            <li>The second LAN Gateway IP address is 114.122.37.1</li>
            <li>The first LAN is for kubernetes nodes</li>
            <li>The second LAN is for local clients</li>
            <li>Master Node's first IP address is 172.16.0.21</li>
            <li>Master Node's second IP address is 114.122.37.100</li>
            <li>Worker Node's IP address is 172.16.0.22</li>
        </ul>

        <h4 class="post-subtitle">Install Fedora Server 38 on Virtualbox</h4>

        <p>Download Fedora image file by visiting <a href="https://fedoraproject.org/server/download/">Fedora's official website</a>.</p>

        <p>Start the virtualbox and click on the <b>New</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-1.jpg" alt="Fedora Server installation">
        </figure>

        <p>Write the name of the VM. Select the OS type and version. Click on <b>Next</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-2.jpg" alt="Fedora Server installation">
        </figure>

        <p>The minimum Kubernetes requirements are: 2 GB or more of RAM and 2 CPUs or more. Check the <b>Enable EFI (special OSes only)</b> and click on <b>Next</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-3.jpg" alt="Fedora Server installation">
        </figure>        

        <p>Select <b>Create a Virtual Hard Disk Now</b> and allocate how much space to your VM. Click on <b>Next</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-4.jpg" alt="Fedora Server installation">
        </figure> 

        <p>Next, click on <b>Finish</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-5.jpg" alt="Fedora Server installation">
        </figure>

        <p>Then, click on <b>Setting</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-6.jpg" alt="Fedora Server installation">
        </figure>

        <p>Select <b>Storage</b> menu and click on <b>Adds optical drive</b>. Navigate to where the Fedora Server ISO image saved. Then click on <b>Ok</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-7.jpg" alt="Fedora Server installation">
        </figure>

        <p>Open terminal and add new nat networks for kubernetes nodes and local clients using <code>VBoxManage</code> command.</p>

        <pre><code class="language-bash hljs">VBoxManage natnetwork add --netname natnet1 --network "172.16.0.0/24" --enable --ipv6 on --dhcp on</code></pre>

        <pre><code class="language-bash hljs">VBoxManage natnetwork add --netname natnet2 --network "114.122.37.0/24" --enable --ipv6 on --dhcp on</code></pre>

        <p>Go back to Virtualbox. Click on <b>Setting</b> button again. Then go to <b>Network</b> menu. Set network adapter 1 type and network adapter 2 type to <b>NAT Network</b>. Then click on <b>Ok</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-8.jpg" alt="Fedora Server installation">
        </figure>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-9.jpg" alt="Fedora Server installation">
        </figure>

        <p>Click on <b>Start</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-10.jpg" alt="Fedora Server installation">
        </figure>

        <p>Select <b>Install Fedora 38</b> on the boot menu.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-11.jpg" alt="Fedora Server installation">
        </figure>

        <p>Select your language and click <b>Continue</b>.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-12.jpg" alt="Fedora Server installation">
        </figure>

        <p>Configure Timezone.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-13.jpg" alt="Fedora Server installation">
        </figure>

        <p>Select your region and city. Then click <b>Done</b>.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-14.jpg" alt="Fedora Server installation">
        </figure>

        <p>Configure Software Selection.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-15.jpg" alt="Fedora Server installation">
        </figure>

        <p>Select minimal install. Then click <b>Done</b>.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-16.jpg" alt="Fedora Server installation">
        </figure>

        <p>Configure installation destination.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-17.jpg" alt="Fedora Server installation">
        </figure>

        <p>On the <b>Storage Configuration</b>, choose <b>custom</b>. Then click <b>Done</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-18.jpg" alt="Fedora Server installation">
        </figure>

        <p>Choose <b>LVM</b> filesystem.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-19.jpg" alt="Fedora Server installation">
        </figure>

        <p>Then click the + symbol to add <b>boot</b> partition and the desired capacity 1GB. Click <b>Add mount point</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-20.jpg" alt="Fedora Server installation">
        </figure>        

        <p>Pick <b>ext4</b> as its default filesystem.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-21.jpg" alt="Fedora Server installation">
        </figure>

        <p>Click the + symbol again to add <b>EFI</b> partition and the desired capacity 512MB. Click <b>Add mount point</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-22.jpg" alt="Fedora Server installation">
        </figure>

        <p>Pick <b>EFI System Partition</b> as its default filesystem.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-23.jpg" alt="Fedora Server installation">
        </figure>

        <p>Click the + symbol again to add <b>root</b> partition and don't need to fill the desired capacity. Click <b>Add mount point</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-24.jpg" alt="Fedora Server installation">
        </figure>

        <p>Pick <b>LVM</b> as its device type and <b>ext4</b> ad its filesystem. Click <b>Done</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-26.jpg" alt="Fedora Server installation">
        </figure>

        <p>A window show up, and click <b>Accept Changes</b>.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-27.jpg" alt="Fedora Server installation">
        </figure>

        <p>Next, configure the user creation.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-28.jpg" alt="Fedora Server installation">
        </figure>

        <p>Create a common user account. Then click <b>Done</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-29.jpg" alt="Fedora Server installation">
        </figure>

        <p>After that, click <b>Begin Installation</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-30.jpg" alt="Fedora Server installation">
        </figure>

        <p>Once the Fedora 38 installation is completed, click <b>Reboot System</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-31.jpg" alt="Fedora Server installation">
        </figure>
        
        <p>Close the virtual machine. Then select <b>Power off the machine</b> and click on <b>Ok</b> button.</p>

        <p>Open terminal, run the following command to clone the VM and make sure not be run as root:</p>

        <pre><code class="language-bash hljs">VBoxManage clonevm "kubemaster" --basefolder=$HOME --mode=machine --name="kubeworker1" --register</code></pre>

        <ul>
            <li><code>"kubemaster"</code> is the VM name to be cloned</li>
            <li><code>--basefolder=$HOME</code> specifies the name of the folder in which to save the configuration for the new VM</li>
            <li><code>--mode=machine</code> specifies to clone the current state of the existing VM without any snapshots</li>
            <li><code>--name="kubeworker1"</code> specifies a new name for the new VM</li>
            <li><code>--register</code> specifies to automatically register the new clone, so it will show up on the Virtualbox Manager without have to be added again</li>
        </ul>

        <p>If the clone is successful, you will see the output similar to the following:</p>
        
        <pre><code class="language-bash hljs">0%...10%...20%...30%...40%...50%...60%...70%...80%...90%...100%
Machine has been successfully cloned as "kubeworker1"</code></pre>

        <h4 class="post-subtitle">Setting Static IP Address on Master Node</h4>

        <p>Use <code>ip</code> command to identify the name of the ethernet interface you want to configure:</p>

        <pre><code class="language-bash hljs">ip addr show</code></pre>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-33.jpg" alt="Fedora Server installation">
        </figure>

        <p>The interface named <code>lo</code> is the loopback interface, used for processes that communicate via the IP protocol. The important interfaces in the listing are <code>enp0s3</code> and <code>enp0s8</code>, the Ethernet interfaces.</p>

        <p>Add static IP address using <code>nmcli</code> command:</p>

        <pre><code class="language-bash hljs">sudo nmcli connection modify enp0s3 ipv4.addresses 172.16.0.21/24</code></pre>

        <pre><code class="language-bash hljs">sudo nmcli connection modify enp0s8 ipv4.addresses 114.122.37.100/24</code></pre>

        <p>Add the gateway IP:</p>

        <pre><code class="language-bash hljs">sudo nmcli connection modify enp0s3 ipv4.gateway 172.16.0.1</code></pre>

        <pre><code class="language-bash hljs">sudo nmcli connection modify enp0s8 ipv4.gateway 114.122.37.1</code></pre>

        <p>Add the dns IP address:</p>

        <pre><code class="language-bash hljs">sudo nmcli connection modify enp0s3 ipv4.dns 172.16.0.1</code></pre>

        <pre><code class="language-bash hljs">sudo nmcli connection modify enp0s8 ipv4.dns 114.122.37.1</code></pre>

        <p>Change the addressing from DHCP to static.</p>

        <pre><code class="language-bash hljs">sudo nmcli connection modify enp0s3 ipv4.method manual</code></pre>

        <pre><code class="language-bash hljs">sudo nmcli connection modify enp0s8 ipv4.method manual</code></pre>

        <p>To make changes into the effect, disable and enable the connection:</p>

        <pre><code class="language-bash hljs">sudo nmcli connection down enp0s3</code></pre>

        <pre><code class="language-bash hljs">sudo nmcli connection up enp0s3</code></pre>

        <pre><code class="language-bash hljs">sudo nmcli connection down enp0s8</code></pre>

        <pre><code class="language-bash hljs">sudo nmcli connection up enp0s8</code></pre>     

        <h4 class="post-subtitle">Setting hostname on Master Node</h4>

        <p>Set the hostname using the following command:</p>

        <pre><code class="language-bash hljs">sudo echo "kubemaster" > /etc/hostname</code></pre>

        <p>Add the following line to <code>/etc/hosts</code> to map the hostname to a IP address:</p>

        <pre><code class="language-bash hljs">172.16.0.21    kubemaster</code></pre>

        <h4 class="post-subtitle">Disable systemd-resolved service on Master Node</h4>

        <p>The <code>systemd-resolved</code> service can be disabled with:</p>

        <pre><code class="language-bash hljs">sudo systemctl stop systemd-resolved
sudo systemctl disable systemd-resolved
sudo systemctl mask systemd-resolved</code></pre>
        
        <p>Delete the symlink <code>/etc/resolv.conf</code></p>

        <pre><code class="language-bash hljs">sudo rm /etc/resolv.conf</code></pre>

        <p>Create a new <code>/etc/resolv.conf</code> file:</p>

        <pre><code class="language-bash hljs">sudo touch /etc/resolv.conf</code></pre>

        <p>Add the following line to <code>/etc/resolv.conf</code>:</p>

        <pre><code class="language-bash hljs">nameserver 172.16.0.1</code></pre>

        <h4 class="post-subtitle">Install containerd (container runtime) on Master Node</h4>

        <p>First, install the required packages to run kubernetes:</p>

        <pre><code class="language-bash hljs">sudo dnf -y install ethtool
sudo dnf -y install socat
sudo dnf -y install iproute-tc
sudo dnf -y install conntrack
sudo dnf -y install openssl
sudo dnf -y install tar
sudo dnf -y install net-tools</code></pre>

        <p>Next, you need to install a container runtime into each node in the cluster so that pods can run there.</p>

        <p>Before install containerd, you need to enable <code>overlay</code> and <code>br_netfilter</code> kernel modules, and <code>net.bridge.bridge-nf-call-iptables</code>, <code>net.bridge.bridge-nf-call-ip6tables</code>, and <code>net.ipv4.ip_forward</code> kernel parameter.</p>

        <p><code>overlay</code> kernel module is required to enable Overlay filesystem support.</p>

        <p><code>br_netfilter</code> kernel module is required to enable transparent masquerading and to facilitate Virtual Extensible LAN (VxLAN) traffic for communication between Kubernetes pods across the cluster.</p>

        <p><code>net.bridge.bridge-nf-call-iptables</code> and <code>net.bridge.bridge-nf-call-ip6tables</code> kernel parameters is required to control whether or not packets traversing the bridge are sent to iptables for processing. In the case of using bridges to connect virtual machines to the network, generally such processing is *not* desired, as it results in guest traffic being blocked due to host iptables rules that only account for the host itself, and not for the guests.</p>

        <p><code>net.ipv4.ip_forward</code> kernel parameter is required to enable IP forwarding.</p>

        <p>Create a file in the <code>/etc/modules-load.d/</code> directory which contains kernel module names to be loaded at boot time.</p>

        <pre><code class="language-bash hljs">cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF</code></pre>

        <p>Load <code>overlay</code> and <code>br_netfilter</code> kernel modules now using the <code>modprobe</code> command.</p>  

        <pre><code class="language-bash hljs">sudo modprobe overlay
sudo modprobe br_netfilter</code></pre>

        <p>Create a file in the <code>/etc/sysctl.d/</code> directory which contains <code>net.bridge.bridge-nf-call-iptables</code>, <code>net.bridge.bridge-nf-call-ip6tables</code>, and <code>net.ipv4.ip_forward</code> kernel parameters to be set at boot time.</p>

        <pre><code class="language-bash hljs">cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF</code></pre>        

        <p>Apply the kernel parameters without reboot using <code>sysctl</code> command.</p>

        <pre><code class="language-bash hljs">sudo sysctl --system</code></pre>

        <p>Verify that the <code>br_netfilter</code>, <code>overlay</code> modules are loaded by running the following commands:</p>

        <pre><code class="language-bash hljs">lsmod | grep br_netfilter
lsmod | grep overlay</code></pre>
        
        <p>Verify that the <code>net.bridge.bridge-nf-call-iptables</code>, <code>net.bridge.bridge-nf-call-ip6tables</code>, and <code>net.ipv4.ip_forward</code> system variables are set to <code>1</code> in your <code>sysctl</code> config by running the following command:</p>

        <pre><code class="language-bash hljs">sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward</code></pre>

        <p>Download the containerd from <a href="https://github.com/containerd/containerd/releases">https://github.com/containerd/containerd/releases</a>.</p>

        <pre><code class="language-bash hljs">curl -OJL https://github.com/containerd/containerd/releases/download/v1.7.2/containerd-1.7.2-linux-amd64.tar.gz</code></pre>
        
        <p>Extract the containerd under <code>/usr/local</code></p>

        <pre><code class="language-bash hljs">sudo tar Cxzvf /usr/local containerd-1.7.2-linux-amd64.tar.gz</code></pre>
        
        <p>Download the <code>containerd.service</code> unit file from <a href="https://raw.githubusercontent.com/containerd/containerd/main/containerd.service">https://raw.githubusercontent.com/containerd/containerd/main/containerd.service</a> into <code>/usr/lib/systemd/system/</code> directory</p>

        <pre><code class="language-bash hljs">curl -o /usr/lib/systemd/system/containerd.service https://raw.githubusercontent.com/containerd/containerd/main/containerd.service</code></pre>

        <p>Reload systemd manager configuration and enable the containerd service.</p>
        
        <pre><code class="language-bash hljs">sudo systemctl daemon-reload
sudo systemctl enable --now containerd</code></pre>
        
        <p>Download the <code>runc</code> binary from <a href="https://github.com/opencontainers/runc/releases">https://github.com/opencontainers/runc/releases</a> and install it as <code>/usr/local/sbin/runc</code>.</p>
        
        <pre><code class="language-bash hljs">curl -OJL https://github.com/opencontainers/runc/releases/download/v1.1.7/runc.amd64
sudo install -m 755 runc.amd64 /usr/local/sbin/runc</code></pre>

        <p>Download the cni-plugins from <a href="https://github.com/containernetworking/plugins/releases">https://github.com/containernetworking/plugins/releases</a> and extract it under <code>/opt/cni/bin</code></p>

        <pre><code class="language-bash hljs">curl -OJL https://github.com/containernetworking/plugins/releases/download/v1.3.0/cni-plugins-linux-amd64-v1.3.0.tgz
sudo mkdir -p /opt/cni/bin
sudo tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.3.0.tgz</code></pre>
        
        <p>Once, we have successfully installed the Containerd. We need to load the Containerd configurations.</p>

        <p>Create <code>/etc/containerd</code> directory and generate the confguration file using <code>containerd</code> command.</p>

        <pre><code class="language-bash hljs">sudo mkdir -p /etc/containerd
sudo containerd config default > /etc/containerd/config.toml</code></pre>

        <p>Open the configuration file <code>/etc/containerd/config.toml</code> and set <code>SystemdCgroup = true</code></p>

        <pre><code class="language-bash hljs">[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
  ...
  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
    SystemdCgroup = true</code></pre> 

        <p>To apply the changes, restart the <code>containerd</code> service.</p>

        <pre><code class="language-bash hljs">sudo systemctl restart containerd</code></pre>

        <h4 class="post-subtitle">Install kubectl, kubeadm, and kubelet on Master Node</h4>

        <p>You will install these packages on all of your nodes (master node and worker node):</p>

        <ul>
            <li><code>kubeadm</code>: the command to bootstrap the cluster</li>
            <li><code>kubelet</code>: the component that runs on all of the machines in your cluster and does things like starting pods and containers</li>
            <li><code>kubectl</code>: the command line util to talk to your cluster.</li>
        </ul>

        <p>Kubernetes versions are expressed as <b>x.y.z</b>, where <b>x</b> is the major version, <b>y</b> is the minor version, and <b>z</b> is the patch version.</p>

        <p><code>kubelet</code>, <code>kube-proxy</code>, <code>kube-controller-manager</code>, <code>kube-scheduler</code>, and <code>cloud-controller-manager</code> version must not be newer than <code>kube-apiserver</code> version.</p>

        <p>The requirements of <code>kubeadm</code> before you begin to install.</p>

        <ul>
            <li>2 GB or more of RAM per node (master node or worker node)</li>
            <li>2 CPUs or more</li>
            <li>Check hostname, machine ID, MAC address, and product_uuid for every node are unique</li>
            <li>Check required ports</li>
            <li>Swap disabled</li>
        </ul>

        <p>You can get the hostname using the command <code>cat /etc/hostname</code>.</p>

        <p>To change the hostname, edit the <code>/etc/hostname</code> file.</p>

        <p>You can get the machine ID using the command <code>cat /etc/machine-id</code>.</p>

        <p>To generate the new machine ID, first remove the <code>/etc/machine-id</code> file. Then generate the new one using this command:</p>

        <pre><code class="language-bash hljs">systemd-machine-id-setup</code></pre>

        <p>You can get the MAC address of the network interfaces using the command <code>ip link</code>.</p>

        <p>To generate the new MAC address of the network interfaces, run the following command on terminal.</p>

        <pre><code class="language-bash hljs">VBoxManage modifyvm "kubemaster" --macaddress1 auto</code></pre>

        <ul>
            <li><code>"kubemaster"</code> is the VM name</li>
            <li><code>--macaddress1</code> is the MAC address of the first network interface</li>
        </ul>

        <p>To list the network interfaces of your VM, run the following command:</p>

        <pre><code class="language-bash hljs">VBoxManage showvminfo kubemaster | grep NIC</code></pre>        
        <p>The output should be similar to:</p>

        <pre><code class="language-bash hljs">NIC 1:                       MAC: 080027EECD2E, Attachment: NAT Network 'natnet1', Cable connected: on, Trace: off (file: none), Type: 82540EM, Reported speed: 0 Mbps, Boot priority: 0, Promisc Policy: deny, Bandwidth group: none
NIC 2:                       disabled
NIC 3:                       disabled
NIC 4:                       disabled
NIC 5:                       disabled
NIC 6:                       disabled
NIC 7:                       disabled
NIC 8:                       disabled</code></pre>
        
        <p>You can get the <code>product_uuid</code> using the command <code>sudo cat /sys/class/dmi/id/product_uuid</code>.</p>

        <p>To generate the new <code>product_uuid</code>, run the command three times on terminal and make sure not be run as root:</p>

        <pre><code class="language-bash hljs">VBoxManage internalcommands sethduuid "$HOME/kubemaster/kubemaster.vdi"
        VBoxManage internalcommands sethduuid "$HOME/kubemaster/kubemaster.vdi"
        VBoxManage internalcommands sethduuid "$HOME/kubemaster/kubemaster.vdi"</code></pre>

        <p>The output should be similar to:</p>

        <pre><code class="language-bash hljs">UUID changed to: 35ddda67-7a61-4c6f-b6ed-31a4a442af8b
UUID changed to: 2fd3c5d1-362e-4db5-a67b-84d67c57cde0
UUID changed to: 23338733-eac6-471e-bbf5-4f9414fcca61</code></pre>
        
        <p>The first UUID will be used as <code>Machine uuid</code> and <code>Hardware uuid</code>.</p>

        <p>Edit <code>$HOME/kubemaster/kubemaster.vbox</code> file. Find the <code>Machine uuid</code> and <code>Hardware uuid</code> and change to <code>35ddda67-7a61-4c6f-b6ed-31a4a442af8b</code>.</p>

        <pre><code class="language-bash hljs">&lt;Machine uuid="{35ddda67-7a61-4c6f-b6ed-31a4a442af8b}" name="kubeworker1" OSType="RedHat_64" snapshotFolder="Snapshots" lastStateChange="2023-07-13T10:05:46Z"&gt;
    ...
&lt;/Machine&gt;</code></pre>

        <pre><code class="language-bash hljs">&lt;Hardware uuid="{35ddda67-7a61-4c6f-b6ed-31a4a442af8b}"&gt;
    ...
&lt;/Hardware&gt;</code></pre>

        <p>The second UUID will be used as <code>System uuid</code>. To set <code>System uuid</code>, run the command:</p>

        <pre><code class="language-bash hljs">VBoxManage setextradata "kubemaster" "VBoxInternal/Devices/efi/0/Config/DmiSystemUuid" "2fd3c5d1-362e-4db5-a67b-84d67c57cde0"</code></pre>
        
        <p>The third UUID will be used as <code>Harddisk uuid</code> and <code>Image uuid</code>.</p>

        <p>Edit <code>$HOME/kubemaster/kubemaster.vbox</code> file. Find the <code>Harddisk uuid</code> and <code>Image uuid</code> and change to <code>941f4e75-d29f-4c58-93d5-53172c07e50e</code>.</p>

        <pre><code class="language-bash hljs">&lt;HardDisks&gt;
        &lt;HardDisk uuid="{941f4e75-d29f-4c58-93d5-53172c07e50e}" location="kubemaster.vdi" format="VDI" type="Normal"/&gt;
      &lt;/HardDisks&gt;</code></pre>

        <pre><code class="language-bash hljs">&lt;StorageController name="NVMe" type="NVMe" PortCount="1" useHostIOCache="false" Bootable="true"&gt;
          &lt;AttachedDevice type="HardDisk" hotpluggable="false" port="0" device="0"&gt;
            &lt;Image uuid="{941f4e75-d29f-4c58-93d5-53172c07e50e}"/&gt;
          &lt;/AttachedDevice&gt;
        &lt;/StorageController&gt;</code></pre>        

        <p>Detach and remove the storage medium from the VM:</p>

        <pre><code class="language-bash hljs">VBoxManage storageattach kubemaster --storagectl "NVMe" --port 0 --device 0 --medium none</code></pre>

        <ul>
            <li><code>fedoraworker1</code> specifies the VM name</li>
            <li><code>--storagectl</code> specifies the name of the storage controller</li>
            <li><code>--port</code> specifies the number of the storage controller's port</li>
            <li><code>--device</code> specifies the number of the port's device </li>
            <li><code>--medium none</code> specifies to remove storage medium</li>
        </ul>

        <p>Remove the storage medium from VirtualBox media registry.</p>

        <pre><code class="language-bash hljs">VBoxManage closemedium disk "$HOME/kubemaster/kubemaster.vdi"</code></pre>

        <p>Then attach the storage medium to the VM:</p>

        <pre><code class="language-bash hljs">VBoxManage storageattach "kubemaster" --storagectl "NVMe" --port 0 --device 0 --type hdd --medium $HOME/kubemaster/kubemaster.vdi</code></pre>                
        
        <p>The ports required for master node are:</p>

        <table class="table table-bordered border-primary">
            <thead>
                <th>Protocol</th>
                <th>Port Range</th>
                <th>Used By</th>
            </thead>
            <tbody>
            <tr>
                <td>TCP</td>
                <td>6443</td>
                <td>kube-apiserver</td>
            </tr>
            <tr>
                <td>TCP</td>
                <td>2379-2380</td>
                <td>kube-apiserver, etcd</td>
            </tr>
            <tr>
                <td>TCP</td>
                <td>10250</td>
                <td>Kubelet API</td>
            </tr>
            <tr>
                <td>TCP</td>
                <td>10259</td>
                <td>kube-scheduler</td>
            </tr>
            <tr>
                <td>TCP</td>
                <td>10257</td>
                <td>kube-controller-manager</td>
            </tr>
            </tbody>
        </table>

        <p>The ports are required to be open in the firewall.</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --add-port=6443/tcp
sudo firewall-cmd --permanent --add-port=2379-2380/tcp
sudo firewall-cmd --permanent --add-port=10250/tcp
sudo firewall-cmd --permanent --add-port=10259/tcp
sudo firewall-cmd --permanent --add-port=10257/tcp
sudo firewall-cmd --permanent --add-port=9153/tcp
sudo firewall-cmd --permanent --add-port=53/tcp
sudo firewall-cmd --permanent --add-port=53/udp
sudo firewall-cmd --permanent --add-masquerade
sudo firewall-cmd --reload</code></pre>
        
        <p>Swap must be disabled in order for the kubelet to work properly. To check if swap is active:</p>

        <ul>
            <li><code>cat /proc/meminfo | grep Swap</code> to see total swap, and free swap</li>
            <li><code>cat /proc/swaps</code> and <code>swapon -s</code> to see which swap devices are being used</li>
            <li><code>vmstat</code> for current virtual memory statistics</li>
        </ul>

        <p>To permanently disable swap:</p>

        <ul>
            <li>Open the <code>/etc/fstab</code> file, search for a swap line and add a # (hashtag) sign in front of the line to comment on the entire line</li>
            <li>The swap-on-zram feature can be disabled with:<br>
                <code>sudo systemctl stop systemd-zram-setup@zram0</code><br>
                <code>sudo systemctl disable systemd-zram-setup@zram0</code><br>
                <code>sudo systemctl mask systemd-zram-setup@zram0</code>
            </li>
            <li>Run <code>swapoff -va</code> to disable memory swapping</li>
        </ul>

        <p>Download the latest release <code>kubectl</code> with the command:</p>

        <pre><code class="language-bash hljs">curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"</code></pre>

        <p>Download the <code>kubectl</code> checksum file:</p>

        <pre><code class="language-bash hljs">curl -LO "https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"</code></pre>   
        
        <p>Validate the <code>kubectl</code> binary against the checksum file:</p>

        <pre><code class="language-bash hljs">echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check</code></pre>

        <p>If valid, the output is:</p>

        <pre><code class="language-bash hljs">kubectl: OK</code></pre>   
        
        <p>Install <code>kubectl</code>:</p>

        <pre><code class="language-bash hljs">sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl</code></pre>

        <p>Check the <code>kubectl</code> version you installed:</p> 

        <pre><code class="language-bash hljs">kubectl version --short</code></pre>

        <p>Install CNI plugins (required for most pod network):</p>

        <pre><code class="language-bash hljs">CNI_PLUGINS_VERSION="v1.3.0"
ARCH="amd64"
DEST="/opt/cni/bin"
sudo mkdir -p "$DEST"
curl -L "https://github.com/containernetworking/plugins/releases/download/${CNI_PLUGINS_VERSION}/cni-plugins-linux-${ARCH}-${CNI_PLUGINS_VERSION}.tgz" | sudo tar -C "$DEST" -xz</code></pre>

        <p>Define the directory to download command files:</p>      
        
        <pre><code class="language-bash hljs">DOWNLOAD_DIR="/usr/local/bin"
sudo mkdir -p "$DOWNLOAD_DIR"</code></pre>

        <p>Install <code>crictl</code> (required for <code>kubeadm</code>/<code>kubelet</code> Container Runtime Interface (CRI)).</p>

        <pre><code class="language-bash hljs">CRICTL_VERSION="v1.27.0"
ARCH="amd64"
curl -L "https://github.com/kubernetes-sigs/cri-tools/releases/download/${CRICTL_VERSION}/crictl-${CRICTL_VERSION}-linux-${ARCH}.tar.gz" | sudo tar -C $DOWNLOAD_DIR -xz</code></pre>
        
        <p>Install <code>kubeadm</code>, <code>kubelet</code>, <code>kubectl</code> and add a <code>kubelet</code> systemd service:</p>

        <pre><code class="language-bash hljs">RELEASE="$(curl -sSL https://dl.k8s.io/release/stable.txt)"
ARCH="amd64"
cd $DOWNLOAD_DIR
sudo curl -L --remote-name-all https://dl.k8s.io/release/${RELEASE}/bin/linux/${ARCH}/{kubeadm,kubelet}
sudo chmod +x {kubeadm,kubelet}
cd -</code></pre>

        <pre><code class="language-bash hljs">RELEASE_VERSION="v0.15.1"
curl -sSL "https://raw.githubusercontent.com/kubernetes/release/${RELEASE_VERSION}/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service" | sed "s:/usr/bin:${DOWNLOAD_DIR}:g" | sudo tee /etc/systemd/system/kubelet.service
sudo mkdir -p /etc/systemd/system/kubelet.service.d
curl -sSL "https://raw.githubusercontent.com/kubernetes/release/${RELEASE_VERSION}/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf" | sed "s:/usr/bin:${DOWNLOAD_DIR}:g" | sudo tee /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code></pre>
        
        <p>Create <code>/etc/kubernetes/manifests</code> directory where kubelet should look for static Pod manifests.</p>

        <pre><code class="language-bash hljs">mkdir -p /etc/kubernetes/manifests</code></pre>

        <p>Enable and start <code>kubelet</code>:</p>

        <pre><code class="language-bash hljs">sudo systemctl enable --now kubelet</code></pre>

        <p>Now let's initialize the cluster on master node by passing a flag that is later needed for the container network.</p>
        
        <pre><code class="language-bash hljs">sudo kubeadm init --pod-network-cidr=192.168.0.0/16 --control-plane-endpoint=kubemaster</code></pre>

        <p>If the cluster initialization is success, it will print output similar to follow:</p>

        <pre><code class="language-bash hljs">[init] Using Kubernetes version: v1.27.3
[preflight] Running pre-flight checks
    [WARNING Firewalld]: firewalld is active, please ensure ports [6443 10250] are open or your cluster may not function correctly
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
W0719 21:22:19.276005     867 checks.go:835] detected that the sandbox image "registry.k8s.io/pause:3.8" of the container runtime is inconsistent with that used by kubeadm. It is recommended that using "registry.k8s.io/pause:3.9" as the CRI sandbox image.
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubemaster kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.16.0.21]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [kubemaster localhost] and IPs [172.16.0.21 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [kubemaster localhost] and IPs [172.16.0.21 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 18.503832 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node kubemaster as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kubemaster as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: nr9zk2.2ysjxp5tceb5h83t
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join kubemaster:6443 --token nr9zk2.2ysjxp5tceb5h83t \
    --discovery-token-ca-cert-hash sha256:ff308c648fefa963b3f5f7ac320141b33c761a58ed62c0087ae382db3b98e653 \
    --control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join kubemaster:6443 --token nr9zk2.2ysjxp5tceb5h83t \
    --discovery-token-ca-cert-hash sha256:ff308c648fefa963b3f5f7ac320141b33c761a58ed62c0087ae382db3b98e653</code></pre>

        <p>To start using your cluster, you need to run the following as a <b>regular user</b>:</p>

        <pre><code class="language-bash hljs">sudo mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config</code></pre>

        <p>Alternatively, if you are the root user, you can run:</p>

        <pre><code class="language-bash hljs">export KUBECONFIG=/etc/kubernetes/admin.conf</code></pre>

        <p>Check if kubectl is working:</p>

        <pre><code class="language-bash hljs">kubectl get nodes</code></pre>

        <p>If kubectl is working then it will show output similar to:</p>

        <pre><code class="language-bash hljs">NAME         STATUS     ROLES           AGE    VERSION
kubemaster   NotReady   control-plane   4m2s   v1.27.3</code></pre>
        
        <p>kubectl taint node fedoramaster node.kubernetes.io/not-ready:NoSchedule-</p>
        
        <p>The status will be <b>NotReady</b> as we haven't set up our networking yet.</p> 

        <p>To list all namespaces, use the command:</p>

        <pre><code class="language-bash hljs">kubectl get namespaces</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">NAME              STATUS   AGE
default           Active   8m26s
kube-node-lease   Active   8m26s
kube-public       Active   8m27s
kube-system       Active   8m27s</code></pre>
        
        <p>To list all resources in <code>default</code> namespace, use the command:</p>

        <pre><code class="language-bash hljs">kubectl get all</code></pre>

        <pre><code class="language-bash hljs">NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   5m37s</code></pre>

        <p>To list all resources in <code>kube-system</code> namespace, use the command:</p>

        <pre><code class="language-bash hljs">kubectl get all -n kube-system</code></pre>
        
        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">NAME                                     READY   STATUS    RESTARTS   AGE
pod/coredns-5d78c9869d-ckcr4             0/1     Pending   0          2m31s
pod/coredns-5d78c9869d-fljqv             0/1     Pending   0          2m31s
pod/etcd-kubemaster                      1/1     Running   0          2m44s
pod/kube-apiserver-kubemaster            1/1     Running   0          2m43s
pod/kube-controller-manager-kubemaster   1/1     Running   0          2m43s
pod/kube-proxy-6bj2x                     1/1     Running   0          2m32s
pod/kube-scheduler-kubemaster            1/1     Running   0          2m43s

NAME               TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
service/kube-dns   ClusterIP   10.96.0.10   &lt;none&gt;        53/UDP,53/TCP,9153/TCP   2m43s

NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/kube-proxy   1         1         1       1            1           kubernetes.io/os=linux   2m43s

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/coredns   0/2     2            0           2m44s

NAME                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/coredns-5d78c9869d   2         2         0       2m32s</code></pre>
        
        <h4 class="post-subtitle">Install calico (container network) on Master Node</h4>

        <p>The ports required for master node are:</p>

        <table class="table table-bordered border-primary">
            <thead>
                <th>Protocol</th>
                <th>Port Range</th>
                <th>Used By</th>
            </thead>
            <tbody>
            <tr>
                <td>TCP</td>
                <td>179</td>
                <td>Calico BGP Port</td>
            </tr>
            <tr>
                <td>TCP</td>
                <td>5473</td>
                <td>Calico Typha</td>
            </tr>
            </tbody>
        </table>

        <p>The ports are required to be open in the firewall.</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --add-port=179/tcp
sudo firewall-cmd --permanent --add-port=5473/tcp
sudo firewall-cmd --reload</code></pre>

        <p>Download the tigera operator manifest file.</p>

        <pre><code class="language-bash hljs">curl -OJL https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/tigera-operator.yaml</code></pre>

        <p>Apply the manifest using the following command:</p>

        <pre><code class="language-bash hljs">kubectl create -f tigera-operator.yaml</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">namespace/tigera-operator created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgpfilters.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/apiservers.operator.tigera.io created
customresourcedefinition.apiextensions.k8s.io/imagesets.operator.tigera.io created
customresourcedefinition.apiextensions.k8s.io/installations.operator.tigera.io created
customresourcedefinition.apiextensions.k8s.io/tigerastatuses.operator.tigera.io created
serviceaccount/tigera-operator created
clusterrole.rbac.authorization.k8s.io/tigera-operator created
clusterrolebinding.rbac.authorization.k8s.io/tigera-operator created
deployment.apps/tigera-operator created</code></pre>
        
        <p>List all resources in <code>tigera-operator</code> namespace, use the following command:</p>

        <pre><code class="language-bash hljs">kubectl get all -n tigera-operator</code></pre>
        
        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">NAME                                  READY   STATUS    RESTARTS   AGE
pod/tigera-operator-5f4668786-mchkg   1/1     Running   0          7m17s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/tigera-operator   1/1     1            1           7m17s

NAME                                        DESIRED   CURRENT   READY   AGE
replicaset.apps/tigera-operator-5f4668786   1         1         1       7m17s</code></pre>
        
        <p>Download the custom resources necessary to configure Calico.</p>

        <pre><code class="language-bash hljs">curl -OJL https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/custom-resources.yaml</code></pre>

        <p>Apply the manifest using the following command:</p>

        <pre><code class="language-bash hljs">kubectl create -f custom-resources.yaml</code></pre>

        <pre><code class="language-bash hljs">installation.operator.tigera.io/default created
apiserver.operator.tigera.io/default created</code></pre>
        
        <p>List all resources in <code>calico-system</code> namespace, use the following command:</p>

        <pre><code class="language-bash hljs">kubectl get all -n calico-system</code></pre>
        
        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">NAME                                           READY   STATUS              RESTARTS   AGE
pod/calico-kube-controllers-6cb95b98f5-bpx9g   0/1     Pending             0          9s
pod/calico-node-phxjc                          0/1     Init:0/2            0          9s
pod/calico-typha-69856d85d5-frh2j              0/1     ContainerCreating   0          9s
pod/csi-node-driver-8k2x6                      0/2     ContainerCreating   0          9s

NAME                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/calico-typha   ClusterIP   10.100.194.232   &lt;none&gt;        5473/TCP   9s

NAME                             DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/calico-node       1         1         0       1            0           kubernetes.io/os=linux   9s
daemonset.apps/csi-node-driver   1         1         0       1            0           kubernetes.io/os=linux   9s

NAME                                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/calico-kube-controllers   0/1     1            0           9s
deployment.apps/calico-typha              0/1     1            0           9s

NAME                                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/calico-kube-controllers-6cb95b98f5   1         1         0       9s
replicaset.apps/calico-typha-69856d85d5              1         1         0       9s</code></pre>

        <p>You can see that all the resources are still not in running state. <b>Wait for everything to be in running state</b>.</p>

        <p>At this point Kubernetes nodes will become <code>Ready</code> because Kubernetes has a networking provider and configuration installed.</p>

        <pre><code class="language-bash hljs">kubectl get nodes</code></pre>

        <p>The output should be similar to:</p>

        <pre><code class="language-bash hljs">NAME         STATUS   ROLES           AGE   VERSION
kubemaster   Ready    control-plane   44m   v1.27.3</code></pre>
        
        <pre><code class="language-bash hljs">kubectl get all -n kube-system</code></pre>

        <p>The output should be similar to:</p>

        <pre><code class="language-bash hljs">NAME                                     READY   STATUS    RESTARTS   AGE
pod/coredns-5d78c9869d-ngzq8             1/1     Running   0          43m
pod/coredns-5d78c9869d-x7pbv             1/1     Running   0          43m
pod/etcd-kubemaster                      1/1     Running   0          43m
pod/kube-apiserver-kubemaster            1/1     Running   0          43m
pod/kube-controller-manager-kubemaster   1/1     Running   0          43m
pod/kube-proxy-5frjt                     1/1     Running   0          43m
pod/kube-scheduler-kubemaster            1/1     Running   0          43m

NAME               TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
service/kube-dns   ClusterIP   10.96.0.10   &lt;none&gt;        53/UDP,53/TCP,9153/TCP   43m

NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/kube-proxy   1         1         1       1            1           kubernetes.io/os=linux   43m

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/coredns   2/2     2            2           43m

NAME                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/coredns-5d78c9869d   2         2         2       43m</code></pre>
        
        <p>Use the following command to download the <code>calicoctl</code> binary.</p>
        
        <pre><code class="language-bash hljs">curl -L https://github.com/projectcalico/calico/releases/latest/download/calicoctl-linux-amd64 -o calicoctl</code></pre>

        <p>Move the <code>calicoctl</code> binary to directory <code>/usr/local/bin/</code>:</p>

        <pre><code class="language-bash hljs">mv calicoctl /usr/local/bin/</code></pre>

        <p>Set the file to be executable:</p>

        <pre><code class="language-bash hljs">chmod +x /usr/local/bin/calicoctl</code></pre>

        <h4 class="post-subtitle">Change MAC Address of the network interface on Worker Node</h4>

        <p>First, make sure your Worker Node VM is off.</p>

        <p>Then to generate the new MAC address of the network interfaces, use <code>VBoxManage modifyvm</code>:</p>

        <pre><code class="language-bash hljs">VBoxManage modifyvm "kubeworker1" --macaddress1 auto</code></pre>

        <ul>
            <li><code>"kubeworker1"</code> is the VM name</li>
            <li><code>--macaddress1</code> is the MAC address of the first network interface</li>
        </ul>

        <p>Set NIC to NAT Network mode:</p>
        
        <pre><code class="language-bash hljs">VBoxManage modifyvm "kubeworker1" --nic1 natnetwork</code></pre>

        <p>Set NIC connected to <code>natnet1</code> network:</p>

        <pre><code class="language-bash hljs">VBoxManage modifyvm "kubeworker1" --natnetwork1 "natnet1"</code></pre>

        <h4 class="post-subtitle">Change Hardware ID on Worker Node</h4>

        <p>Make sure your Worker Node VM is off.</p>

        <p>To generate the new <code>product_uuid</code>, run the command three times on terminal and make sure not be run as root:</p>

        <pre><code class="language-bash hljs">VBoxManage internalcommands sethduuid "$HOME/kubeworker1/kubeworker1.vdi"
VBoxManage internalcommands sethduuid "$HOME/kubeworker1/kubeworker1.vdi"
VBoxManage internalcommands sethduuid "$HOME/kubeworker1/kubeworker1.vdi"</code></pre>

        <p>The output should be similar to:</p>

        <pre><code class="language-bash hljs">UUID changed to: 659d2b89-91aa-4b42-9e32-1a0c6ca73fb9
UUID changed to: 48ac2a50-0c97-4aab-a5a4-694bf5700359
UUID changed to: 0b43b0b9-b2b9-456c-b271-3619069072c5</code></pre>
        
        <p>The first UUID will be used as <code>Machine uuid</code> and <code>Hardware uuid</code>.</p>

        <p>Edit <code>$HOME/kubeworker1/kubeworker1.vbox</code> file. Find the <code>Machine uuid</code> and <code>Hardware uuid</code> and change to <code>659d2b89-91aa-4b42-9e32-1a0c6ca73fb9</code>.</p>

        <pre><code class="language-bash hljs">&lt;Machine uuid="{659d2b89-91aa-4b42-9e32-1a0c6ca73fb9}" name="kubeworker1" OSType="RedHat_64" snapshotFolder="Snapshots" lastStateChange="2023-07-13T10:05:46Z"&gt;
    ...
&lt;/Machine&gt;</code></pre>

        <pre><code class="language-bash hljs">&lt;Hardware uuid="{659d2b89-91aa-4b42-9e32-1a0c6ca73fb9}"&gt;
    ...
&lt;/Hardware&gt;</code></pre>

        <p>The second UUID will be used as <code>System uuid</code>. To set <code>System uuid</code>, run the command:</p>

        <pre><code class="language-bash hljs">VBoxManage setextradata "kubeworker1" "VBoxInternal/Devices/efi/0/Config/DmiSystemUuid" "48ac2a50-0c97-4aab-a5a4-694bf5700359"</code></pre>
        
        <p>The third UUID will be used as <code>Harddisk uuid</code> and <code>Image uuid</code>.</p>

        <p>Edit <code>$HOME/kubeworker1/kubeworker1.vbox</code> file. Find the <code>Harddisk uuid</code> and <code>Image uuid</code> and change to <code>0b43b0b9-b2b9-456c-b271-3619069072c5</code>.</p>

        <pre><code class="language-bash hljs">&lt;HardDisks&gt;
        &lt;HardDisk uuid="{0b43b0b9-b2b9-456c-b271-3619069072c5}" location="kubeworker1.vdi" format="VDI" type="Normal"/&gt;
      &lt;/HardDisks&gt;</code></pre>

        <pre><code class="language-bash hljs">&lt;StorageController name="NVMe" type="NVMe" PortCount="1" useHostIOCache="false" Bootable="true"&gt;
          &lt;AttachedDevice type="HardDisk" hotpluggable="false" port="0" device="0"&gt;
            &lt;Image uuid="{0b43b0b9-b2b9-456c-b271-3619069072c5}"/&gt;
          &lt;/AttachedDevice&gt;
        &lt;/StorageController&gt;</code></pre>

        <p>Detach and remove the storage medium from the VM:</p>

        <pre><code class="language-bash hljs">VBoxManage storageattach kubeworker1 --storagectl "NVMe" --port 0 --device 0 --medium none</code></pre>

        <ul>
            <li><code>kubeworker1</code> specifies the VM name</li>
            <li><code>--storagectl</code> specifies the name of the storage controller</li>
            <li><code>--port</code> specifies the number of the storage controller's port</li>
            <li><code>--device</code> specifies the number of the port's device </li>
            <li><code>--medium none</code> specifies to remove storage medium</li>
        </ul>

        <p>Remove the storage medium from VirtualBox media registry.</p>

        <pre><code class="language-bash hljs">VBoxManage closemedium disk "$HOME/kubeworker1/kubeworker1.vdi"</code></pre>

        <p>Then attach the storage medium to the VM:</p>

        <pre><code class="language-bash hljs">VBoxManage storageattach "kubeworker1" --storagectl "NVMe" --port 0 --device 0 --type hdd --medium $HOME/kubeworker1/kubeworker1.vdi</code></pre>

        <h4 class="post-subtitle">Setting Static IP Address on Worker Node</h4>

        <p>Use <code>ip</code> command to identify the name of the ethernet interface you want to configure:</p>

        <pre><code class="language-bash hljs">ip addr show</code></pre>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-33.jpg" alt="Fedora Server installation">
        </figure>

        <p>The interface named <code>lo</code> is the loopback interface, used for processes that communicate via the IP protocol. The important interface in the listing is <code>enp0s3</code>, the Ethernet interface.</p>

        <p>Add static IP address using <code>nmcli</code> command:</p>

        <pre><code class="language-bash hljs">sudo nmcli connection modify enp0s3 ipv4.addresses 172.16.0.22/24</code></pre>

        <p>Add the gateway IP:</p>

        <pre><code class="language-bash hljs">sudo nmcli connection modify enp0s3 ipv4.gateway 172.16.0.1</code></pre>

        <p>Add the dns IP address:</p>

        <pre><code class="language-bash hljs">sudo nmcli connection modify enp0s3 ipv4.dns 172.16.0.1</code></pre>

        <p>Change the addressing from DHCP to static.</p>

        <pre><code class="language-bash hljs">sudo nmcli connection modify enp0s3 ipv4.method manual</code></pre>

        <p>To make changes into the effect, disable and enable the connection:</p>

        <pre><code class="language-bash hljs">sudo nmcli connection down enp0s3</code></pre>

        <pre><code class="language-bash hljs">sudo nmcli connection up enp0s3</code></pre>     

        <h4 class="post-subtitle">Setting hostname on Worker Node</h4>

        <p>Set the hostname using the following command:</p>

        <pre><code class="language-bash hljs">sudo echo "kubeworker1" > /etc/hostname</code></pre>

        <p>Add the following line to <code>/etc/hosts</code> to map the hostname to a IP address:</p>

        <pre><code class="language-bash hljs">172.16.0.21    kubemaster
172.16.0.22    kubeworker1</code></pre>

        <h4 class="post-subtitle">Disable systemd-resolved service on Worker Node</h4>

        <p>The <code>systemd-resolved</code> service can be disabled with:</p>

        <pre><code class="language-bash hljs">sudo systemctl stop systemd-resolved
sudo systemctl disable systemd-resolved
sudo systemctl mask systemd-resolved</code></pre>
        
        <p>Delete the symlink <code>/etc/resolv.conf</code>:</p>

        <pre><code class="language-bash hljs">sudo rm /etc/resolv.conf</code></pre>

        <p>Create a new <code>/etc/resolv.conf</code> file:</p>

        <pre><code class="language-bash hljs">sudo touch /etc/resolv.conf</code></pre>

        <p>Add the following line to <code>/etc/resolv.conf</code>:</p>

        <pre><code class="language-bash hljs">nameserver 172.16.0.1</code></pre>

        <h4 class="post-subtitle">Setting Machine ID on Worker Node</h4>

        <p>First, remove the <code>/etc/machine-id</code> file:</p>
        
        <pre><code class="language-bash hljs">sudo rm /etc/machine-id</code></pre>        

        <p>Then generate the new one using this command:</p>

        <pre><code class="language-bash hljs">systemd-machine-id-setup</code></pre>

        <h4 class="post-subtitle">Install containerd (container runtime) on Worker Node</h4>

        <p>First, install the required packages to run kubernetes:</p>

        <pre><code class="language-bash hljs">sudo dnf -y install ethtool
sudo dnf -y install socat
sudo dnf -y install iproute-tc
sudo dnf -y install conntrack
sudo dnf -y install openssl
sudo dnf -y install tar
sudo dnf -y install net-tools</code></pre>

        <p>Before install containerd, you need to enable <code>overlay</code> and <code>br_netfilter</code> kernel modules, and <code>net.bridge.bridge-nf-call-iptables</code>, <code>net.bridge.bridge-nf-call-ip6tables</code>, and <code>net.ipv4.ip_forward</code> kernel parameter.</p>

        <p>Create a file in the <code>/etc/modules-load.d/</code> directory which contains kernel module names to be loaded at boot time.</p>

        <pre><code class="language-bash hljs">cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF</code></pre>

        <p>Load <code>overlay</code> and <code>br_netfilter</code> kernel modules now using the <code>modprobe</code> command.</p>  

        <pre><code class="language-bash hljs">sudo modprobe overlay
sudo modprobe br_netfilter</code></pre>

        <p>Create a file in the <code>/etc/sysctl.d/</code> directory which contains <code>net.bridge.bridge-nf-call-iptables</code>, <code>net.bridge.bridge-nf-call-ip6tables</code>, and <code>net.ipv4.ip_forward</code> kernel parameters to be set at boot time.</p>

        <pre><code class="language-bash hljs">cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF</code></pre>        

        <p>Apply the kernel parameters without reboot using <code>sysctl</code> command.</p>

        <pre><code class="language-bash hljs">sudo sysctl --system</code></pre>

        <p>Verify that the <code>br_netfilter</code>, <code>overlay</code> modules are loaded by running the following commands:</p>

        <pre><code class="language-bash hljs">lsmod | grep br_netfilter
lsmod | grep overlay</code></pre>
        
        <p>Verify that the <code>net.bridge.bridge-nf-call-iptables</code>, <code>net.bridge.bridge-nf-call-ip6tables</code>, and <code>net.ipv4.ip_forward</code> system variables are set to <code>1</code> in your <code>sysctl</code> config by running the following command:</p>

        <pre><code class="language-bash hljs">sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward</code></pre>

        <p>Download the containerd from <a href="https://github.com/containerd/containerd/releases">https://github.com/containerd/containerd/releases</a>.</p>

        <pre><code class="language-bash hljs">curl -OJL https://github.com/containerd/containerd/releases/download/v1.7.2/containerd-1.7.2-linux-amd64.tar.gz</code></pre>

        <p>Extract the containerd under <code>/usr/local</code></p>

        <pre><code class="language-bash hljs">sudo tar Cxzvf /usr/local containerd-1.7.2-linux-amd64.tar.gz</code></pre>
        
        <p>Download the <code>containerd.service</code> unit file from <a href="https://raw.githubusercontent.com/containerd/containerd/main/containerd.service">https://raw.githubusercontent.com/containerd/containerd/main/containerd.service</a> into <code>/usr/lib/systemd/system/</code> directory</p>

        <pre><code class="language-bash hljs">curl -o /usr/lib/systemd/system/containerd.service https://raw.githubusercontent.com/containerd/containerd/main/containerd.service</code></pre>

        <p>Reload systemd manager configuration and enable the containerd service.</p>
        
        <pre><code class="language-bash hljs">sudo systemctl daemon-reload
sudo systemctl enable --now containerd</code></pre>
        
        <p>Download the <code>runc</code> binary from <a href="https://github.com/opencontainers/runc/releases">https://github.com/opencontainers/runc/releases</a> and install it as <code>/usr/local/sbin/runc</code>.</p>
        
        <pre><code class="language-bash hljs">curl -OJL https://github.com/opencontainers/runc/releases/download/v1.1.7/runc.amd64
sudo install -m 755 runc.amd64 /usr/local/sbin/runc</code></pre>

        <p>Download the cni-plugins from <a href="https://github.com/containernetworking/plugins/releases">https://github.com/containernetworking/plugins/releases</a> and extract it under <code>/opt/cni/bin</code></p>

        <pre><code class="language-bash hljs">curl -OJL https://github.com/containernetworking/plugins/releases/download/v1.3.0/cni-plugins-linux-amd64-v1.3.0.tgz
sudo mkdir -p /opt/cni/bin
sudo tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.3.0.tgz</code></pre>
        
        <p>Once, we have successfully installed the Containerd. We need to load the Containerd configurations.</p>

        <p>Create <code>/etc/containerd</code> directory and generate the confguration file using <code>containerd</code> command.</p>

        <pre><code class="language-bash hljs">sudo mkdir -p /etc/containerd
sudo containerd config default > /etc/containerd/config.toml</code></pre>

        <p>Open the configuration file <code>/etc/containerd/config.toml</code> and set <code>SystemdCgroup = true</code></p>

        <pre><code class="language-bash hljs">[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
  ...
  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
    SystemdCgroup = true</code></pre> 

        <p>To apply the changes, restart the <code>containerd</code> service.</p>

        <pre><code class="language-bash hljs">sudo systemctl restart containerd</code></pre>

        <h4 class="post-subtitle">Install kubectl, kubeadm, and kubelet on Worker Node</h4>

        <p>The ports required for worker node are:</p>

        <table class="table table-bordered border-primary">
            <thead>
                <th>Protocol</th>
                <th>Port Range</th>
                <th>Used By</th>
            </thead>
            <tbody>
            <tr>
                <td>TCP</td>
                <td>10250</td>
                <td>Kubelet API</td>
            </tr>
            <tr>
                <td>TCP</td>
                <td>30000-32767</td>
                <td>NodePort Services</td>
            </tr>
            </tbody>
        </table>

        <p>The ports are required to be open in the firewall.</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --add-port=10250/tcp
sudo firewall-cmd --permanent --add-port=30000-32767/tcp
sudo firewall-cmd --reload</code></pre>
        
        <p>Swap must be disabled in order for the kubelet to work properly. To permanently disable swap:</p>

        <ul>
            <li>Open the <code>/etc/fstab</code> file, search for a swap line and add a # (hashtag) sign in front of the line to comment on the entire line</li>
            <li>The swap-on-zram feature can be disabled with:<br>
                <code>sudo systemctl stop systemd-zram-setup@zram0</code><br>
                <code>sudo systemctl disable systemd-zram-setup@zram0</code><br>
                <code>sudo systemctl mask systemd-zram-setup@zram0</code>
            </li>
            <li>Run <code>swapoff -va</code> to disable memory swapping</li>
        </ul>

        <p>Download the latest release <code>kubectl</code> with the command:</p>

        <pre><code class="language-bash hljs">curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"</code></pre>

        <p>Download the <code>kubectl</code> checksum file:</p>

        <pre><code class="language-bash hljs">curl -LO "https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"</code></pre>   
        
        <p>Validate the <code>kubectl</code> binary against the checksum file:</p>

        <pre><code class="language-bash hljs">echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check</code></pre>

        <p>If valid, the output is:</p>

        <pre><code class="language-bash hljs">kubectl: OK</code></pre>   
        
        <p>Install <code>kubectl</code>:</p>

        <pre><code class="language-bash hljs">sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl</code></pre>

        <p>Check the <code>kubectl</code> version you installed:</p> 

        <pre><code class="language-bash hljs">kubectl version --short</code></pre>

        <p>Install CNI plugins (required for most pod network):</p>

        <pre><code class="language-bash hljs">CNI_PLUGINS_VERSION="v1.3.0"
ARCH="amd64"
DEST="/opt/cni/bin"
sudo mkdir -p "$DEST"
curl -L "https://github.com/containernetworking/plugins/releases/download/${CNI_PLUGINS_VERSION}/cni-plugins-linux-${ARCH}-${CNI_PLUGINS_VERSION}.tgz" | sudo tar -C "$DEST" -xz</code></pre>

        <p>Define the directory to download command files:</p>      
        
        <pre><code class="language-bash hljs">DOWNLOAD_DIR="/usr/local/bin"
sudo mkdir -p "$DOWNLOAD_DIR"</code></pre>

        <p>Install <code>crictl</code> (required for <code>kubeadm</code>/<code>kubelet</code> Container Runtime Interface (CRI)).</p>

        <pre><code class="language-bash hljs">CRICTL_VERSION="v1.27.0"
ARCH="amd64"
curl -L "https://github.com/kubernetes-sigs/cri-tools/releases/download/${CRICTL_VERSION}/crictl-${CRICTL_VERSION}-linux-${ARCH}.tar.gz" | sudo tar -C $DOWNLOAD_DIR -xz</code></pre>
        
        <p>Install <code>kubeadm</code>, <code>kubelet</code>, <code>kubectl</code> and add a <code>kubelet</code> systemd service:</p>

        <pre><code class="language-bash hljs">RELEASE="$(curl -sSL https://dl.k8s.io/release/stable.txt)"
ARCH="amd64"
cd $DOWNLOAD_DIR
sudo curl -L --remote-name-all https://dl.k8s.io/release/${RELEASE}/bin/linux/${ARCH}/{kubeadm,kubelet}
sudo chmod +x {kubeadm,kubelet}
cd -</code></pre>

        <pre><code class="language-bash hljs">RELEASE_VERSION="v0.15.1"
curl -sSL "https://raw.githubusercontent.com/kubernetes/release/${RELEASE_VERSION}/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service" | sed "s:/usr/bin:${DOWNLOAD_DIR}:g" | sudo tee /etc/systemd/system/kubelet.service
sudo mkdir -p /etc/systemd/system/kubelet.service.d
curl -sSL "https://raw.githubusercontent.com/kubernetes/release/${RELEASE_VERSION}/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf" | sed "s:/usr/bin:${DOWNLOAD_DIR}:g" | sudo tee /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code></pre>
        
        <p>Create <code>/etc/kubernetes/manifests</code> directory where kubelet should look for static pod manifests.</p>

        <pre><code class="language-bash hljs">mkdir -p /etc/kubernetes/manifests</code></pre>

        <p>Enable and start <code>kubelet</code>:</p>

        <pre><code class="language-bash hljs">sudo systemctl enable --now kubelet</code></pre>
            
        <p>If you have lost the <code>kubeadm join</code> command with the token id then you can generate a new one by running the following command on the <b>master node</b></p>

        <pre><code class="language-bash hljs">kubeadm token create --print-join-command</code></pre>

        <p>Now let's connect worker node to the master node using <code>kubeadm join</code> command:</p>
        
        <pre><code class="language-bash hljs">kubeadm join kubemaster:6443 --token 2rzeso.snqfpw46z7zaszb1 --discovery-token-ca-cert-hash sha256:bd7fffd83e3fd971df50acb6950c483e7145227ea4eca388a07967e3627fba96</code></pre>
        
        <p>If the join succeeds then it will print output similar to follow:</p>

        <pre><code class="language-bash hljs">[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.</code></pre>
        
        <p>To start using your cluster, you need to run the following as a <b>regular user</b>:</p>

        <pre><code class="language-bash hljs">sudo mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/kubelet.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config</code></pre>

        <p>Alternatively, if you are the root user, you can run:</p>

        <pre><code class="language-bash hljs">export KUBECONFIG=/etc/kubernetes/kubelet.conf</code></pre>
        
        <p>Check if kubectl is working:</p>

        <pre><code class="language-bash hljs">kubectl get nodes</code></pre>

        <p>If kubectl is working then it will show output similar to:</p>

        <pre><code class="language-bash hljs">NAME            STATUS   ROLES           AGE     VERSION
kubemaster    Ready    control-plane   3h17m   v1.27.3
kubeworker1   NotReady    &lt;none&gt;          29m     v1.27.3</code></pre>

        <p>The status will be <b>NotReady</b> as the master node is still setting up the networking.</p> 

        <p>On the master node, it will create a new calico node pod. Check the pod state using the command:</p>

        <pre><code class="language-bash hljs">kubectl get all -A</code></pre>
        
        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">NAMESPACE          NAME                                           READY   STATUS              RESTARTS        AGE
calico-apiserver   pod/calico-apiserver-746fbc54c9-m7sgk          1/1     Running             2 (6m32s ago)   22h
calico-apiserver   pod/calico-apiserver-746fbc54c9-xgsdz          1/1     Running             2 (6m32s ago)   22h
calico-system      pod/calico-kube-controllers-6cb95b98f5-bpx9g   1/1     Running             2 (6m32s ago)   22h
calico-system      pod/calico-node-lmgvk                          0/1     Init:0/2            0               63s
calico-system      pod/calico-node-phxjc                          1/1     Running             2 (6m32s ago)   22h
calico-system      pod/calico-typha-69856d85d5-frh2j              1/1     Running             2 (6m32s ago)   22h
calico-system      pod/csi-node-driver-8k2x6                      2/2     Running             4 (6m32s ago)   22h
calico-system      pod/csi-node-driver-hz2jd                      0/2     ContainerCreating   0               63s
kube-system        pod/coredns-5d78c9869d-ngzq8                   1/1     Running             2 (6m32s ago)   23h
kube-system        pod/coredns-5d78c9869d-x7pbv                   1/1     Running             2 (6m32s ago)   23h
kube-system        pod/etcd-kubemaster                            1/1     Running             2 (6m33s ago)   23h
kube-system        pod/kube-apiserver-kubemaster                  1/1     Running             2 (6m32s ago)   23h
kube-system        pod/kube-controller-manager-kubemaster         1/1     Running             2 (6m32s ago)   23h
kube-system        pod/kube-proxy-5frjt                           1/1     Running             2 (6m32s ago)   23h
kube-system        pod/kube-proxy-6wthc                           0/1     ContainerCreating   0               63s
kube-system        pod/kube-scheduler-kubemaster                  1/1     Running             2 (6m32s ago)   23h
tigera-operator    pod/tigera-operator-5f4668786-mchkg            1/1     Running             4 (6m2s ago)    22h

NAMESPACE          NAME                                      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                  AGE
calico-apiserver   service/calico-api                        ClusterIP   10.105.92.196    &lt;none&gt;        443/TCP                  22h
calico-system      service/calico-kube-controllers-metrics   ClusterIP   None             &lt;none&gt;        9094/TCP                 22h
calico-system      service/calico-typha                      ClusterIP   10.100.194.232   &lt;none&gt;        5473/TCP                 22h
default            service/kubernetes                        ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP                  23h
kube-system        service/kube-dns                          ClusterIP   10.96.0.10       &lt;none&gt;        53/UDP,53/TCP,9153/TCP   23h

NAMESPACE       NAME                             DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
calico-system   daemonset.apps/calico-node       2         2         1       2            1           kubernetes.io/os=linux   22h
calico-system   daemonset.apps/csi-node-driver   2         2         1       2            1           kubernetes.io/os=linux   22h
kube-system     daemonset.apps/kube-proxy        2         2         1       2            1           kubernetes.io/os=linux   23h

NAMESPACE          NAME                                      READY   UP-TO-DATE   AVAILABLE   AGE
calico-apiserver   deployment.apps/calico-apiserver          2/2     2            2           22h
calico-system      deployment.apps/calico-kube-controllers   1/1     1            1           22h
calico-system      deployment.apps/calico-typha              1/1     1            1           22h
kube-system        deployment.apps/coredns                   2/2     2            2           23h
tigera-operator    deployment.apps/tigera-operator           1/1     1            1           22h

NAMESPACE          NAME                                                 DESIRED   CURRENT   READY   AGE
calico-apiserver   replicaset.apps/calico-apiserver-746fbc54c9          2         2         2       22h
calico-system      replicaset.apps/calico-kube-controllers-6cb95b98f5   1         1         1       22h
calico-system      replicaset.apps/calico-typha-69856d85d5              1         1         1       22h
kube-system        replicaset.apps/coredns-5d78c9869d                   2         2         2       23h
tigera-operator    replicaset.apps/tigera-operator-5f4668786            1         1         1       22h</code></pre>

        <p>To see more info about the pod, use the command:</p>

        <pre><code class="language-bash hljs">kubectl describe pod/calico-node-grfnr -n calico-system</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">Name:                 calico-node-lmgvk
Namespace:            calico-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Service Account:      calico-node
Node:                 kubeworker1/172.16.0.22
Start Time:           Thu, 20 Jul 2023 21:11:50 +0700
Labels:               app.kubernetes.io/name=calico-node
                      controller-revision-hash=57d9485f79
                      k8s-app=calico-node
                      pod-template-generation=1
Annotations:          hash.operator.tigera.io/cni-config: 9f0a12e03c58671de56ed3876cb88f1c43cef5dc
                      hash.operator.tigera.io/system: bb4746872201725da2dea19756c475aa67d9c1e9
                      hash.operator.tigera.io/tigera-ca-private: 7675b87693668c11faf4b4f275c014fd575b0c6b
Status:               Pending
IP:                   172.16.0.22
IPs:
  IP:           172.16.0.22
Controlled By:  DaemonSet/calico-node
Init Containers:
  flexvol-driver:
    Container ID:    containerd://3bc13d8131404c3f8134efd48a7cc8a169d1613ffc5358ba300ad12bcdeabae1
    Image:           docker.io/calico/pod2daemon-flexvol:v3.26.1
    Image ID:        docker.io/calico/pod2daemon-flexvol@sha256:2aefd77a4f8289c88cfe24c0db38822de3132292d1ea4ac9192abc9583e4b54c
    Port:            &lt;none&gt;
    Host Port:       &lt;none&gt;
    SeccompProfile:  RuntimeDefault
    State:           Terminated
      Reason:        Completed
      Exit Code:     0
      Started:       Thu, 20 Jul 2023 21:14:01 +0700
      Finished:      Thu, 20 Jul 2023 21:14:01 +0700
    Ready:           True
    Restart Count:   0
    Environment:     &lt;none&gt;
    Mounts:
      /host/driver from flexvol-driver-host (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-txnjv (ro)
  install-cni:
    Container ID:    
    Image:           docker.io/calico/cni:v3.26.1
    Image ID:        
    Port:            &lt;none&gt;
    Host Port:       &lt;none&gt;
    SeccompProfile:  RuntimeDefault
    Command:
      /opt/cni/bin/install
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Environment:
      CNI_CONF_NAME:            10-calico.conflist
      SLEEP:                    false
      CNI_NET_DIR:              /etc/cni/net.d
      CNI_NETWORK_CONFIG:       &lt;set to the key 'config' of config map 'cni-config'&gt;  Optional: false
      KUBERNETES_SERVICE_HOST:  10.96.0.1
      KUBERNETES_SERVICE_PORT:  443
    Mounts:
      /host/etc/cni/net.d from cni-net-dir (rw)
      /host/opt/cni/bin from cni-bin-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-txnjv (ro)
Containers:
  calico-node:
    Container ID:    
    Image:           docker.io/calico/node:v3.26.1
    Image ID:        
    Port:            &lt;none&gt;
    Host Port:       &lt;none&gt;
    SeccompProfile:  RuntimeDefault
    State:           Waiting
      Reason:        PodInitializing
    Ready:           False
    Restart Count:   0
    Liveness:        http-get http://localhost:9099/liveness delay=0s timeout=10s period=10s #success=1 #failure=3
    Readiness:       exec [/bin/calico-node -bird-ready -felix-ready] delay=0s timeout=5s period=10s #success=1 #failure=3
    Environment:
      DATASTORE_TYPE:                      kubernetes
      WAIT_FOR_DATASTORE:                  true
      CLUSTER_TYPE:                        k8s,operator,bgp
      CALICO_DISABLE_FILE_LOGGING:         false
      FELIX_DEFAULTENDPOINTTOHOSTACTION:   ACCEPT
      FELIX_HEALTHENABLED:                 true
      FELIX_HEALTHPORT:                    9099
      NODENAME:                             (v1:spec.nodeName)
      NAMESPACE:                           calico-system (v1:metadata.namespace)
      FELIX_TYPHAK8SNAMESPACE:             calico-system
      FELIX_TYPHAK8SSERVICENAME:           calico-typha
      FELIX_TYPHACAFILE:                   /etc/pki/tls/certs/tigera-ca-bundle.crt
      FELIX_TYPHACERTFILE:                 /node-certs/tls.crt
      FELIX_TYPHAKEYFILE:                  /node-certs/tls.key
      FIPS_MODE_ENABLED:                   false
      FELIX_TYPHACN:                       typha-server
      CALICO_MANAGE_CNI:                   true
      CALICO_IPV4POOL_CIDR:                192.168.0.0/16
      CALICO_IPV4POOL_VXLAN:               CrossSubnet
      CALICO_IPV4POOL_BLOCK_SIZE:          26
      CALICO_IPV4POOL_NODE_SELECTOR:       all()
      CALICO_IPV4POOL_DISABLE_BGP_EXPORT:  false
      CALICO_NETWORKING_BACKEND:           bird
      IP:                                  autodetect
      IP_AUTODETECTION_METHOD:             first-found
      IP6:                                 none
      FELIX_IPV6SUPPORT:                   false
      KUBERNETES_SERVICE_HOST:             10.96.0.1
      KUBERNETES_SERVICE_PORT:             443
    Mounts:
      /etc/pki/tls/cert.pem from tigera-ca-bundle (ro,path="ca-bundle.crt")
      /etc/pki/tls/certs from tigera-ca-bundle (ro)
      /host/etc/cni/net.d from cni-net-dir (rw)
      /lib/modules from lib-modules (ro)
      /node-certs from node-certs (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/calico from var-lib-calico (rw)
      /var/log/calico/cni from cni-log-dir (rw)
      /var/run/calico from var-run-calico (rw)
      /var/run/nodeagent from policysync (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-txnjv (ro)
Conditions:
  Type              Status
  Initialized       False 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  policysync:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/nodeagent
    HostPathType:  DirectoryOrCreate
  tigera-ca-bundle:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      tigera-ca-bundle
    Optional:  false
  node-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  node-certs
    Optional:    false
  var-run-calico:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/calico
    HostPathType:  
  var-lib-calico:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/calico
    HostPathType:  
  cni-bin-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /opt/cni/bin
    HostPathType:  
  cni-net-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  cni-log-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /var/log/calico/cni
    HostPathType:  
  flexvol-driver-host:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds
    HostPathType:  DirectoryOrCreate
  kube-api-access-txnjv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 :NoSchedule op=Exists
                             :NoExecute op=Exists
                             CriticalAddonsOnly op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  8m5s   default-scheduler  Successfully assigned calico-system/calico-node-lmgvk to kubeworker1
  Normal  Pulling    8m     kubelet            Pulling image "docker.io/calico/pod2daemon-flexvol:v3.26.1"
  Normal  Pulled     5m54s  kubelet            Successfully pulled image "docker.io/calico/pod2daemon-flexvol:v3.26.1" in 47.864589232s (2m5.390789865s including waiting)
  Normal  Created    5m54s  kubelet            Created container flexvol-driver
  Normal  Started    5m54s  kubelet            Started container flexvol-driver
  Normal  Pulling    5m53s  kubelet            Pulling image "docker.io/calico/cni:v3.26.1"</code></pre>

        <p><b>Wait for the pod to be in running state</b>.</p>

        <p>Then check the status of a Calico instance on the master node:</p>

        <pre><code class="language-bash hljs">calicoctl node status</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">Calico process is running.

IPv4 BGP status
+--------------+-------------------+-------+----------+-------------+
| PEER ADDRESS |     PEER TYPE     | STATE |  SINCE   |    INFO     |
+--------------+-------------------+-------+----------+-------------+
| 172.16.0.22  | node-to-node mesh | up    | 14:43:05 | Established |
+--------------+-------------------+-------+----------+-------------+

IPv6 BGP status
No IPv6 peers found.</code></pre>

        <p>Check the status of a Calico instance on the worker node:</p>

        <pre><code class="language-bash hljs">calicoctl node status</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">Calico process is running.

IPv4 BGP status
+--------------+-------------------+-------+----------+-------------+
| PEER ADDRESS |     PEER TYPE     | STATE |  SINCE   |    INFO     |
+--------------+-------------------+-------+----------+-------------+
| 172.16.0.21  | node-to-node mesh | up    | 14:43:05 | Established |
+--------------+-------------------+-------+----------+-------------+

IPv6 BGP status
No IPv6 peers found.</code></pre>

        <h4 class="post-subtitle">Install MetalLB (Load Balancer) on Master Node</h4>

        <p>Download the MetalLB manifest for the Kubernetes clusters.</p>

        <pre><code class="language-bash hljs">curl -OJL https://raw.githubusercontent.com/metallb/metallb/v0.13.10/config/manifests/metallb-native.yaml</code></pre>

        <p>Change the value of <code>failurePolicy</code> from <code>Fail</code> to <code>Ignore</code> in the <code>metallb-native.yaml</code> file.</p>

        <pre><code class="language-bash hljs">sed -i 's/failurePolicy: Fail/failurePolicy: Ignore/' metallb-native.yaml</code></pre>

        <p>To install MetalLB, apply the manifest:</p>

        <pre><code class="language-bash hljs">kubectl create -f metallb-native.yaml</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">namespace/metallb-system created
customresourcedefinition.apiextensions.k8s.io/addresspools.metallb.io created
customresourcedefinition.apiextensions.k8s.io/bfdprofiles.metallb.io created
customresourcedefinition.apiextensions.k8s.io/bgpadvertisements.metallb.io created
customresourcedefinition.apiextensions.k8s.io/bgppeers.metallb.io created
customresourcedefinition.apiextensions.k8s.io/communities.metallb.io created
customresourcedefinition.apiextensions.k8s.io/ipaddresspools.metallb.io created
customresourcedefinition.apiextensions.k8s.io/l2advertisements.metallb.io created
serviceaccount/controller created
serviceaccount/speaker created
role.rbac.authorization.k8s.io/controller created
role.rbac.authorization.k8s.io/pod-lister created
clusterrole.rbac.authorization.k8s.io/metallb-system:controller created
clusterrole.rbac.authorization.k8s.io/metallb-system:speaker created
rolebinding.rbac.authorization.k8s.io/controller created
rolebinding.rbac.authorization.k8s.io/pod-lister created
clusterrolebinding.rbac.authorization.k8s.io/metallb-system:controller created
clusterrolebinding.rbac.authorization.k8s.io/metallb-system:speaker created
configmap/metallb-excludel2 created
secret/webhook-server-cert created
service/webhook-service created
deployment.apps/controller created
daemonset.apps/speaker created
validatingwebhookconfiguration.admissionregistration.k8s.io/metallb-webhook-configuration created</code></pre>
        
        <p>This will deploy MetalLB to your cluster, under the <code>metallb-system</code> namespace. The components in the manifest are:</p>

        <ul>
            <li>The <code>metallb-system/controller</code> deployment. This is the cluster-wide controller that handles IP address assignments</li>
            <li>The <code>metallb-system/speaker</code> daemonset. This is the component that speaks the protocol(s) of your choice to make the services reachable</li>
            <li>Service accounts for the controller and speaker, along with the RBAC permissions that the components need to function</li>
        </ul>

        <p>The installation manifest does not include a configuration file. MetalLB's components will still start, but will remain idle until you start deploying resources.</p>

        <p>To list all resources in <code>metallb-system</code> namespace, use the command:</p>

        <pre><code class="language-bash hljs">kubectl get all -n metallb-system</code></pre>
        
        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">NAME                              READY   STATUS              RESTARTS   AGE
pod/controller-595f88d88f-gk98r   0/1     ContainerCreating   0          77s
pod/speaker-2rg52                 0/1     ContainerCreating   0          77s
pod/speaker-r8p6g                 0/1     ContainerCreating   0          77s

NAME                      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
service/webhook-service   ClusterIP   10.106.144.178   &lt;none&gt;        443/TCP   77s

NAME                     DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/speaker   2         2         0       2            0           kubernetes.io/os=linux   77s

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/controller   0/1     1            0           77s

NAME                                    DESIRED   CURRENT   READY   AGE
replicaset.apps/controller-595f88d88f   1         1         0       77s</code></pre>
        
        <p>You can see that all the resources are still not in running state. <b>Wait for everything to be in running state</b>.</p>        
        
        <p>If the error message is <code>ErrImagePull</code> or <code>ImagePullBackOff</code> then delete the pod using the command:</p>

        <pre><code class="language-bash hljs">kubectl delete pod/controller-595f88d88f-gk98r -n metallb-system</code></pre>

        <p><code>Deployment/StatefulSet/ReplicaSet/DaemonSet</code> will reschedule a new one in its place automatically.</p>

        <!--<p>If the error message is <code>Error: secret "memberlist" not found</code> then delete the pod and create a secret object named memberlist using the command:</p>

        <pre><code class="language-bash hljs">kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey="$(openssl rand -base64 128)"</code></pre>-->

        <p>When everything is in running state, the response should look similar to this:</p>

        <pre><code class="language-bash hljs">NAME                              READY   STATUS    RESTARTS   AGE
pod/controller-595f88d88f-gk98r   1/1     Running   0          12m
pod/speaker-2rg52                 1/1     Running   0          12m
pod/speaker-r8p6g                 1/1     Running   0          12m

NAME                      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
service/webhook-service   ClusterIP   10.106.144.178   &lt;none&gt;        443/TCP   12m

NAME                     DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/speaker   2         2         2       2            2           kubernetes.io/os=linux   12m

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/controller   1/1     1            1           12m

NAME                                    DESIRED   CURRENT   READY   AGE
replicaset.apps/controller-595f88d88f   1         1         1       12m</code></pre>


        <p>Create a <code>metallb-addresspool.yaml</code> configuration file to enable the layer 2 mode as below:</p>

        <pre><code class="language-bash hljs">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ipaddresspool
  namespace: metallb-system
spec:
  addresses:
  - 192.170.0.0/24
  avoidBuggyIPs: true
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: l2advertisement
  namespace: metallb-system
spec:
  ipAddressPools:
  - ipaddresspool</code></pre>        

        <p>Apply the configuration file:</p>

        <pre><code class="language-bash hljs">kubectl create -f metallb-addresspool.yaml</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">ipaddresspool.metallb.io/ipaddresspool created
l2advertisement.metallb.io/l2advertisement created</code></pre>
        
        <p>The ports required by MetalLB In Layer 2 Mode for master node and worker node are:</p>

        <table class="table table-bordered border-primary">
            <thead>
                <th>Protocol</th>
                <th>Port Range</th>
                <th>Used By</th>
            </thead>
            <tbody>
            <tr>
                <td>TCP</td>
                <td>7946</td>
                <td>metallb speaker</td>
            </tr>
            <tr>
                <td>UDP</td>
                <td>7946</td>
                <td>metallb speaker</td>
            </tr>
            </tbody>
        </table>

        <p>The ports are required to be open in the firewall.</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --add-port=7946/tcp
sudo firewall-cmd --permanent --add-port=7946/udp
sudo firewall-cmd --reload</code></pre>

        <p>Add label to kubeworker1 node, so we can assign pods to that node:</p>
        
        <pre><code class="language-bash hljs">kubectl label nodes kubeworker1 nodelabel=kubeworker1</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">node/kubeworker1 labeled</code></pre>        

        <p>Let's verify that the new label is added by running:</p>

        <pre><code class="language-bash hljs">kubectl get nodes --show-labels</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">NAME          STATUS   ROLES           AGE    VERSION   LABELS
kubemaster    Ready    control-plane   2d1h   v1.27.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=kubemaster,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=
kubeworker1   Ready    &lt;none&gt;          26h    v1.27.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=kubeworker1,kubernetes.io/os=linux,nodelabel=kubeworker1</code></pre>                        
        
        <h4 class="post-subtitle">Install Nginx Ingress Controller on Master Node</h4>

        <p>Nginx Ingress controller is an Ingress controller for Kubernetes using NGINX as a reverse proxy and load balancer.</p>

        <p>Download NGINX Ingress Controller manifest:</p>

        <pre><code class="language-bash hljs">curl -o ingress-nginx.yaml https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/baremetal/deploy.yaml</code></pre>

        <p>Change the value of <code>failurePolicy</code> from <code>Fail</code> to <code>Ignore</code> in the <code>ingress-nginx.yaml</code> file.</p>

        <pre><code class="language-bash hljs">sed -i 's/failurePolicy: Fail/failurePolicy: Ignore/' ingress-nginx.yaml</code></pre>

        <pre><code class="language-bash hljs">sed -i 's/--patch-failure-policy=Fail/--patch-failure-policy=Ignore/' ingress-nginx.yaml</code></pre>

        <p>Apply Nginx ingress controller manifest deployment file:</p>

        <pre><code class="language-bash hljs">kubectl create -f ingress-nginx.yaml</code></pre>        

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">namespace/ingress-nginx created
serviceaccount/ingress-nginx created
serviceaccount/ingress-nginx-admission created
role.rbac.authorization.k8s.io/ingress-nginx created
role.rbac.authorization.k8s.io/ingress-nginx-admission created
clusterrole.rbac.authorization.k8s.io/ingress-nginx created
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission created
rolebinding.rbac.authorization.k8s.io/ingress-nginx created
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx created
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created
configmap/ingress-nginx-controller created
service/ingress-nginx-controller created
service/ingress-nginx-controller-admission created
deployment.apps/ingress-nginx-controller created
job.batch/ingress-nginx-admission-create created
job.batch/ingress-nginx-admission-patch created
ingressclass.networking.k8s.io/nginx created
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission created</code></pre>
        
        <p>To list all resources in <code>ingress-nginx</code> namespace, use the command:</p>

        <pre><code class="language-bash hljs">kubectl get all -n ingress-nginx</code></pre>
        
        <p>The response should look similar to this:</p>
        
        <pre><code class="language-bash hljs">NAME                                            READY   STATUS              RESTARTS   AGE
pod/ingress-nginx-admission-create-7fkfc        0/1     ContainerCreating   0          15s
pod/ingress-nginx-admission-patch-vnbwf         0/1     ContainerCreating   0          15s
pod/ingress-nginx-controller-5c778bffff-4bb6x   0/1     ContainerCreating   0          15s

NAME                                         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
service/ingress-nginx-controller             NodePort    10.107.65.165    &lt;none&gt;        80:30609/TCP,443:30996/TCP   15s
service/ingress-nginx-controller-admission   ClusterIP   10.104.236.122   &lt;none&gt;        443/TCP                      15s

NAME                                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/ingress-nginx-controller   0/1     1            0           15s

NAME                                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/ingress-nginx-controller-5c778bffff   1         1         0       15s

NAME                                       COMPLETIONS   DURATION   AGE
job.batch/ingress-nginx-admission-create   0/1           15s        15s
job.batch/ingress-nginx-admission-patch    0/1           15s        15s</code></pre>

        <p>You can see that all the resources are still not in running state. <b>Wait for everything to be in running state</b>.</p>

        <p>When all the resources are ready. The response should look similar to this:</p>        

        <pre><code class="language-bash hljs">NAME                                            READY   STATUS      RESTARTS   AGE
pod/ingress-nginx-admission-create-7fkfc        0/1     Completed   0          116s
pod/ingress-nginx-admission-patch-vnbwf         0/1     Completed   0          116s
pod/ingress-nginx-controller-5c778bffff-4bb6x   1/1     Running     0          116s

NAME                                         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
service/ingress-nginx-controller             NodePort    10.107.65.165    &lt;none&gt;        80:30609/TCP,443:30996/TCP   116s
service/ingress-nginx-controller-admission   ClusterIP   10.104.236.122   &lt;none&gt;        443/TCP                      116s

NAME                                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/ingress-nginx-controller   1/1     1            1           116s

NAME                                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/ingress-nginx-controller-5c778bffff   1         1         1       116s

NAME                                       COMPLETIONS   DURATION   AGE
job.batch/ingress-nginx-admission-create   1/1           20s        116s
job.batch/ingress-nginx-admission-patch    1/1           20s        116s</code></pre>

        <p>Edit <code>ingress-nginx-controller</code> service.</p>

        <pre><code class="language-bash hljs">kubectl edit service/ingress-nginx-controller -n ingress-nginx</code></pre>

        <p>Change the value of <code>type</code> from <code>NodePort</code> to <code>LoadBalancer</code>.</p>

        <pre><code class="language-bash hljs">apiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2023-07-25T05:22:21Z"
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.8.1
  name: ingress-nginx-controller
  namespace: ingress-nginx
  resourceVersion: "90209"
  uid: 67865f69-35af-4e2d-aa61-897556d7ae1b
spec:
  clusterIP: 10.107.65.165
  clusterIPs:
  - 10.107.65.165
  externalTrafficPolicy: Cluster
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - appProtocol: http
    name: http
    nodePort: 30609
    port: 80
    protocol: TCP
    targetPort: http
  - appProtocol: https
    name: https
    nodePort: 30996
    port: 443
    protocol: TCP
    targetPort: https
  selector:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
  sessionAffinity: None
  type: LoadBalancer
status:
  loadBalancer: {}</code></pre>        
        
        <p>Confirm successful edited of the service.</p>

        <pre><code class="language-bash hljs">service/ingress-nginx-controller edited</code></pre>

        <p>Nginx Controller Service will be assigned an IP address automatically from Address Pool as configured in MetalLB and assigned ports which are required to be open.</p>

        <pre><code class="language-bash hljs">kubectl get service ingress-nginx-controller --namespace=ingress-nginx</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">NAME                       TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx-controller   LoadBalancer   10.107.65.165   192.170.0.1   80:30609/TCP,443:30996/TCP   4m6s</code></pre>

        <p>The ports required for ingress nginx controller are:</p>

        <table class="table table-bordered border-primary">
            <thead>
                <th>Protocol</th>
                <th>Port Range</th>
                <th>Used By</th>
            </thead>
            <tbody>
            <tr>
                <td>TCP</td>
                <td>80</td>
                <td>cluster ip, external ip</td>
            </tr>
            <tr>
                <td>TCP</td>
                <td>443</td>
                <td>cluster ip, external ip</td>
            </tr>
            <tr>
                <td>TCP</td>
                <td>30609</td>
                <td>node ip</td>
            </tr>
            <tr>
                <td>TCP</td>
                <td>30996</td>
                <td>node ip</td>
            </tr>
            </tbody>
        </table>

        <p>The ports are required to be open in the firewall on master node.</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --add-port=80/tcp
sudo firewall-cmd --permanent --add-port=443/tcp
sudo firewall-cmd --reload</code></pre>
        
        <p>The ports are required to be open in the firewall on worker node.</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --add-port=80/tcp
sudo firewall-cmd --permanent --add-port=443/tcp
sudo firewall-cmd --permanent --add-port=30609/tcp
sudo firewall-cmd --permanent --add-port=30996/tcp
sudo firewall-cmd --reload</code></pre>
        
        <p>If you have multiple worker nodes, to know which worker node the <code>ingress-nginx-controller</code> pod is running on, run the command on the master node:</p>

        <pre><code class="language-bash hljs">kubectl describe pod/ingress-nginx-controller-5c778bffff-4bb6x -n ingress-nginx | grep Node:</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">Node:             kubeworker1/172.16.0.22</code></pre>

        <p>On worker node where <code>ingress-nginx-controller</code> pod is running on, use <code>curl</code> to test the cluster IP address of <code>ingress-nginx-controller</code> service:</p>

        <pre><code class="language-bash hljs">curl http://10.107.65.165:80</code></pre>

        <pre><code class="language-bash hljs">curl -k https://10.107.65.165:443</code></pre>

        <p>Since there isn't any ingress resource configured, the response will return "404 Not Found" similar to follow:</p>

        <pre><code class="language-bash hljs">&lt;html&gt;
&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;
&lt;body&gt;
&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;
&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;
&lt;/body&gt;
&lt;/html&gt;</code></pre>

        <p>On worker node where <code>ingress-nginx-controller</code> pod is running on, use <code>curl</code> to test the external IP address of <code>ingress-nginx-controller</code> service:</p>

        <pre><code class="language-bash hljs">curl http://192.170.0.1:80</code></pre>

        <pre><code class="language-bash hljs">curl -k https://192.170.0.1:443</code></pre>

        <p>Since there isn't any ingress resource configured, the response will return "404 Not Found" similar to follow:</p>

        <pre><code class="language-bash hljs">&lt;html&gt;
&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;
&lt;body&gt;
&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;
&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;
&lt;/body&gt;
&lt;/html&gt;</code></pre>
        
        <p>On worker node where <code>ingress-nginx-controller</code> pod is running on, you also can use the worker node IP address:</p>

        <pre><code class="language-bash hljs">curl http://172.16.0.22:30609</code></pre>

        <pre><code class="language-bash hljs">curl -k https://172.16.0.22:30996</code></pre>

        <p>Since there isn't any ingress resource configured, the response will return "404 Not Found" similar to follow:</p>

        <pre><code class="language-bash hljs">&lt;html&gt;
&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;
&lt;body&gt;
&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;
&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;
&lt;/body&gt;
&lt;/html&gt;</code></pre>

        <p>On master node and other worker nodes, use <code>curl</code> to test tcp connection on cluster IP and on external IP address of <code>ingress-nginx-controller</code> service and on the worker node IP address where <code>ingress-nginx-controller</code> pod is running on:</p>

        <pre><code class="language-bash hljs">curl http://10.107.65.165:80</code></pre>

        <pre><code class="language-bash hljs">curl -k https://10.107.65.165:443</code></pre>

        <pre><code class="language-bash hljs">curl http://192.170.0.1:80</code></pre>

        <pre><code class="language-bash hljs">curl -k https://192.170.0.1:443</code></pre>

        <pre><code class="language-bash hljs">curl http://172.16.0.22:30609</code></pre>

        <pre><code class="language-bash hljs">curl -k https://172.16.0.22:30996</code></pre>

        <p>The curl response when try to test tcp connection on cluster IP external IP address of <code>ingress-nginx-controller</code> service will fail since the firewall is blocking the request:</p>

        <pre><code class="language-bash hljs">curl: (7) Failed to connect to 10.107.65.165 port 80 after 1 ms: Couldn't connect to server</code></pre>

        <pre><code class="language-bash hljs">curl: (7) Failed to connect to 10.107.65.165 port 443 after 1 ms: Couldn't connect to server</code></pre>

        <pre><code class="language-bash hljs">curl: (7) Failed to connect to 192.170.0.1 port 80 after 1 ms: Couldn't connect to server</code></pre>

        <pre><code class="language-bash hljs">curl: (7) Failed to connect to 192.170.0.1 port 443 after 1 ms: Couldn't connect to server</code></pre>

        <p>But the curl response when try to test tcp connection on the worker node IP address where <code>ingress-nginx-controller</code> pod is running on will return "404 Not Found" since there isn't any ingress resource configured:</p>

        <pre><code class="language-bash hljs">&lt;html&gt;
&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;
&lt;body&gt;
&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;
&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;
&lt;/body&gt;
&lt;/html&gt;</code></pre>

        <p>Let's inspect why the firewall blocked the request.</p>

        <p>Enable <code>firewalld</code> logging for denied packets on <b>the worker node</b> where <code>ingress-nginx-controller</code> pod is running on. Edit the <code>/etc/firewalld/firewalld.conf</code>. Set the value of <code>LogDenied</code> option from <code>off</code> to <code>all</code>.</p>

        <p>Restart the <code>firewalld</code> service:</p>

        <pre><code class="language-bash hljs">sudo systemctl restart firewalld</code></pre>

        <p><b>On master node and other worker nodes</b>, run the <code>curl</code> command again:</p>

        <pre><code class="language-bash hljs">curl http://10.107.65.165:80</code></pre>

        <pre><code class="language-bash hljs">curl -k https://10.107.65.165:443</code></pre>

        <pre><code class="language-bash hljs">curl http://192.170.0.1:80</code></pre>

        <pre><code class="language-bash hljs">curl -k https://192.170.0.1:443</code></pre>

        <p>On worker node where <code>ingress-nginx-controller</code> pod is running on, to view the denied packets, use <code>dmesg</code> command:</p>
        
        <pre><code class="language-bash hljs">dmesg | grep -i REJECT</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">[ 8484.826734] filter_FWD_public_REJECT: IN=enp0s3 OUT=cali7368eccef27 MAC=08:00:27:10:a0:5f:08:00:27:fe:60:d3:08:00 SRC=172.16.0.21 DST=192.168.90.8 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=47217 DF PROTO=TCP SPT=59916 DPT=80 WINDOW=64240 RES=0x00 SYN URGP=0 MARK=0x10000 
[ 8484.842467] filter_FWD_public_REJECT: IN=enp0s3 OUT=cali7368eccef27 MAC=08:00:27:10:a0:5f:08:00:27:fe:60:d3:08:00 SRC=172.16.0.21 DST=192.168.90.8 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=38798 DF PROTO=TCP SPT=5163 DPT=443 WINDOW=64240 RES=0x00 SYN URGP=0 MARK=0x10000 
[ 8484.863771] filter_FWD_public_REJECT: IN=enp0s3 OUT=cali7368eccef27 MAC=08:00:27:10:a0:5f:08:00:27:fe:60:d3:08:00 SRC=172.16.0.21 DST=192.168.90.8 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=15091 DF PROTO=TCP SPT=58803 DPT=80 WINDOW=64240 RES=0x00 SYN URGP=0 MARK=0x10000 
[ 8484.881376] filter_FWD_public_REJECT: IN=enp0s3 OUT=cali7368eccef27 MAC=08:00:27:10:a0:5f:08:00:27:fe:60:d3:08:00 SRC=172.16.0.21 DST=192.168.90.8 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=28939 DF PROTO=TCP SPT=19211 DPT=443 WINDOW=64240 RES=0x00 SYN URGP=0 MARK=0x10000</code></pre>        
        <p>Create an new zone using calico network interface name (ie. <code>cali7368eccef27</code>). This is the zone for <code>ingress-nginx-controller</code> calico pod network.</p>
        
        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --new-zone cali7368eccef27</code></pre>

        <p>Then assign <code>ingress-nginx-controller</code> calico pod network interface <code>cali7368eccef27</code> to this zone.</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --change-interface=cali7368eccef27 --zone=cali7368eccef27</code></pre>

        <p>Next, create a policy:</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --new-policy cali7368eccef27pol</code></pre>

        <p>Set the policies to traffic forwarded from lan interface <code>enp0s3</code> to calico network interface <code>cali7368eccef27</code>:</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --policy cali7368eccef27pol --add-ingress-zone public
sudo firewall-cmd --permanent --policy cali7368eccef27pol --add-egress-zone cali7368eccef27</code></pre>        
        <p>To finish up the policy settings, set it to accept new connections by default:</p>
        
        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --policy cali7368eccef27pol --set-target ACCEPT</code></pre>

        <p>Then load the new zone and policy into the active runtime state:</p>
        
        <pre><code class="language-bash hljs">sudo firewall-cmd --reload</code></pre>

        <p><b>On master node and other worker nodes</b>, test to run the <code>curl</code> command again:</p>

        <pre><code class="language-bash hljs">curl http://10.107.65.165:80</code></pre>

        <pre><code class="language-bash hljs">curl -k https://10.107.65.165:443</code></pre>

        <pre><code class="language-bash hljs">curl http://192.170.0.1:80</code></pre>

        <pre><code class="language-bash hljs">curl -k https://192.170.0.1:443</code></pre>

        <p>Since there isn't any ingress resource configured, the response will return "404 Not Found" similar to follow:</p>

        <pre><code class="language-bash hljs">&lt;html&gt;
&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;
&lt;body&gt;
&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;
&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;
&lt;/body&gt;
&lt;/html&gt;</code></pre>

        <h4 class="post-subtitle">Deploy a web-application using Nginx server to Worker Node from Master Node</h4>

        <p>First generate a self signed rsa key and certificate that the server can use for TLS:</p>

        <pre><code class="language-bash hljs">mkdir ~/nginx
openssl req -x509 -nodes -days 365 -newkey rsa:4096 -keyout ~/nginx/nginx.key -out ~/nginx/nginx.crt</code></pre>
        
        <p>Fill out the certificate form:</p>

        <pre><code class="language-bash hljs">...+........+.+...+..+......+.........+...+.+......+...+..+...+.+.....+..........+......+...+......+......+.....+.+..+...+...+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*.............+.+.....+.+...........+....+...+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*......+.............+......+....................+.......+..................+...............+...........+.+........+......+.............+.....+.+..............+...+.......+...............+...+..+.+........................+............+.....+.............+...+..+.+......+.........+.................+....+.....+.+.............................+.+..................+..+.......+..+.............+...........+.......+...+...........+...+.......+.....+..................+...+....+...........+.........+..................+.+.................+...+....+............+.........+.....+....+...............+......+...............+..+............+.+.....+.+...+............+...+..+.+..+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
.....+...+......+...+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*.....+...+...+.......+...+...+.....+....+............+.....+...+...+...............+...+....+...........+.+......+..+.+......+...+..+...+...+....+..............+......+.+..+.+.........+......+.....+............+.......+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*........+..+.............+..+.......+...........+.......+.....+..............................+...+.......+......+......+...+...+...+..+....+...+...+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
-----
You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter '.', the field will be left blank.
-----
Country Name (2 letter code) [AU]:ID
State or Province Name (full name) [Some-State]:Jakarta
Locality Name (eg, city) []:Jakarta
Organization Name (eg, company) [Internet Widgits Pty Ltd]:hemimorphite
Organizational Unit Name (eg, section) []:     
Common Name (e.g. server FQDN or YOUR name) []:kubeworker1
Email Address []:yangsamuel91@gmail.com</code></pre>
        
        <p>Encode the key and certificate data in base64</p>

        <pre><code class="language-bash hljs">cat ~/nginx/nginx.crt | base64</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">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</code></pre>

        <pre><code class="language-bash hljs">cat ~/nginx/nginx.key | base64</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">LS0tLS1CRUdJTiBQUklWQVRFIEtFWS0tLS0tCk1JSUpRZ0lCQURBTkJna3Foa2lHOXcwQkFRRUZBQVNDQ1N3d2dna29BZ0VBQW9JQ0FRRGhpbytOVUd1MGlJc0wKN1pLN1dhb0czZHNKZFZET0R3STM2NWw0Rm85RVROSG0valdIRzBwM2QyZmZvaEJUeWFwQTFLaXNGRjhxalBRMwplNFBHQ1k0UEl1YjQ0WG9NWG8rQ2VLbmdqc1p1U2lSMXBlQm0xbGdzNlliZEZ1eVVBeWRPUnd1Z0lmSFJqT052CmR6Q2QvNmtMaVdSRWxscmhEbS9GMzhZRkIxd1kvRENmcjFFV3J6RFJSdEk3ODE4eXBDc2ZnMUpzbUNvNHJqclUKRUp6V1ZsZTN6MlJnd1czUkt4SktMUXkyMVZrc3BvZWtRQlN5U09rd1ZMWXQ0QW9VYmk2Q1pQazY5UVJmKzY5LwpZVVNjVXBxSzZHYitVRExtSmk1OEJaMFc0QWQ2elI3Yzlqc2kxMVBkMmc2VmN1SFhIWTMzb0FZYm1NSk5wQm5vCnRwbnBkVGZONGFtWFhjNnhpd3hkM2JEckdGREJDOXd2TFBwYUxkUVVHWkppN0JBTlNNMTRsWmxHOGZvL0tUS2UKUlRuZEdLMlkvajJsMUlDbjc4NjhSbnNiWEdLSDdqeklZUDByU2JzQkpHMDhlY3d0K1hpdjAxei9YeDdkMjdrWQpQSkxielNvYkVsS2lvcWhDM2xRek4ycDZkN3E2blNhbVh6b3UwS1liTlU0eXJhQi84U01BL3hXNTQ4WEJhQnlqClA5NWJSbXdWN3cvZGJNMUNEL2c3eTZMbkZQbnQ0aEhpSWM0eWp5Vkhyb1QveWxCZFpxdHZZa1VsblNxTDBsekgKcDl6RWE5cm95Yk9XYzdmcVpuOFhmK3ZEQU1qREh4NTV0M2xLQjBONndWSXZtdDFCSGJtTTRRSG55Z3hkRWlTbQp3L2g1QzFURnBRbWs3QVJvYW9jcWZTbWhkTncrNXdJREFRQUJBb0lDQUFoMXdwY05kSmFUMSs1N3FpQkxRLzlHCmZ3UElhN1BNVnVGYU5yK3BsUGhGRHZYa2txSEV6Z1NwU1hNNjJrb0xMeFg1eXprOU1pbFJmaG9VTzMxUFA2cEkKcndsZk5nWXFNdnB0MWlaU3N1c1lmTUc3YTJaZ0hEdGltWWp5dTNaRVN1ZUpKK2tYU1UvQnBoYi8vMExORGtWSApyY1FhYU5CaHhJYU1iRk5Cb1hqcXJGUjNTaGcxa1llWStCVzVmVHZ3am1hbW1JM3pQVHhybW15LzJUaHROUVpYCkdEcUZqRDN1WWZKTmo3ek9aRHhyUDlIVEkvY24wWW1pTFptclFZa1hwbis5aFlveGpJSVN2V0tyRThCRVA5WWEKS0Zwb1NIM1F4WnRFeHR2NWhreUZ3R1p6RmN3NDc1cVNlNkU2QjQ2L0VJNUFzSkVTaThOSHIzaHhlSkZFMjRBYgpiR3daaWJTK0I4WlFKamlXNGM1Tm5CZ2MrMVhvUWZqR0ZMMmJxR3M5QVVMcjJ6YWVxMGRxNjlmazUzazNSOWlOCllhem1zU04rVWJKQ1dVVWdNRUlDWEp1SXVxNmF6VEVpcUk2cHZod2RCckVPdjZBYnhHRklDNDNtM1l2QTNjOVQKQmZDekZLaXpwOWlHMkdUUklMS1pWVDdBbElRb0U1cHQxcStEczRLQkdOMjlkdy9Jc0hqYzhZSzA1MTk4OEhwOQp3d3RzQXZqaVdUZ2U3TkY3NmhCVW5yVjkzeFM5cHFhWVhLc0V4Mzl3SXlkUURqVTUyVWg3TlJldncwT3V4aEc1Clh3djFoS1pZc0c0cEtUbmNET0tOWnpMTUQ4c25NOGswc2ZMNjMzL1dJREl0NXZEekErQnpSTU5iR0ZLUDZ2a3YKVnhjbmVFc0NvbzBDZlhBemJINjFBb0lCQVFEek4zRUtSME5NcE81QXptbUdQenVJemIvMXVvdU5Rd3NrcU11bgpibFNFNkV1TXA5V3BJM3YwS0N4Q1RRNmZ1WWExVzhrSHQwR3Jzc2cxODBkWjVVSDdUdTF2TlVKbVlWaXJGSmNrCkZNRkNla2JZb01GSTk4RFo0Vnd5OEJCTjNPQ1MybG91ajVESTJnV2pTSHlyTFBzeVBoNEs1NkNsa1U0QjVaV3UKYlpJbDhGWlR6NmxOL05vcDFKYmxCZldxc2lpbU13eTZPbzhjYkRSMC8vSVJGcTAvWEVOOEMvb0VSRzA2ZHlhaQpwR3k5SUdhZ1g4MzVObkZEK20zZWdySmdIc0RjcjFCYTluRUJud1RBcWp1TUlKYnVkWWMrU1JwYXJvdW13QVRICldWdVRkVWhiWldpNE9pUTZQRUw1aWo0U0R5dXNHaTVudXRLUk1oRmYxbHY1SElaN0FvSUJBUUR0WlVyQzZFMUQKNlBjV1BKVndTVjNRczdDUTUrRkdpbElTeVhmQStEbVREZmNWd3Q5SXB4ckhKZWtrZ1lVMndYZFptMy9iN0U2UAp6RGRQVXAzSUJEV3E5cU1ISGtUWmNZNzh4QUhzT0RCYVMrNmJNVjNIWDZZOWFNeGxTZDI0RGxtTjRJSDRYY3ZsCkN0amZGMmtKSmg1Um5QNmw5amxXbUc1c2loMWRUV3FYaVMwTXltUjBJaFNXT01QTmdwOGtFMFJoN3RSTlEzMWwKbHlWTU9xWTE2a3Y2bzJWQTJsNkM1bUc0cnJtU2FzZDNxYjRrMlJDMGZNU05BVjVzbGw4K0tHVmhGRVBjMENNVQptTHN4T1Q0Q2VSN1pCOXFVMDlFUE9WOGhGNlhySHptNmQ3UUR0ekc5ZTFXSG9yek1GQVpWL0ZSbmp4L1d1VkpYCklXYlUrT0NOcWRPRkFvSUJBRVh0eTkrVHE4THVyTjRQT3dIeFRsSEFMcEFkYWFCZEJXZ3Q3QmdndmNaVTc5dnEKS2FGdTVXWEc0eXJROHdKem1rcXNzRGs2dEhRRWZGSWV0NFllUjRGVktTbFBXOThKNEMrWVJQaUh5eVVzK1EreAo4TURsRXhyU0UrdEZZTHF5WmhOekduakdxRzlIV0ZXaE1zOFlxTVRDWFdydlBCNCtqL28xbzNxNllFbnVOelB6CndnNzlRMURCb2YwSllLQUtoMnQ1eEJBTFpEaGMyQnhIdHJhNUdXamRRejM0UjVOUWhVTkpITitvZk9kSDJKb0YKMGJmMHNrNUVRN0MxVFVvYnJZSUdHZ2w5VC9LU1lSQndJWnpoaGVQY0FOMmtzU0lmaTJHUFZoQm5IZEJnNlVQMQovVXp5MHBXMzI5M1pwSTBXdFl6UEhrYU96Wm9YbGkxQytjQy9OZDhDZ2dFQkFMVEpQODhBdzk2a3FvNm5vcEo1CmZOVjBFbEc0Rk9uNGFwcGVEVEhLbDJYeTY2a0oxNnJuZjFBTUFlbklMUi9PNDhvOHpuazRFM2dVNkVZalAzOVUKSFY3T0pzZEQrT0N6UjFZZEd1Mit1S3Y3U1lHc2JhTm9weHY3RkRWS3RHdjNtYUdmU0x0UkN2YXBkUkVvTGRoRwp6QzRRNWlpVFE0VTczbFFRTmw2WWwwVkJ0U29aYzdpeFA0WkxRbW5lUUwxall0OUYxeTNqNkxvV0NkUnNrYWxiCmZYTXBQZkhPbXMwTEQzNXFxVzNrblhUMnNwUXpMaktWRmNYZ05mMXg0cVJlZFI0aUpiQWlYYVRRenpXa1J5SHgKQlZuNFRqQ3F3bm5ha2lTTWN0R0UzUnl4RGtrS0dQb1kwOERRWVFEMWV3RytnTFRBM1ZsVGxtNCtSS2FKeGRCOApkVlVDZ2dFQVg2Tk1uZE1vQnJ3enBMZVc3c3JwSXVWRFBnRkg4TXNFVlZUZnlURm54WWNtNEkrNlF6clJFeVM5CkVoTjA0WWRXdEErU3h3Y3FrQjNNZ3pjV0JDa0xIZ0kyWHFpS2NKWEtsT0tqZHdEWkpRK2hZRHhsLzduWW1Ncm8KMEU0QWU1OENXWnhpTXFJQ2VjL2ZCamdQUzBocHFuOEZEdVdHT3dxWHpkRE5mQm11aklmeUE1SmN4K2Z1VkF3ZgpTclZ4MEJkRWhpOUhveUplTEtuNGtucTZneHhzaElVUXJmQTVTYWNlQ0YzV2NRM1l5UWZHOE1sNUhsSEJhZkx4Cnl5U0NkMU8wUzRCeE1yaW4vYWkyK3lrMUJMQUY5ZXNSWDJISVVJdVdHVUs3OTZsSk1pelpLOTJoN3FOWThlU3oKMTZSNHhsaDBEdnRkdWhOREVMcjA3VC8wZHFKeThRPT0KLS0tLS1FTkQgUFJJVkFURSBLRVktLS0tLQo=</code></pre>

        <p>Then create your NGINX server configuration file <code>default.conf</code>:</p>

        <pre><code class="language-bash hljs">server {
    listen 80 default_server;
    listen [::]:80 default_server ipv6only=on;

    listen 443 ssl;

    root /usr/share/nginx/html;
    index index.html;

    server_name localhost;
    ssl_certificate /etc/nginx/ssl/tls.crt;
    ssl_certificate_key /etc/nginx/ssl/tls.key;

    location / {
            try_files $uri $uri/ =404;
    }
}</code></pre>
        

        <p>Create a kubernetes yaml tls secret file for creating a secret named <code>nginx-secret</code></p>

        <pre><code class="language-bash hljs">apiVersion: v1
data: 
  tls.crt: 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
  tls.key: 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
kind: Secret
metadata: 
  name: nginx-secret
  namespace: default
type: kubernetes.io/tls</code></pre>
        
        <p>Apply the tls secret manifest file:</p>

        <pre><code class="language-bash hljs">kubectl create -f nginx-secret.yaml</code></pre>        

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">secret/nginx-secret created</code></pre>

        <p>Create a <code>ConfigMap</code> that stores Nginx configuration:</p>

        <pre><code class="language-bash hljs">kubectl create configmap nginx-configmap --from-file=default.conf</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">configmap/nginx-configmap created</code></pre>

        <p>Create a kubernetes yaml deployment file for deploying a pod named <code>nginx-server</code> that running Nginx image:</p>
        
        <pre><code class="language-bash hljs">apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-server
  namespace: default
  labels:
    app: web
spec:
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      volumes:
      - name: secret-volume
        secret:
          secretName: nginx-secret
      - name: configmap-volume
        configMap:
          name: nginx-configmap
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
        - containerPort: 443
        volumeMounts:
        - mountPath: /etc/nginx/ssl
          name: secret-volume
        - mountPath: /etc/nginx/conf.d
          name: configmap-volume
      nodeSelector:
        nodelabel: kubeworker1</code></pre>        

        <p>Apply the nginx server manifest deployment file:</p>

        <pre><code class="language-bash hljs">kubectl create -f nginx-server.yaml</code></pre>        

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">deployment.apps/nginx-server created</code></pre>

        <p>Create a kubernetes yaml load balancer service file for exposing the pod with the label <code>app=web</code> to a service named <code>nginx-server-service</code> on a TCP port of 80:</code></p>        
        <pre><code class="language-bash hljs">apiVersion: v1
kind: Service
metadata:
  name: nginx-server-service
  namespace: default
spec:
  selector:
    app: web
  ports:
    - name: web-http
      protocol: TCP
      appProtocol: http
      targetPort: http
      port: 80
      targetPort: 80
    - name: web-https
      protocol: TCP
      appProtocol: https
      targetPort: https
      port: 443
      targetPort: 443
  type: LoadBalancer</code></pre>

        <p>Apply the nginx server service manifest file:</p>

        <pre><code class="language-bash hljs">kubectl create -f nginx-server-service.yaml</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">service/nginx-server-service created</code></pre>

        <p>Nginx Controller Service will be assigned an IP address automatically from Address Pool as configured in MetalLB and assigned ports which are required to be open.</p>

        <pre><code class="language-bash hljs">kubectl get service nginx-server-service --namespace=default</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">NAME                   TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
nginx-server-service   LoadBalancer   10.107.131.88   192.170.0.2   80:31653/TCP,443:30552/TCP   5m23s</code></pre>

        <p>The ports required by nginx server service are:</p>

        <table class="table table-bordered border-primary">
            <thead>
                <th>Protocol</th>
                <th>Port Range</th>
                <th>Used By</th>
            </thead>
            <tbody>
            <tr>
                <td>TCP</td>
                <td>80</td>
                <td>cluster ip, external ip</td>
            </tr>
            <tr>
                <td>TCP</td>
                <td>443</td>
                <td>cluster ip, external ip</td>
            </tr>
            <tr>
                <td>TCP</td>
                <td>31653</td>
                <td>node ip</td>
            </tr>
            <tr>
                <td>TCP</td>
                <td>30552</td>
                <td>node ip</td>
            </tr>
            </tbody>
        </table>

        <p>The ports are required to be open in the firewall on master node.</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --add-port=80/tcp
sudo firewall-cmd --permanent --add-port=443/tcp
sudo firewall-cmd --reload</code></pre>
        
        <p>The ports are required to be open in the firewall on worker node.</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --add-port=80/tcp
sudo firewall-cmd --permanent --add-port=443/tcp
sudo firewall-cmd --permanent --add-port=31025/tcp
sudo firewall-cmd --permanent --add-port=30625/tcp
sudo firewall-cmd --reload</code></pre>
    
        <p>If you have multiple worker nodes, you need to know which worker node the <code>nginx-server</code> pod is running on. To know which worker node the pod is running on, run the command:</p>

        <pre><code class="language-bash hljs">kubectl describe pod/ingress-nginx-controller-5c778bffff-4bb6x -n ingress-nginx | grep Node:</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">Node:             kubeworker1/172.16.0.22</code></pre>

        <p>On worker node where the <code>nginx-server</code> pod is running on, try to access your application using the cluster IP and the external IP address of <code>nginx-server-service</code> and the worker node IP which the <code>nginx-server</code> pod is running on:</p>

        <pre><code class="language-bash hljs">curl http://10.107.131.88:80</code></pre>

        <pre><code class="language-bash hljs">curl -k https://10.107.131.88:443</code></pre>

        <pre><code class="language-bash hljs">curl http://192.170.0.2:80</code></pre>

        <pre><code class="language-bash hljs">curl -k https://192.170.0.2:443</code></pre>

        <pre><code class="language-bash hljs">curl http://172.16.0.22:31653</code></pre>

        <pre><code class="language-bash hljs">curl -k https://172.16.0.22:30552</code></pre>

        <p>The response to a successful request is similar to:</p>

        <pre><code class="language-bash hljs">&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href="http://nginx.org/"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href="http://nginx.com/"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;</code></pre>

        <p>On master node and other worker nodes, try to access your application using the cluster IP, external IP and node IP address of <code>nginx-server-service</code>:</p>

        <pre><code class="language-bash hljs">curl http://10.107.131.88:80</code></pre>

        <pre><code class="language-bash hljs">curl -k https://10.107.131.88:443</code></pre>

        <pre><code class="language-bash hljs">curl http://192.170.0.2:80</code></pre>

        <pre><code class="language-bash hljs">curl -k https://192.170.0.2:443</code></pre>

        <p>The curl response when try to test tcp connection on cluster IP external IP address of <code>nginx-server-service</code> will fail since the firewall is blocking the request:</p>
        
        <pre><code class="language-bash hljs">curl: (7) Failed to connect to 10.107.131.88 port 80 after 1 ms: Couldn't connect to server</code></pre>

        <pre><code class="language-bash hljs">curl: (7) Failed to connect to 10.107.131.88 port 443 after 1 ms: Couldn't connect to server</code></pre>

        <pre><code class="language-bash hljs">curl: (7) Failed to connect to 192.170.0.2 port 80 after 1 ms: Couldn't connect to server</code></pre>

        <pre><code class="language-bash hljs">curl: (7) Failed to connect to 192.170.0.2 port 443 after 1 ms: Couldn't connect to server</code></pre>

        <p>But otherwise the curl response when try to test tcp connection on the worker node IP address where <code>ingress-nginx-controller</code> pod is running on returns:</p>

        <pre><code class="language-bash hljs">&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href="http://nginx.org/"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href="http://nginx.com/"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;</code></pre>

        <p>On worker node where the <code>nginx-server</code> pod is running on, use <code>dmesg</code> command to view the denied packets:</p>
        
        <pre><code class="language-bash hljs">dmesg | grep -i REJECT</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">[29039.265405] filter_FWD_public_REJECT: IN=enp0s3 OUT=cali9aba158b53d MAC=08:00:27:10:a0:5f:08:00:27:fe:60:d3:08:00 SRC=172.16.0.21 DST=192.168.90.11 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=15179 DF PROTO=TCP SPT=35290 DPT=80 WINDOW=64240 RES=0x00 SYN URGP=0 MARK=0x10000 
[29039.291039] filter_FWD_public_REJECT: IN=enp0s3 OUT=cali9aba158b53d MAC=08:00:27:10:a0:5f:08:00:27:fe:60:d3:08:00 SRC=172.16.0.21 DST=192.168.90.11 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=21020 DF PROTO=TCP SPT=4065 DPT=443 WINDOW=64240 RES=0x00 SYN URGP=0 MARK=0x10000 
[29039.302519] filter_FWD_public_REJECT: IN=enp0s3 OUT=cali9aba158b53d MAC=08:00:27:10:a0:5f:08:00:27:fe:60:d3:08:00 SRC=172.16.0.21 DST=192.168.90.11 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=26229 DF PROTO=TCP SPT=7973 DPT=80 WINDOW=64240 RES=0x00 SYN URGP=0 MARK=0x10000 
[29039.310790] filter_FWD_public_REJECT: IN=enp0s3 OUT=cali9aba158b53d MAC=08:00:27:10:a0:5f:08:00:27:fe:60:d3:08:00 SRC=172.16.0.21 DST=192.168.90.11 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=58856 DF PROTO=TCP SPT=9648 DPT=443 WINDOW=64240 RES=0x00 SYN URGP=0 MARK=0x10000</code></pre>       
        <p>Create an new zone using calico network interface name (ie. <code>cali9aba158b53d</code>). <code>cali9aba158b53d</code> is the zone for <code>nginx-server</code> calico pod network.</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --new-zone cali9aba158b53d</code></pre>

        <p>Then assign <code>nginx-server</code> calico pod network interface <code>cali9aba158b53d</code> to this zone.</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --change-interface=cali9aba158b53d --zone=cali9aba158b53d</code></pre>

        <p>Next, create a policy:</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --new-policy cali9aba158b53dpol</code></pre>

        <p>Set the policies to traffic forwarded from lan interface <code>enp0s3</code> to calico network interface <code>cali7368eccef27</code>:</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --policy cali9aba158b53dpol --add-ingress-zone public
sudo firewall-cmd --permanent --policy cali9aba158b53dpol --add-egress-zone cali9aba158b53d</code></pre>        
        
        <p>To finish up the policy settings, set it to accept new connections by default:</p>
        
        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --policy cali9aba158b53dpol --set-target ACCEPT</code></pre>

        <p>Then load the new zone and policy into the active runtime state:</p>
        
        <pre><code class="language-bash hljs">sudo firewall-cmd --reload</code></pre>

        <p><b>On master node and other worker nodes</b>, test to run the <code>curl</code> command again:</p>

        <pre><code class="language-bash hljs">curl http://10.107.131.88:80</code></pre>

        <pre><code class="language-bash hljs">curl -k https://10.107.131.88:443</code></pre>

        <pre><code class="language-bash hljs">curl http://192.170.0.2:80</code></pre>

        <pre><code class="language-bash hljs">curl -k https://192.170.0.2:443</code></pre>

        <pre><code class="language-bash hljs">curl http://172.16.0.22:31653</code></pre>

        <pre><code class="language-bash hljs">curl -k https://172.16.0.22:30552</code></pre>

        <p>The request should be successful now.</p>

        <pre><code class="language-bash hljs">&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href="http://nginx.org/"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href="http://nginx.com/"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;</code></pre>
        
        <p>Generate a self signed rsa key and certificate that later the ingress resource can use for TLS:</p>

        <pre><code class="language-bash hljs">openssl req -x509 -nodes -days 365 -newkey rsa:4096 -keyout ~/nginx/ingress.key -out ~/nginx/ingress.crt</code></pre>
        
        <p>Fill out the certificate form:</p>

        <pre><code class="language-bash hljs">..+.........+...+.............+.........+..+...+..........+........+....+...+........+...+.+...+...+...+.....+.+.....+....+...........+.+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*.+...+..+.+...........+.........+.+..............+....+..+.+..+......+.+........+.+.....+.......+.....+.........+....+.....+....+.....................+..+....+......+..+...+..........+...+...........+...+..........+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*.........+.....+....+.....+................+......+..............+................+.....+...+......+....+...........+..........+.........+...+..................+............+........+.......+...............+...+.....+............+.........+.............+......+......+.....+...+.+............+.........+...+.....+......+..........+..+.........+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
....+.+...+......+...........+.+...+......+...+..................+..............+.+..+...+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*.................+....+...+......+..+....+......+.....+...+......+...+............+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*........+......+........+.......+...+......+.....+......+........................+....+......+...........+...+...............+......+....+.......................+.......+..+...................+..+..........+..+.+.....+.......+.........+..............+...............+.....................+....+...........+...+...+...............+...+..........+......+......+.........+........+......+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
-----
You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter '.', the field will be left blank.
-----
Country Name (2 letter code) [XX]:ID
State or Province Name (full name) []:Jakarta
Locality Name (eg, city) [Default City]:Jakarta
Organization Name (eg, company) [Default Company Ltd]:hemimorphite
Organizational Unit Name (eg, section) []:
Common Name (eg, your name or your server's hostname) []:hemimorphite.app
Email Address []:yangsamuel91@gmail.com</code></pre>
        
        <p>Encode the key and certificate data in base64</p>

        <pre><code class="language-bash hljs">cat ~/nginx/ingress.crt | base64</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">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</code></pre>

        <pre><code class="language-bash hljs">cat ~/nginx/ingress.key | base64</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">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</code></pre>

        <p>Create a kubernetes yaml tls secret file for creating a secret named <code>ingress-secret</code></p>

        <pre><code class="language-bash hljs">apiVersion: v1
data: 
  tls.crt: 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
  tls.key: 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
kind: Secret
metadata: 
  name: ingress-secret
  namespace: default
type: kubernetes.io/tls</code></pre>
        
        <p>Apply the tls secret manifest file:</p>

        <pre><code class="language-bash hljs">kubectl create -f ingress-secret.yaml</code></pre>        

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">secret/ingress-secret created</code></pre>

        <p>Create a ingress resource that contains rules with a rewrite annotation for redirecting the request path <code>/something</code> to <code>nginx-server-service</code> on a TCP port of 80 and 443:</code></p>

        <pre><code class="language-bash hljs">apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx-server-ingress
  namespace: default
  annotations:
    nginx.ingress.kubernetes.io/use-regex: "true"
    nginx.ingress.kubernetes.io/rewrite-target: /$2
spec:
  ingressClassName: nginx
  tls:
  - secretName: ingress-secret
  rules:
  - http:
      paths:
      - path: /something(/|$)(.*)
        pathType: ImplementationSpecific
        backend:
          service:
            name: nginx-server-service
            port:
              number: 80</code></pre>

        <p><code>$2</code> means the second regex group of <code>/something(/|$)(.*)</code>, that is <code>(.*)</code>. Any characters captured by <code>(.*)</code> will be assigned to <code>$2</code> in the rewrite-target annotation (eg. <code>/something</code> rewrites to <code>/</code>, <code>/something/new</code> rewrites to <code>/new</code>).</p>

        <p>Apply the ingress resource manifest file:</p>

        <pre><code class="language-bash hljs">kubectl create -f nginx-server-ingress.yaml</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">ingress.networking.k8s.io/nginx-server-ingress created</code></pre>

        <p>To list all ingress resources, use the command:</p>

        <pre><code class="language-bash hljs">kubectl get ingresses</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">NAME                   CLASS   HOSTS   ADDRESS       PORTS     AGE
nginx-server-ingress   nginx   *       172.16.0.22   80, 443   5m51s</code></pre>

        <p>On master node and worker nodes, try to access your application using the cluster IP and the external IP address of <code>ingress-nginx-controller</code> service and the worker node IP address where <code>ingress-nginx-controller</code> pod is running on:</p>

        <pre><code class="language-bash hljs">curl http://10.107.65.165:80/something</code></pre>

        <pre><code class="language-bash hljs">curl -k https://10.107.65.165:443/something</code></pre>

        <pre><code class="language-bash hljs">curl http://192.170.0.1:80/something</code></pre>

        <pre><code class="language-bash hljs">curl -k https://192.170.0.1:443/something</code></pre>

        <pre><code class="language-bash hljs">curl http://172.16.0.22:30609/something</code></pre>

        <pre><code class="language-bash hljs">curl -k https://172.16.0.22:30996/something</code></pre>

        <p>The curl response will return "502 Bad Gateway" since the firewall is blocking the curl request.</p>

        <pre><code class="language-bash hljs">&lt;html&gt;
&lt;head&gt;&lt;title&gt;502 Bad Gateway&lt;/title&gt;&lt;/head&gt;
&lt;body&gt;
&lt;center&gt;&lt;h1&gt;502 Bad Gateway&lt;/h1&gt;&lt;/center&gt;
&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;
&lt;/body&gt;
&lt;/html&gt;</code></pre>

        <p>On worker node, to view the denied packets, use <code>dmesg</code> command:</p>
        
        <pre><code class="language-bash hljs">dmesg | grep -i REJECT</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">[27430.987420] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=6053 DF PROTO=TCP SPT=49274 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27430.988258] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=46895 DF PROTO=TCP SPT=49280 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27430.992201] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=21590 DF PROTO=TCP SPT=49284 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27431.041655] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=50033 DF PROTO=TCP SPT=49290 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27431.042189] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=20178 DF PROTO=TCP SPT=49298 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27431.043061] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=23163 DF PROTO=TCP SPT=49310 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27431.052636] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=23192 DF PROTO=TCP SPT=49320 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27431.979329] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=23193 DF PROTO=TCP SPT=49320 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27431.980157] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=55915 DF PROTO=TCP SPT=49330 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27432.924389] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=55916 DF PROTO=TCP SPT=49330 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27432.925277] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=15452 DF PROTO=TCP SPT=49342 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27433.869141] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=15453 DF PROTO=TCP SPT=49342 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27433.961525] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=43556 DF PROTO=TCP SPT=49348 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27434.932766] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=43557 DF PROTO=TCP SPT=49348 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27434.934358] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=12923 DF PROTO=TCP SPT=49354 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27435.877665] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=12924 DF PROTO=TCP SPT=49354 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27435.878217] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=5169 DF PROTO=TCP SPT=49364 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27436.823613] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=5170 DF PROTO=TCP SPT=49364 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27436.922655] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=9553 DF PROTO=TCP SPT=49380 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27437.886286] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=9554 DF PROTO=TCP SPT=49380 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27437.887678] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=47980 DF PROTO=TCP SPT=49390 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27438.831341] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=47981 DF PROTO=TCP SPT=49390 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27438.831926] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=27766 DF PROTO=TCP SPT=49398 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27439.776698] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=27767 DF PROTO=TCP SPT=49398 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000</code></pre>        
        <p>Create two new zones using calico network interface names (ie. <code>cali7368eccef27</code> and <code>cali9aba158b53d</code>). <code>cali7368eccef27</code> is the zone for <code>ingress-nginx-controller</code> calico pod network and <code>cali9aba158b53d</code> is the zone for <code>nginx-server</code> calico pod network.</p>
        
        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --new-zone cali7368eccef27
sudo firewall-cmd --permanent --new-zone cali9aba158b53d</code></pre>

        <p>Then assign <code>cali7368eccef27</code> zone with <code>cali7368eccef27</code> network interface and assign <code>cali9aba158b53d</code> zone with <code>cali9aba158b53d</code> network interface.</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --change-interface=cali7368eccef27 --zone=cali7368eccef27
sudo firewall-cmd --permanent --change-interface=cali9aba158b53d --zone=cali9aba158b53d</code></pre>

        <p>Create a policy:</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --new-policy cali736cali9abpol</code></pre>

        <p>Set the policies to traffic forwarded from calico network interface <code>cali7368eccef27</code> to calico network interface <code>cali9aba158b53d</code>:</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --policy cali736cali9abpol --add-ingress-zone cali7368eccef27
sudo firewall-cmd --permanent --policy cali736cali9abpol --add-egress-zone cali9aba158b53d</code></pre>        
        <p>To finish up the policy settings, set it to accept new connections by default:</p>
        
        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --policy cali736cali9abpol --set-target ACCEPT</code></pre>

        <p>Then load the new zone and policy into the active runtime state:</p>
        
        <pre><code class="language-bash hljs">sudo firewall-cmd --reload</code></pre>

        <p><b>On master node and worker nodes</b>, test to run the <code>curl</code> command again:</p>

        <pre><code class="language-bash hljs">curl http://10.107.65.165:80/something</code></pre>

        <pre><code class="language-bash hljs">curl -k https://10.107.65.165:443/something</code></pre>

        <pre><code class="language-bash hljs">curl http://192.170.0.1:80/something</code></pre>

        <pre><code class="language-bash hljs">curl -k https://192.170.0.1:443/something</code></pre>

        <pre><code class="language-bash hljs">curl http://172.16.0.22:30609/something</code></pre>

        <pre><code class="language-bash hljs">curl -k https://172.16.0.22:30996/something</code></pre>

        <p>The request should be successful now.</p>

        <pre><code class="language-bash hljs">&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href="http://nginx.org/"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href="http://nginx.com/"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;</code></pre>
        
        <p>Create a port forward from the remote port on the <code>ingress-nginx-controller</code> service to the local port on local address 114.122.37.100 on your machine:</p>

        <pre><code class="language-bash hljs">kubectl port-forward --address 114.122.37.100 service/ingress-nginx-controller -n ingress-nginx 80:80 443:443</code></pre>

        <p>If the port forwarding is successful, the response should look similar to this:</p>

        <pre><code class="language-bash hljs">Forwarding from 114.122.37.100:80 -> 80
Forwarding from 114.122.37.100:443 -> 443</code></pre>

        <p>Now, you can access your application on the local clients LAN by</p>

        <pre><code class="language-bash hljs">curl http://114.122.37.100/something
curl https://114.122.37.100/something</code></pre>        
        
        <p>Opening a web browser and navigating to <code>http://114.122.37.100/something</code> or <code>https://114.122.37.100/something</code>.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-34.jpg" alt="Fedora Server installation">
        </figure>        
    </article>

    <div class="post-tags">
	<div class="title">Tags</div>
	<ul class="tags">
		
		<li><a href="https://hemimorphite.github.io/tag/virtualbox" class="tag">virtualbox</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/fedora" class="tag">fedora</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/linux" class="tag">linux</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/kubernetes" class="tag">kubernetes</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/kubeadm" class="tag">kubeadm</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/containerd" class="tag">containerd</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/project-calico" class="tag">project calico</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/metallb" class="tag">metallb</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/nginx" class="tag">nginx</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/ingress-controller" class="tag">ingress controller</a></li>
		
		<li><a href="https://hemimorphite.github.io/tag/firewalld" class="tag">firewalld</a></li>
		
	</ul>
</div>


    <div class="post-share">
    <div class="title">Share this post</div>
    <ul class="rounded-social-buttons">
        <li><a href="https://www.facebook.com/sharer/sharer.php?u=https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/" class="social-button facebook"><i class="fab fa-facebook-f"></i></a></li>
        <li><a href="http://twitter.com/share?text=Hey+guys%2c+check+this+out!&amp;url=https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/&amp;hashtags=qemu,linux,fedora 38,firewalld,DHCP" class="social-button twitter"><i class="fab fa-twitter"></i></a></li>
        <li><a href="https://www.linkedin.com/sharing/share-offsite/?url=https://hemimorphite.github.io/2023/08/16/setup-fedora-server-38-as-a-dhcp-server/" class="social-button linkedin"><i class="fab fa-linkedin"></i></a></li>
    </ul>
</div>
</div>]]></content><author><name>Samuel Yang</name></author><category term="Tutorials" /><category term="year-2023" /><category term="month-07" /><category term="day-27" /><category term="virtualbox" /><category term="fedora" /><category term="linux" /><category term="kubernetes" /><category term="kubeadm" /><category term="containerd" /><category term="project calico" /><category term="metallb" /><category term="nginx" /><category term="ingress controller" /><category term="firewalld" /><summary type="html"><![CDATA[Learn how to create a Kubernetes cluster with Kubeadm, Containerd, Calico, MetalLB, and NGINX Ingress Controller on Bare Metal Machine Fedora 38 using a step-by-step guide. Simplify the process and get your cluster up and running efficiently.]]></summary></entry></feed>