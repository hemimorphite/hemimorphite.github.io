<!DOCTYPE html>
<html lang="en">
	<head>
		
		
	    <meta charset="utf-8">
	    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
	    <meta name="description" content="Learn how to create a Kubernetes cluster with Kubeadm, Containerd, Calico, MetalLB, and NGINX Ingress Controller on Bare Metal Machine Fedora 38 using a step-by-step guide. Simplify the process and get your cluster up and running efficiently.">
	    <meta name="author" content="Samuel Yang">
	    <link rel="canonical" href="https://hemimorphite.github.io/zh/2023/07/27/create-a-kubernetes-cluster-with-kubeadm-containerd-calico-metallb-and-nginx-ingress-controller-on-bare-metal-machine-fedora-38/">
	    <link rel="icon" type="image/x-icon" href="https://hemimorphite.github.io/zh/favicon.ico">
	    <meta property="fb:app_id" content="771418017802270">
		<meta property="og:title" content="Create a Kubernetes Cluster with Kubeadm, Containerd, Calico, MetalLB, and NGINX Ingress Controller on Bare Metal Machine Fedora 38">
		<meta property="og:type" content="article">
		<meta property="og:site_name" content="Hemimorphite">
		<meta property="og:url" content="https://hemimorphite.github.io/zh/2023/07/27/create-a-kubernetes-cluster-with-kubeadm-containerd-calico-metallb-and-nginx-ingress-controller-on-bare-metal-machine-fedora-38/">
		<meta property="og:description" content="Learn how to create a Kubernetes cluster with Kubeadm, Containerd, Calico, MetalLB, and NGINX Ingress Controller on Bare Metal Machine Fedora 38 using a step-by-step guide. Simplify the process and get your cluster up and running efficiently.">
		<meta property="og:image" content="https://hemimorphite.github.io/assets/images/kubernetes.jpg">

		<meta name="twitter:card" content="summary">
		<meta name="twitter:title" content="Create a Kubernetes Cluster with Kubeadm, Containerd, Calico, MetalLB, and NGINX Ingress Controller on Bare Metal Machine Fedora 38">
		<meta name="twitter:description" content="Learn how to create a Kubernetes cluster with Kubeadm, Containerd, Calico, MetalLB, and NGINX Ingress Controller on Bare Metal Machine Fedora 38 using a step-by-step guide. Simplify the process and get your cluster up and running efficiently.">
		<meta name="twitter:creator" content="Samuel Yang">
		<meta name="twitter:image:src" content="https://hemimorphite.github.io/assets/images/kubernetes.jpg">
	
	    <link href="https://fonts.googleapis.com/css?family=Roboto:100,100i,300,300i,400,400i,500,500i,700,700i,900,900i&display=swap" rel="stylesheet">

	    <title>Create a Kubernetes Cluster with Kubeadm, Containerd, Calico, MetalLB, and NGINX Ingress Controller on Bare Metal Machine Fedora 38</title>

	    <!-- Bootstrap core CSS -->
	    <link rel="stylesheet" href="https://hemimorphite.github.io/assets/vendor/bootstrap/css/bootstrap.css">

	    <!-- Additional CSS Files -->
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
		<link rel="stylesheet" href="https://hemimorphite.github.io/assets/vendor/highlight.js/css/default.min.css">
	    <link rel="stylesheet" href="https://hemimorphite.github.io/assets/vendor/highlight.js/css/themes/monokai-sublime.css">
	    <link rel="stylesheet" href="https://hemimorphite.github.io/assets/css/fontawesome.css">
		<link rel="stylesheet" href="https://hemimorphite.github.io/assets/vendor/latex-css/custom.css">
	    <link rel="stylesheet" href="https://hemimorphite.github.io/assets/css/hemimorphite.css">
  	</head>

  	<body>
  		<!-- ***** Preloader Start ***** -->
	    <!--<div id="preloader">
	        <div class="jumper">
	            <div></div>
	            <div></div>
	            <div></div>
	        </div>
	    </div>-->  
	    <!-- ***** Preloader End ***** -->
	    
	    <!-- Header -->
	    <header>
			<nav class="navbar navbar-expand-lg">
				<div class="container">
					<h2 class="navbar-brand"><a href="https://hemimorphite.github.io/zh/">Hemimorphite</a></h2>
					<button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
						<span class="icon"></span>
					</button>
					<div class="collapse navbar-collapse" id="navbarResponsive">
						<ul class="navbar-nav ms-auto">
							<li class="nav-item ">
								<a class="nav-link" href="https://hemimorphite.github.io/zh/">Home
									
								</a>
							</li>
						
						
						
						

						
						
						

						
						  	
						  	<li class="nav-item ">
								<a class="nav-link" href="https://hemimorphite.github.io/zh/about">About
									
								</a>
							</li>
						  	
						  	
						
						
						

						
						
						

						
						
						

						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						

						
						  	
						  	
						
						
						</ul>
					</div>
				</div>
			</nav>
	    </header>
	    
		<section>
			<div class="container">
				<div class="row">
					<div class="col-12">
						<script type="text/javascript">
							atOptions = {
								'key' : '9cf40e8a46ab80f525196d3376630858',
								'format' : 'iframe',
								'height' : 90,
								'width' : 728,
								'params' : {}
							};
						</script>
						<script type="text/javascript" src="//www.topcreativeformat.com/9cf40e8a46ab80f525196d3376630858/invoke.js"></script>
					</div>
				</div>
			</div>
		</section>

	    <!-- Page Content -->
<section>
	<div class="container">
		<div class="row">
			<div class="col-12">
				<script type="text/javascript">
					atOptions = {
						'key' : '9cf40e8a46ab80f525196d3376630858',
						'format' : 'iframe',
						'height' : 90,
						'width' : 728,
						'params' : {}
					};
				</script>
				<script type="text/javascript" src="//www.topcreativeformat.com/9cf40e8a46ab80f525196d3376630858/invoke.js"></script>
			</div>
			<div class="col-lg-2">
				<a href="https://hemimorphite.github.io/zh/" type="button" class="btn btn-all ms-auto"><i class="fa-solid fa-arrow-left"></i> All posts</a>

				<div>
					<script type="text/javascript">
						atOptions = {
							'key' : '038e6b1f96b2e2f8ee26abfaebf2b477',
							'format' : 'iframe',
							'height' : 300,
							'width' : 160,
							'params' : {}
						};
					</script>
					<script type="text/javascript" src="//www.topcreativeformat.com/038e6b1f96b2e2f8ee26abfaebf2b477/invoke.js"></script>
				</div>
				<div>
					<script type="text/javascript">
						atOptions = {
							'key' : '49182664b5d567ebee9725d9737c0006',
							'format' : 'iframe',
							'height' : 600,
							'width' : 160,
							'params' : {}
						};
					</script>
					<script type="text/javascript" src="//www.topcreativeformat.com/49182664b5d567ebee9725d9737c0006/invoke.js"></script>
				</div>
				<div>
					<script type="text/javascript">
						atOptions = {
							'key' : '038e6b1f96b2e2f8ee26abfaebf2b477',
							'format' : 'iframe',
							'height' : 300,
							'width' : 160,
							'params' : {}
						};
					</script>
					<script type="text/javascript" src="//www.topcreativeformat.com/038e6b1f96b2e2f8ee26abfaebf2b477/invoke.js"></script>
				</div>
				<div>
					<script type="text/javascript">
						atOptions = {
							'key' : '49182664b5d567ebee9725d9737c0006',
							'format' : 'iframe',
							'height' : 600,
							'width' : 160,
							'params' : {}
						};
					</script>
					<script type="text/javascript" src="//www.topcreativeformat.com/49182664b5d567ebee9725d9737c0006/invoke.js"></script>
				</div>
			</div>
			<div class="col-lg-10">
				<div class="blog-post">
    <h2 class="post-title">Create a Kubernetes Cluster with Kubeadm, Containerd, Calico, MetalLB, and NGINX Ingress Controller on Bare Metal Machine Fedora 38</h2>
<div class="post-author">
    <span class="avatar"></span>
    <span class="info"><span class="date">Published July 27, 2023</span><br><span class="name">By Samuel Yang</span></span>
</div>

<figure class="post-image">
    <img src="/assets/images/kubernetes.jpg" alt="Blog Cover">
</figure>

    <article class="post-content">
        <p>A Kubernetes cluster has two main components: the control plane (master node) and data plane (worker node).</p>

        <p>So our cluster will have 1 master node and 1 worker node. The Operating system used for the master node and worker node are Fedora 38.</p>

        <p>We'll assume:</p>

        <ul>
            <li>The first LAN subnet of 172.16.0.0/24</li>
            <li>The second LAN subnet of 114.122.37.0/24</li>
            <li>The first LAN Gateway IP address is 172.16.0.1</li>
            <li>The second LAN Gateway IP address is 114.122.37.1</li>
            <li>The first LAN is for kubernetes nodes</li>
            <li>The second LAN is for local clients</li>
            <li>Master Node's first IP address is 172.16.0.21</li>
            <li>Master Node's second IP address is 114.122.37.100</li>
            <li>Worker Node's IP address is 172.16.0.22</li>
        </ul>

        <h4 class="post-subtitle">Install Fedora Server 38 on Virtualbox</h4>

        <p>Download Fedora image file by visiting <a href="https://fedoraproject.org/server/download/">Fedora's official website</a>.</p>

        <p>Start the virtualbox and click on the <b>New</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-1.jpg" alt="Fedora Server installation">
        </figure>

        <p>Write the name of the VM. Select the OS type and version. Click on <b>Next</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-2.jpg" alt="Fedora Server installation">
        </figure>

        <p>The minimum Kubernetes requirements are: 2 GB or more of RAM and 2 CPUs or more. Check the <b>Enable EFI (special OSes only)</b> and click on <b>Next</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-3.jpg" alt="Fedora Server installation">
        </figure>        

        <p>Select <b>Create a Virtual Hard Disk Now</b> and allocate how much space to your VM. Click on <b>Next</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-4.jpg" alt="Fedora Server installation">
        </figure> 

        <p>Next, click on <b>Finish</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-5.jpg" alt="Fedora Server installation">
        </figure>

        <p>Then, click on <b>Setting</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-6.jpg" alt="Fedora Server installation">
        </figure>

        <p>Select <b>Storage</b> menu and click on <b>Adds optical drive</b>. Navigate to where the Fedora Server ISO image saved. Then click on <b>Ok</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-7.jpg" alt="Fedora Server installation">
        </figure>

        <p>Open terminal and add new nat networks for kubernetes nodes and local clients using <code>VBoxManage</code> command.</p>

        <pre><code class="language-bash hljs">VBoxManage natnetwork add --netname natnet1 --network "172.16.0.0/24" --enable --ipv6 on --dhcp on</code></pre>

        <pre><code class="language-bash hljs">VBoxManage natnetwork add --netname natnet2 --network "114.122.37.0/24" --enable --ipv6 on --dhcp on</code></pre>

        <p>Go back to Virtualbox. Click on <b>Setting</b> button again. Then go to <b>Network</b> menu. Set network adapter 1 type and network adapter 2 type to <b>NAT Network</b>. Then click on <b>Ok</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-8.jpg" alt="Fedora Server installation">
        </figure>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-9.jpg" alt="Fedora Server installation">
        </figure>

        <p>Click on <b>Start</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-10.jpg" alt="Fedora Server installation">
        </figure>

        <p>Select <b>Install Fedora 38</b> on the boot menu.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-11.jpg" alt="Fedora Server installation">
        </figure>

        <p>Select your language and click <b>Continue</b>.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-12.jpg" alt="Fedora Server installation">
        </figure>

        <p>Configure Timezone.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-13.jpg" alt="Fedora Server installation">
        </figure>

        <p>Select your region and city. Then click <b>Done</b>.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-14.jpg" alt="Fedora Server installation">
        </figure>

        <p>Configure Software Selection.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-15.jpg" alt="Fedora Server installation">
        </figure>

        <p>Select minimal install. Then click <b>Done</b>.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-16.jpg" alt="Fedora Server installation">
        </figure>

        <p>Configure installation destination.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-17.jpg" alt="Fedora Server installation">
        </figure>

        <p>On the <b>Storage Configuration</b>, choose <b>custom</b>. Then click <b>Done</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-18.jpg" alt="Fedora Server installation">
        </figure>

        <p>Choose <b>LVM</b> filesystem.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-19.jpg" alt="Fedora Server installation">
        </figure>

        <p>Then click the + symbol to add <b>boot</b> partition and the desired capacity 1GB. Click <b>Add mount point</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-20.jpg" alt="Fedora Server installation">
        </figure>        

        <p>Pick <b>ext4</b> as its default filesystem.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-21.jpg" alt="Fedora Server installation">
        </figure>

        <p>Click the + symbol again to add <b>EFI</b> partition and the desired capacity 512MB. Click <b>Add mount point</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-22.jpg" alt="Fedora Server installation">
        </figure>

        <p>Pick <b>EFI System Partition</b> as its default filesystem.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-23.jpg" alt="Fedora Server installation">
        </figure>

        <p>Click the + symbol again to add <b>root</b> partition and don't need to fill the desired capacity. Click <b>Add mount point</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-24.jpg" alt="Fedora Server installation">
        </figure>

        <p>Pick <b>LVM</b> as its device type and <b>ext4</b> ad its filesystem. Click <b>Done</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-26.jpg" alt="Fedora Server installation">
        </figure>

        <p>A window show up, and click <b>Accept Changes</b>.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-27.jpg" alt="Fedora Server installation">
        </figure>

        <p>Next, configure the user creation.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-28.jpg" alt="Fedora Server installation">
        </figure>

        <p>Create a common user account. Then click <b>Done</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-29.jpg" alt="Fedora Server installation">
        </figure>

        <p>After that, click <b>Begin Installation</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-30.jpg" alt="Fedora Server installation">
        </figure>

        <p>Once the Fedora 38 installation is completed, click <b>Reboot System</b> button.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-31.jpg" alt="Fedora Server installation">
        </figure>
        
        <p>Close the virtual machine. Then select <b>Power off the machine</b> and click on <b>Ok</b> button.</p>

        <p>Open terminal, run the following command to clone the VM and make sure not be run as root:</p>

        <pre><code class="language-bash hljs">VBoxManage clonevm "kubemaster" --basefolder=$HOME --mode=machine --name="kubeworker1" --register</code></pre>

        <ul>
            <li><code>"kubemaster"</code> is the VM name to be cloned</li>
            <li><code>--basefolder=$HOME</code> specifies the name of the folder in which to save the configuration for the new VM</li>
            <li><code>--mode=machine</code> specifies to clone the current state of the existing VM without any snapshots</li>
            <li><code>--name="kubeworker1"</code> specifies a new name for the new VM</li>
            <li><code>--register</code> specifies to automatically register the new clone, so it will show up on the Virtualbox Manager without have to be added again</li>
        </ul>

        <p>If the clone is successful, you will see the output similar to the following:</p>
        
        <pre><code class="language-bash hljs">0%...10%...20%...30%...40%...50%...60%...70%...80%...90%...100%
Machine has been successfully cloned as "kubeworker1"</code></pre>

        <h4 class="post-subtitle">Setting Static IP Address on Master Node</h4>

        <p>Use <code>ip</code> command to identify the name of the ethernet interface you want to configure:</p>

        <pre><code class="language-bash hljs">ip addr show</code></pre>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-33.jpg" alt="Fedora Server installation">
        </figure>

        <p>The interface named <code>lo</code> is the loopback interface, used for processes that communicate via the IP protocol. The important interfaces in the listing are <code>enp0s3</code> and <code>enp0s8</code>, the Ethernet interfaces.</p>

        <p>Add static IP address using <code>nmcli</code> command:</p>

        <pre><code class="language-bash hljs">sudo nmcli connection modify enp0s3 ipv4.addresses 172.16.0.21/24</code></pre>

        <pre><code class="language-bash hljs">sudo nmcli connection modify enp0s8 ipv4.addresses 114.122.37.100/24</code></pre>

        <p>Add the gateway IP:</p>

        <pre><code class="language-bash hljs">sudo nmcli connection modify enp0s3 ipv4.gateway 172.16.0.1</code></pre>

        <pre><code class="language-bash hljs">sudo nmcli connection modify enp0s8 ipv4.gateway 114.122.37.1</code></pre>

        <p>Add the dns IP address:</p>

        <pre><code class="language-bash hljs">sudo nmcli connection modify enp0s3 ipv4.dns 172.16.0.1</code></pre>

        <pre><code class="language-bash hljs">sudo nmcli connection modify enp0s8 ipv4.dns 114.122.37.1</code></pre>

        <p>Change the addressing from DHCP to static.</p>

        <pre><code class="language-bash hljs">sudo nmcli connection modify enp0s3 ipv4.method manual</code></pre>

        <pre><code class="language-bash hljs">sudo nmcli connection modify enp0s8 ipv4.method manual</code></pre>

        <p>To make changes into the effect, disable and enable the connection:</p>

        <pre><code class="language-bash hljs">sudo nmcli connection down enp0s3</code></pre>

        <pre><code class="language-bash hljs">sudo nmcli connection up enp0s3</code></pre>

        <pre><code class="language-bash hljs">sudo nmcli connection down enp0s8</code></pre>

        <pre><code class="language-bash hljs">sudo nmcli connection up enp0s8</code></pre>     

        <h4 class="post-subtitle">Setting hostname on Master Node</h4>

        <p>Set the hostname using the following command:</p>

        <pre><code class="language-bash hljs">sudo echo "kubemaster" > /etc/hostname</code></pre>

        <p>Add the following line to <code>/etc/hosts</code> to map the hostname to a IP address:</p>

        <pre><code class="language-bash hljs">172.16.0.21    kubemaster</code></pre>

        <h4 class="post-subtitle">Disable systemd-resolved service on Master Node</h4>

        <p>The <code>systemd-resolved</code> service can be disabled with:</p>

        <pre><code class="language-bash hljs">sudo systemctl stop systemd-resolved
sudo systemctl disable systemd-resolved
sudo systemctl mask systemd-resolved</code></pre>
        
        <p>Delete the symlink <code>/etc/resolv.conf</code></p>

        <pre><code class="language-bash hljs">sudo rm /etc/resolv.conf</code></pre>

        <p>Create a new <code>/etc/resolv.conf</code> file:</p>

        <pre><code class="language-bash hljs">sudo touch /etc/resolv.conf</code></pre>

        <p>Add the following line to <code>/etc/resolv.conf</code>:</p>

        <pre><code class="language-bash hljs">nameserver 172.16.0.1</code></pre>

        <h4 class="post-subtitle">Install containerd (container runtime) on Master Node</h4>

        <p>First, install the required packages to run kubernetes:</p>

        <pre><code class="language-bash hljs">sudo dnf -y install ethtool
sudo dnf -y install socat
sudo dnf -y install iproute-tc
sudo dnf -y install conntrack
sudo dnf -y install openssl
sudo dnf -y install tar
sudo dnf -y install net-tools</code></pre>

        <p>Next, you need to install a container runtime into each node in the cluster so that pods can run there.</p>

        <p>Before install containerd, you need to enable <code>overlay</code> and <code>br_netfilter</code> kernel modules, and <code>net.bridge.bridge-nf-call-iptables</code>, <code>net.bridge.bridge-nf-call-ip6tables</code>, and <code>net.ipv4.ip_forward</code> kernel parameter.</p>

        <p><code>overlay</code> kernel module is required to enable Overlay filesystem support.</p>

        <p><code>br_netfilter</code> kernel module is required to enable transparent masquerading and to facilitate Virtual Extensible LAN (VxLAN) traffic for communication between Kubernetes pods across the cluster.</p>

        <p><code>net.bridge.bridge-nf-call-iptables</code> and <code>net.bridge.bridge-nf-call-ip6tables</code> kernel parameters is required to control whether or not packets traversing the bridge are sent to iptables for processing. In the case of using bridges to connect virtual machines to the network, generally such processing is *not* desired, as it results in guest traffic being blocked due to host iptables rules that only account for the host itself, and not for the guests.</p>

        <p><code>net.ipv4.ip_forward</code> kernel parameter is required to enable IP forwarding.</p>

        <p>Create a file in the <code>/etc/modules-load.d/</code> directory which contains kernel module names to be loaded at boot time.</p>

        <pre><code class="language-bash hljs">cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF</code></pre>

        <p>Load <code>overlay</code> and <code>br_netfilter</code> kernel modules now using the <code>modprobe</code> command.</p>  

        <pre><code class="language-bash hljs">sudo modprobe overlay
sudo modprobe br_netfilter</code></pre>

        <p>Create a file in the <code>/etc/sysctl.d/</code> directory which contains <code>net.bridge.bridge-nf-call-iptables</code>, <code>net.bridge.bridge-nf-call-ip6tables</code>, and <code>net.ipv4.ip_forward</code> kernel parameters to be set at boot time.</p>

        <pre><code class="language-bash hljs">cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF</code></pre>        

        <p>Apply the kernel parameters without reboot using <code>sysctl</code> command.</p>

        <pre><code class="language-bash hljs">sudo sysctl --system</code></pre>

        <p>Verify that the <code>br_netfilter</code>, <code>overlay</code> modules are loaded by running the following commands:</p>

        <pre><code class="language-bash hljs">lsmod | grep br_netfilter
lsmod | grep overlay</code></pre>
        
        <p>Verify that the <code>net.bridge.bridge-nf-call-iptables</code>, <code>net.bridge.bridge-nf-call-ip6tables</code>, and <code>net.ipv4.ip_forward</code> system variables are set to <code>1</code> in your <code>sysctl</code> config by running the following command:</p>

        <pre><code class="language-bash hljs">sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward</code></pre>

        <p>Download the containerd from <a href="https://github.com/containerd/containerd/releases">https://github.com/containerd/containerd/releases</a>.</p>

        <pre><code class="language-bash hljs">curl -OJL https://github.com/containerd/containerd/releases/download/v1.7.2/containerd-1.7.2-linux-amd64.tar.gz</code></pre>
        
        <p>Extract the containerd under <code>/usr/local</code></p>

        <pre><code class="language-bash hljs">sudo tar Cxzvf /usr/local containerd-1.7.2-linux-amd64.tar.gz</code></pre>
        
        <p>Download the <code>containerd.service</code> unit file from <a href="https://raw.githubusercontent.com/containerd/containerd/main/containerd.service">https://raw.githubusercontent.com/containerd/containerd/main/containerd.service</a> into <code>/usr/lib/systemd/system/</code> directory</p>

        <pre><code class="language-bash hljs">curl -o /usr/lib/systemd/system/containerd.service https://raw.githubusercontent.com/containerd/containerd/main/containerd.service</code></pre>

        <p>Reload systemd manager configuration and enable the containerd service.</p>
        
        <pre><code class="language-bash hljs">sudo systemctl daemon-reload
sudo systemctl enable --now containerd</code></pre>
        
        <p>Download the <code>runc</code> binary from <a href="https://github.com/opencontainers/runc/releases">https://github.com/opencontainers/runc/releases</a> and install it as <code>/usr/local/sbin/runc</code>.</p>
        
        <pre><code class="language-bash hljs">curl -OJL https://github.com/opencontainers/runc/releases/download/v1.1.7/runc.amd64
sudo install -m 755 runc.amd64 /usr/local/sbin/runc</code></pre>

        <p>Download the cni-plugins from <a href="https://github.com/containernetworking/plugins/releases">https://github.com/containernetworking/plugins/releases</a> and extract it under <code>/opt/cni/bin</code></p>

        <pre><code class="language-bash hljs">curl -OJL https://github.com/containernetworking/plugins/releases/download/v1.3.0/cni-plugins-linux-amd64-v1.3.0.tgz
sudo mkdir -p /opt/cni/bin
sudo tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.3.0.tgz</code></pre>
        
        <p>Once, we have successfully installed the Containerd. We need to load the Containerd configurations.</p>

        <p>Create <code>/etc/containerd</code> directory and generate the confguration file using <code>containerd</code> command.</p>

        <pre><code class="language-bash hljs">sudo mkdir -p /etc/containerd
sudo containerd config default > /etc/containerd/config.toml</code></pre>

        <p>Open the configuration file <code>/etc/containerd/config.toml</code> and set <code>SystemdCgroup = true</code></p>

        <pre><code class="language-bash hljs">[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
  ...
  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
    SystemdCgroup = true</code></pre> 

        <p>To apply the changes, restart the <code>containerd</code> service.</p>

        <pre><code class="language-bash hljs">sudo systemctl restart containerd</code></pre>

        <h4 class="post-subtitle">Install kubectl, kubeadm, and kubelet on Master Node</h4>

        <p>You will install these packages on all of your nodes (master node and worker node):</p>

        <ul>
            <li><code>kubeadm</code>: the command to bootstrap the cluster</li>
            <li><code>kubelet</code>: the component that runs on all of the machines in your cluster and does things like starting pods and containers</li>
            <li><code>kubectl</code>: the command line util to talk to your cluster.</li>
        </ul>

        <p>Kubernetes versions are expressed as <b>x.y.z</b>, where <b>x</b> is the major version, <b>y</b> is the minor version, and <b>z</b> is the patch version.</p>

        <p><code>kubelet</code>, <code>kube-proxy</code>, <code>kube-controller-manager</code>, <code>kube-scheduler</code>, and <code>cloud-controller-manager</code> version must not be newer than <code>kube-apiserver</code> version.</p>

        <p>The requirements of <code>kubeadm</code> before you begin to install.</p>

        <ul>
            <li>2 GB or more of RAM per node (master node or worker node)</li>
            <li>2 CPUs or more</li>
            <li>Check hostname, machine ID, MAC address, and product_uuid for every node are unique</li>
            <li>Check required ports</li>
            <li>Swap disabled</li>
        </ul>

        <p>You can get the hostname using the command <code>cat /etc/hostname</code>.</p>

        <p>To change the hostname, edit the <code>/etc/hostname</code> file.</p>

        <p>You can get the machine ID using the command <code>cat /etc/machine-id</code>.</p>

        <p>To generate the new machine ID, first remove the <code>/etc/machine-id</code> file. Then generate the new one using this command:</p>

        <pre><code class="language-bash hljs">systemd-machine-id-setup</code></pre>

        <p>You can get the MAC address of the network interfaces using the command <code>ip link</code>.</p>

        <p>To generate the new MAC address of the network interfaces, run the following command on terminal.</p>

        <pre><code class="language-bash hljs">VBoxManage modifyvm "kubemaster" --macaddress1 auto</code></pre>

        <ul>
            <li><code>"kubemaster"</code> is the VM name</li>
            <li><code>--macaddress1</code> is the MAC address of the first network interface</li>
        </ul>

        <p>To list the network interfaces of your VM, run the following command:</p>

        <pre><code class="language-bash hljs">VBoxManage showvminfo kubemaster | grep NIC</code></pre>        
        <p>The output should be similar to:</p>

        <pre><code class="language-bash hljs">NIC 1:                       MAC: 080027EECD2E, Attachment: NAT Network 'natnet1', Cable connected: on, Trace: off (file: none), Type: 82540EM, Reported speed: 0 Mbps, Boot priority: 0, Promisc Policy: deny, Bandwidth group: none
NIC 2:                       disabled
NIC 3:                       disabled
NIC 4:                       disabled
NIC 5:                       disabled
NIC 6:                       disabled
NIC 7:                       disabled
NIC 8:                       disabled</code></pre>
        
        <p>You can get the <code>product_uuid</code> using the command <code>sudo cat /sys/class/dmi/id/product_uuid</code>.</p>

        <p>To generate the new <code>product_uuid</code>, run the command three times on terminal and make sure not be run as root:</p>

        <pre><code class="language-bash hljs">VBoxManage internalcommands sethduuid "$HOME/kubemaster/kubemaster.vdi"
        VBoxManage internalcommands sethduuid "$HOME/kubemaster/kubemaster.vdi"
        VBoxManage internalcommands sethduuid "$HOME/kubemaster/kubemaster.vdi"</code></pre>

        <p>The output should be similar to:</p>

        <pre><code class="language-bash hljs">UUID changed to: 35ddda67-7a61-4c6f-b6ed-31a4a442af8b
UUID changed to: 2fd3c5d1-362e-4db5-a67b-84d67c57cde0
UUID changed to: 23338733-eac6-471e-bbf5-4f9414fcca61</code></pre>
        
        <p>The first UUID will be used as <code>Machine uuid</code> and <code>Hardware uuid</code>.</p>

        <p>Edit <code>$HOME/kubemaster/kubemaster.vbox</code> file. Find the <code>Machine uuid</code> and <code>Hardware uuid</code> and change to <code>35ddda67-7a61-4c6f-b6ed-31a4a442af8b</code>.</p>

        <pre><code class="language-bash hljs">&lt;Machine uuid="{35ddda67-7a61-4c6f-b6ed-31a4a442af8b}" name="kubeworker1" OSType="RedHat_64" snapshotFolder="Snapshots" lastStateChange="2023-07-13T10:05:46Z"&gt;
    ...
&lt;/Machine&gt;</code></pre>

        <pre><code class="language-bash hljs">&lt;Hardware uuid="{35ddda67-7a61-4c6f-b6ed-31a4a442af8b}"&gt;
    ...
&lt;/Hardware&gt;</code></pre>

        <p>The second UUID will be used as <code>System uuid</code>. To set <code>System uuid</code>, run the command:</p>

        <pre><code class="language-bash hljs">VBoxManage setextradata "kubemaster" "VBoxInternal/Devices/efi/0/Config/DmiSystemUuid" "2fd3c5d1-362e-4db5-a67b-84d67c57cde0"</code></pre>
        
        <p>The third UUID will be used as <code>Harddisk uuid</code> and <code>Image uuid</code>.</p>

        <p>Edit <code>$HOME/kubemaster/kubemaster.vbox</code> file. Find the <code>Harddisk uuid</code> and <code>Image uuid</code> and change to <code>941f4e75-d29f-4c58-93d5-53172c07e50e</code>.</p>

        <pre><code class="language-bash hljs">&lt;HardDisks&gt;
        &lt;HardDisk uuid="{941f4e75-d29f-4c58-93d5-53172c07e50e}" location="kubemaster.vdi" format="VDI" type="Normal"/&gt;
      &lt;/HardDisks&gt;</code></pre>

        <pre><code class="language-bash hljs">&lt;StorageController name="NVMe" type="NVMe" PortCount="1" useHostIOCache="false" Bootable="true"&gt;
          &lt;AttachedDevice type="HardDisk" hotpluggable="false" port="0" device="0"&gt;
            &lt;Image uuid="{941f4e75-d29f-4c58-93d5-53172c07e50e}"/&gt;
          &lt;/AttachedDevice&gt;
        &lt;/StorageController&gt;</code></pre>        

        <p>Detach and remove the storage medium from the VM:</p>

        <pre><code class="language-bash hljs">VBoxManage storageattach kubemaster --storagectl "NVMe" --port 0 --device 0 --medium none</code></pre>

        <ul>
            <li><code>fedoraworker1</code> specifies the VM name</li>
            <li><code>--storagectl</code> specifies the name of the storage controller</li>
            <li><code>--port</code> specifies the number of the storage controller's port</li>
            <li><code>--device</code> specifies the number of the port's device </li>
            <li><code>--medium none</code> specifies to remove storage medium</li>
        </ul>

        <p>Remove the storage medium from VirtualBox media registry.</p>

        <pre><code class="language-bash hljs">VBoxManage closemedium disk "$HOME/kubemaster/kubemaster.vdi"</code></pre>

        <p>Then attach the storage medium to the VM:</p>

        <pre><code class="language-bash hljs">VBoxManage storageattach "kubemaster" --storagectl "NVMe" --port 0 --device 0 --type hdd --medium $HOME/kubemaster/kubemaster.vdi</code></pre>                
        
        <p>The ports required for master node are:</p>

        <table class="table table-bordered border-primary">
            <thead>
                <th>Protocol</th>
                <th>Port Range</th>
                <th>Used By</th>
            </thead>
            <tbody>
            <tr>
                <td>TCP</td>
                <td>6443</td>
                <td>kube-apiserver</td>
            </tr>
            <tr>
                <td>TCP</td>
                <td>2379-2380</td>
                <td>kube-apiserver, etcd</td>
            </tr>
            <tr>
                <td>TCP</td>
                <td>10250</td>
                <td>Kubelet API</td>
            </tr>
            <tr>
                <td>TCP</td>
                <td>10259</td>
                <td>kube-scheduler</td>
            </tr>
            <tr>
                <td>TCP</td>
                <td>10257</td>
                <td>kube-controller-manager</td>
            </tr>
            </tbody>
        </table>

        <p>The ports are required to be open in the firewall.</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --add-port=6443/tcp
sudo firewall-cmd --permanent --add-port=2379-2380/tcp
sudo firewall-cmd --permanent --add-port=10250/tcp
sudo firewall-cmd --permanent --add-port=10259/tcp
sudo firewall-cmd --permanent --add-port=10257/tcp
sudo firewall-cmd --permanent --add-port=9153/tcp
sudo firewall-cmd --permanent --add-port=53/tcp
sudo firewall-cmd --permanent --add-port=53/udp
sudo firewall-cmd --permanent --add-masquerade
sudo firewall-cmd --reload</code></pre>
        
        <p>Swap must be disabled in order for the kubelet to work properly. To check if swap is active:</p>

        <ul>
            <li><code>cat /proc/meminfo | grep Swap</code> to see total swap, and free swap</li>
            <li><code>cat /proc/swaps</code> and <code>swapon -s</code> to see which swap devices are being used</li>
            <li><code>vmstat</code> for current virtual memory statistics</li>
        </ul>

        <p>To permanently disable swap:</p>

        <ul>
            <li>Open the <code>/etc/fstab</code> file, search for a swap line and add a # (hashtag) sign in front of the line to comment on the entire line</li>
            <li>The swap-on-zram feature can be disabled with:<br>
                <code>sudo systemctl stop systemd-zram-setup@zram0</code><br>
                <code>sudo systemctl disable systemd-zram-setup@zram0</code><br>
                <code>sudo systemctl mask systemd-zram-setup@zram0</code>
            </li>
            <li>Run <code>swapoff -va</code> to disable memory swapping</li>
        </ul>

        <p>Download the latest release <code>kubectl</code> with the command:</p>

        <pre><code class="language-bash hljs">curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"</code></pre>

        <p>Download the <code>kubectl</code> checksum file:</p>

        <pre><code class="language-bash hljs">curl -LO "https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"</code></pre>   
        
        <p>Validate the <code>kubectl</code> binary against the checksum file:</p>

        <pre><code class="language-bash hljs">echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check</code></pre>

        <p>If valid, the output is:</p>

        <pre><code class="language-bash hljs">kubectl: OK</code></pre>   
        
        <p>Install <code>kubectl</code>:</p>

        <pre><code class="language-bash hljs">sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl</code></pre>

        <p>Check the <code>kubectl</code> version you installed:</p> 

        <pre><code class="language-bash hljs">kubectl version --short</code></pre>

        <p>Install CNI plugins (required for most pod network):</p>

        <pre><code class="language-bash hljs">CNI_PLUGINS_VERSION="v1.3.0"
ARCH="amd64"
DEST="/opt/cni/bin"
sudo mkdir -p "$DEST"
curl -L "https://github.com/containernetworking/plugins/releases/download/${CNI_PLUGINS_VERSION}/cni-plugins-linux-${ARCH}-${CNI_PLUGINS_VERSION}.tgz" | sudo tar -C "$DEST" -xz</code></pre>

        <p>Define the directory to download command files:</p>      
        
        <pre><code class="language-bash hljs">DOWNLOAD_DIR="/usr/local/bin"
sudo mkdir -p "$DOWNLOAD_DIR"</code></pre>

        <p>Install <code>crictl</code> (required for <code>kubeadm</code>/<code>kubelet</code> Container Runtime Interface (CRI)).</p>

        <pre><code class="language-bash hljs">CRICTL_VERSION="v1.27.0"
ARCH="amd64"
curl -L "https://github.com/kubernetes-sigs/cri-tools/releases/download/${CRICTL_VERSION}/crictl-${CRICTL_VERSION}-linux-${ARCH}.tar.gz" | sudo tar -C $DOWNLOAD_DIR -xz</code></pre>
        
        <p>Install <code>kubeadm</code>, <code>kubelet</code>, <code>kubectl</code> and add a <code>kubelet</code> systemd service:</p>

        <pre><code class="language-bash hljs">RELEASE="$(curl -sSL https://dl.k8s.io/release/stable.txt)"
ARCH="amd64"
cd $DOWNLOAD_DIR
sudo curl -L --remote-name-all https://dl.k8s.io/release/${RELEASE}/bin/linux/${ARCH}/{kubeadm,kubelet}
sudo chmod +x {kubeadm,kubelet}
cd -</code></pre>

        <pre><code class="language-bash hljs">RELEASE_VERSION="v0.15.1"
curl -sSL "https://raw.githubusercontent.com/kubernetes/release/${RELEASE_VERSION}/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service" | sed "s:/usr/bin:${DOWNLOAD_DIR}:g" | sudo tee /etc/systemd/system/kubelet.service
sudo mkdir -p /etc/systemd/system/kubelet.service.d
curl -sSL "https://raw.githubusercontent.com/kubernetes/release/${RELEASE_VERSION}/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf" | sed "s:/usr/bin:${DOWNLOAD_DIR}:g" | sudo tee /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code></pre>
        
        <p>Create <code>/etc/kubernetes/manifests</code> directory where kubelet should look for static Pod manifests.</p>

        <pre><code class="language-bash hljs">mkdir -p /etc/kubernetes/manifests</code></pre>

        <p>Enable and start <code>kubelet</code>:</p>

        <pre><code class="language-bash hljs">sudo systemctl enable --now kubelet</code></pre>

        <p>Now let's initialize the cluster on master node by passing a flag that is later needed for the container network.</p>
        
        <pre><code class="language-bash hljs">sudo kubeadm init --pod-network-cidr=192.168.0.0/16 --control-plane-endpoint=kubemaster</code></pre>

        <p>If the cluster initialization is success, it will print output similar to follow:</p>

        <pre><code class="language-bash hljs">[init] Using Kubernetes version: v1.27.3
[preflight] Running pre-flight checks
    [WARNING Firewalld]: firewalld is active, please ensure ports [6443 10250] are open or your cluster may not function correctly
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
W0719 21:22:19.276005     867 checks.go:835] detected that the sandbox image "registry.k8s.io/pause:3.8" of the container runtime is inconsistent with that used by kubeadm. It is recommended that using "registry.k8s.io/pause:3.9" as the CRI sandbox image.
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubemaster kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.16.0.21]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [kubemaster localhost] and IPs [172.16.0.21 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [kubemaster localhost] and IPs [172.16.0.21 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 18.503832 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node kubemaster as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kubemaster as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: nr9zk2.2ysjxp5tceb5h83t
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join kubemaster:6443 --token nr9zk2.2ysjxp5tceb5h83t \
    --discovery-token-ca-cert-hash sha256:ff308c648fefa963b3f5f7ac320141b33c761a58ed62c0087ae382db3b98e653 \
    --control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join kubemaster:6443 --token nr9zk2.2ysjxp5tceb5h83t \
    --discovery-token-ca-cert-hash sha256:ff308c648fefa963b3f5f7ac320141b33c761a58ed62c0087ae382db3b98e653</code></pre>

        <p>To start using your cluster, you need to run the following as a <b>regular user</b>:</p>

        <pre><code class="language-bash hljs">sudo mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config</code></pre>

        <p>Alternatively, if you are the root user, you can run:</p>

        <pre><code class="language-bash hljs">export KUBECONFIG=/etc/kubernetes/admin.conf</code></pre>

        <p>Check if kubectl is working:</p>

        <pre><code class="language-bash hljs">kubectl get nodes</code></pre>

        <p>If kubectl is working then it will show output similar to:</p>

        <pre><code class="language-bash hljs">NAME         STATUS     ROLES           AGE    VERSION
kubemaster   NotReady   control-plane   4m2s   v1.27.3</code></pre>
        
        <p>kubectl taint node fedoramaster node.kubernetes.io/not-ready:NoSchedule-</p>
        
        <p>The status will be <b>NotReady</b> as we haven't set up our networking yet.</p> 

        <p>To list all namespaces, use the command:</p>

        <pre><code class="language-bash hljs">kubectl get namespaces</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">NAME              STATUS   AGE
default           Active   8m26s
kube-node-lease   Active   8m26s
kube-public       Active   8m27s
kube-system       Active   8m27s</code></pre>
        
        <p>To list all resources in <code>default</code> namespace, use the command:</p>

        <pre><code class="language-bash hljs">kubectl get all</code></pre>

        <pre><code class="language-bash hljs">NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   5m37s</code></pre>

        <p>To list all resources in <code>kube-system</code> namespace, use the command:</p>

        <pre><code class="language-bash hljs">kubectl get all -n kube-system</code></pre>
        
        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">NAME                                     READY   STATUS    RESTARTS   AGE
pod/coredns-5d78c9869d-ckcr4             0/1     Pending   0          2m31s
pod/coredns-5d78c9869d-fljqv             0/1     Pending   0          2m31s
pod/etcd-kubemaster                      1/1     Running   0          2m44s
pod/kube-apiserver-kubemaster            1/1     Running   0          2m43s
pod/kube-controller-manager-kubemaster   1/1     Running   0          2m43s
pod/kube-proxy-6bj2x                     1/1     Running   0          2m32s
pod/kube-scheduler-kubemaster            1/1     Running   0          2m43s

NAME               TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
service/kube-dns   ClusterIP   10.96.0.10   &lt;none&gt;        53/UDP,53/TCP,9153/TCP   2m43s

NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/kube-proxy   1         1         1       1            1           kubernetes.io/os=linux   2m43s

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/coredns   0/2     2            0           2m44s

NAME                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/coredns-5d78c9869d   2         2         0       2m32s</code></pre>
        
        <h4 class="post-subtitle">Install calico (container network) on Master Node</h4>

        <p>The ports required for master node are:</p>

        <table class="table table-bordered border-primary">
            <thead>
                <th>Protocol</th>
                <th>Port Range</th>
                <th>Used By</th>
            </thead>
            <tbody>
            <tr>
                <td>TCP</td>
                <td>179</td>
                <td>Calico BGP Port</td>
            </tr>
            <tr>
                <td>TCP</td>
                <td>5473</td>
                <td>Calico Typha</td>
            </tr>
            </tbody>
        </table>

        <p>The ports are required to be open in the firewall.</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --add-port=179/tcp
sudo firewall-cmd --permanent --add-port=5473/tcp
sudo firewall-cmd --reload</code></pre>

        <p>Download the tigera operator manifest file.</p>

        <pre><code class="language-bash hljs">curl -OJL https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/tigera-operator.yaml</code></pre>

        <p>Apply the manifest using the following command:</p>

        <pre><code class="language-bash hljs">kubectl create -f tigera-operator.yaml</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">namespace/tigera-operator created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgpfilters.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/apiservers.operator.tigera.io created
customresourcedefinition.apiextensions.k8s.io/imagesets.operator.tigera.io created
customresourcedefinition.apiextensions.k8s.io/installations.operator.tigera.io created
customresourcedefinition.apiextensions.k8s.io/tigerastatuses.operator.tigera.io created
serviceaccount/tigera-operator created
clusterrole.rbac.authorization.k8s.io/tigera-operator created
clusterrolebinding.rbac.authorization.k8s.io/tigera-operator created
deployment.apps/tigera-operator created</code></pre>
        
        <p>List all resources in <code>tigera-operator</code> namespace, use the following command:</p>

        <pre><code class="language-bash hljs">kubectl get all -n tigera-operator</code></pre>
        
        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">NAME                                  READY   STATUS    RESTARTS   AGE
pod/tigera-operator-5f4668786-mchkg   1/1     Running   0          7m17s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/tigera-operator   1/1     1            1           7m17s

NAME                                        DESIRED   CURRENT   READY   AGE
replicaset.apps/tigera-operator-5f4668786   1         1         1       7m17s</code></pre>
        
        <p>Download the custom resources necessary to configure Calico.</p>

        <pre><code class="language-bash hljs">curl -OJL https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/custom-resources.yaml</code></pre>

        <p>Apply the manifest using the following command:</p>

        <pre><code class="language-bash hljs">kubectl create -f custom-resources.yaml</code></pre>

        <pre><code class="language-bash hljs">installation.operator.tigera.io/default created
apiserver.operator.tigera.io/default created</code></pre>
        
        <p>List all resources in <code>calico-system</code> namespace, use the following command:</p>

        <pre><code class="language-bash hljs">kubectl get all -n calico-system</code></pre>
        
        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">NAME                                           READY   STATUS              RESTARTS   AGE
pod/calico-kube-controllers-6cb95b98f5-bpx9g   0/1     Pending             0          9s
pod/calico-node-phxjc                          0/1     Init:0/2            0          9s
pod/calico-typha-69856d85d5-frh2j              0/1     ContainerCreating   0          9s
pod/csi-node-driver-8k2x6                      0/2     ContainerCreating   0          9s

NAME                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/calico-typha   ClusterIP   10.100.194.232   &lt;none&gt;        5473/TCP   9s

NAME                             DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/calico-node       1         1         0       1            0           kubernetes.io/os=linux   9s
daemonset.apps/csi-node-driver   1         1         0       1            0           kubernetes.io/os=linux   9s

NAME                                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/calico-kube-controllers   0/1     1            0           9s
deployment.apps/calico-typha              0/1     1            0           9s

NAME                                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/calico-kube-controllers-6cb95b98f5   1         1         0       9s
replicaset.apps/calico-typha-69856d85d5              1         1         0       9s</code></pre>

        <p>You can see that all the resources are still not in running state. <b>Wait for everything to be in running state</b>.</p>

        <p>At this point Kubernetes nodes will become <code>Ready</code> because Kubernetes has a networking provider and configuration installed.</p>

        <pre><code class="language-bash hljs">kubectl get nodes</code></pre>

        <p>The output should be similar to:</p>

        <pre><code class="language-bash hljs">NAME         STATUS   ROLES           AGE   VERSION
kubemaster   Ready    control-plane   44m   v1.27.3</code></pre>
        
        <pre><code class="language-bash hljs">kubectl get all -n kube-system</code></pre>

        <p>The output should be similar to:</p>

        <pre><code class="language-bash hljs">NAME                                     READY   STATUS    RESTARTS   AGE
pod/coredns-5d78c9869d-ngzq8             1/1     Running   0          43m
pod/coredns-5d78c9869d-x7pbv             1/1     Running   0          43m
pod/etcd-kubemaster                      1/1     Running   0          43m
pod/kube-apiserver-kubemaster            1/1     Running   0          43m
pod/kube-controller-manager-kubemaster   1/1     Running   0          43m
pod/kube-proxy-5frjt                     1/1     Running   0          43m
pod/kube-scheduler-kubemaster            1/1     Running   0          43m

NAME               TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
service/kube-dns   ClusterIP   10.96.0.10   &lt;none&gt;        53/UDP,53/TCP,9153/TCP   43m

NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/kube-proxy   1         1         1       1            1           kubernetes.io/os=linux   43m

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/coredns   2/2     2            2           43m

NAME                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/coredns-5d78c9869d   2         2         2       43m</code></pre>
        
        <p>Use the following command to download the <code>calicoctl</code> binary.</p>
        
        <pre><code class="language-bash hljs">curl -L https://github.com/projectcalico/calico/releases/latest/download/calicoctl-linux-amd64 -o calicoctl</code></pre>

        <p>Move the <code>calicoctl</code> binary to directory <code>/usr/local/bin/</code>:</p>

        <pre><code class="language-bash hljs">mv calicoctl /usr/local/bin/</code></pre>

        <p>Set the file to be executable:</p>

        <pre><code class="language-bash hljs">chmod +x /usr/local/bin/calicoctl</code></pre>

        <h4 class="post-subtitle">Change MAC Address of the network interface on Worker Node</h4>

        <p>First, make sure your Worker Node VM is off.</p>

        <p>Then to generate the new MAC address of the network interfaces, use <code>VBoxManage modifyvm</code>:</p>

        <pre><code class="language-bash hljs">VBoxManage modifyvm "kubeworker1" --macaddress1 auto</code></pre>

        <ul>
            <li><code>"kubeworker1"</code> is the VM name</li>
            <li><code>--macaddress1</code> is the MAC address of the first network interface</li>
        </ul>

        <p>Set NIC to NAT Network mode:</p>
        
        <pre><code class="language-bash hljs">VBoxManage modifyvm "kubeworker1" --nic1 natnetwork</code></pre>

        <p>Set NIC connected to <code>natnet1</code> network:</p>

        <pre><code class="language-bash hljs">VBoxManage modifyvm "kubeworker1" --natnetwork1 "natnet1"</code></pre>

        <h4 class="post-subtitle">Change Hardware ID on Worker Node</h4>

        <p>Make sure your Worker Node VM is off.</p>

        <p>To generate the new <code>product_uuid</code>, run the command three times on terminal and make sure not be run as root:</p>

        <pre><code class="language-bash hljs">VBoxManage internalcommands sethduuid "$HOME/kubeworker1/kubeworker1.vdi"
VBoxManage internalcommands sethduuid "$HOME/kubeworker1/kubeworker1.vdi"
VBoxManage internalcommands sethduuid "$HOME/kubeworker1/kubeworker1.vdi"</code></pre>

        <p>The output should be similar to:</p>

        <pre><code class="language-bash hljs">UUID changed to: 659d2b89-91aa-4b42-9e32-1a0c6ca73fb9
UUID changed to: 48ac2a50-0c97-4aab-a5a4-694bf5700359
UUID changed to: 0b43b0b9-b2b9-456c-b271-3619069072c5</code></pre>
        
        <p>The first UUID will be used as <code>Machine uuid</code> and <code>Hardware uuid</code>.</p>

        <p>Edit <code>$HOME/kubeworker1/kubeworker1.vbox</code> file. Find the <code>Machine uuid</code> and <code>Hardware uuid</code> and change to <code>659d2b89-91aa-4b42-9e32-1a0c6ca73fb9</code>.</p>

        <pre><code class="language-bash hljs">&lt;Machine uuid="{659d2b89-91aa-4b42-9e32-1a0c6ca73fb9}" name="kubeworker1" OSType="RedHat_64" snapshotFolder="Snapshots" lastStateChange="2023-07-13T10:05:46Z"&gt;
    ...
&lt;/Machine&gt;</code></pre>

        <pre><code class="language-bash hljs">&lt;Hardware uuid="{659d2b89-91aa-4b42-9e32-1a0c6ca73fb9}"&gt;
    ...
&lt;/Hardware&gt;</code></pre>

        <p>The second UUID will be used as <code>System uuid</code>. To set <code>System uuid</code>, run the command:</p>

        <pre><code class="language-bash hljs">VBoxManage setextradata "kubeworker1" "VBoxInternal/Devices/efi/0/Config/DmiSystemUuid" "48ac2a50-0c97-4aab-a5a4-694bf5700359"</code></pre>
        
        <p>The third UUID will be used as <code>Harddisk uuid</code> and <code>Image uuid</code>.</p>

        <p>Edit <code>$HOME/kubeworker1/kubeworker1.vbox</code> file. Find the <code>Harddisk uuid</code> and <code>Image uuid</code> and change to <code>0b43b0b9-b2b9-456c-b271-3619069072c5</code>.</p>

        <pre><code class="language-bash hljs">&lt;HardDisks&gt;
        &lt;HardDisk uuid="{0b43b0b9-b2b9-456c-b271-3619069072c5}" location="kubeworker1.vdi" format="VDI" type="Normal"/&gt;
      &lt;/HardDisks&gt;</code></pre>

        <pre><code class="language-bash hljs">&lt;StorageController name="NVMe" type="NVMe" PortCount="1" useHostIOCache="false" Bootable="true"&gt;
          &lt;AttachedDevice type="HardDisk" hotpluggable="false" port="0" device="0"&gt;
            &lt;Image uuid="{0b43b0b9-b2b9-456c-b271-3619069072c5}"/&gt;
          &lt;/AttachedDevice&gt;
        &lt;/StorageController&gt;</code></pre>

        <p>Detach and remove the storage medium from the VM:</p>

        <pre><code class="language-bash hljs">VBoxManage storageattach kubeworker1 --storagectl "NVMe" --port 0 --device 0 --medium none</code></pre>

        <ul>
            <li><code>kubeworker1</code> specifies the VM name</li>
            <li><code>--storagectl</code> specifies the name of the storage controller</li>
            <li><code>--port</code> specifies the number of the storage controller's port</li>
            <li><code>--device</code> specifies the number of the port's device </li>
            <li><code>--medium none</code> specifies to remove storage medium</li>
        </ul>

        <p>Remove the storage medium from VirtualBox media registry.</p>

        <pre><code class="language-bash hljs">VBoxManage closemedium disk "$HOME/kubeworker1/kubeworker1.vdi"</code></pre>

        <p>Then attach the storage medium to the VM:</p>

        <pre><code class="language-bash hljs">VBoxManage storageattach "kubeworker1" --storagectl "NVMe" --port 0 --device 0 --type hdd --medium $HOME/kubeworker1/kubeworker1.vdi</code></pre>

        <h4 class="post-subtitle">Setting Static IP Address on Worker Node</h4>

        <p>Use <code>ip</code> command to identify the name of the ethernet interface you want to configure:</p>

        <pre><code class="language-bash hljs">ip addr show</code></pre>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-33.jpg" alt="Fedora Server installation">
        </figure>

        <p>The interface named <code>lo</code> is the loopback interface, used for processes that communicate via the IP protocol. The important interface in the listing is <code>enp0s3</code>, the Ethernet interface.</p>

        <p>Add static IP address using <code>nmcli</code> command:</p>

        <pre><code class="language-bash hljs">sudo nmcli connection modify enp0s3 ipv4.addresses 172.16.0.22/24</code></pre>

        <p>Add the gateway IP:</p>

        <pre><code class="language-bash hljs">sudo nmcli connection modify enp0s3 ipv4.gateway 172.16.0.1</code></pre>

        <p>Add the dns IP address:</p>

        <pre><code class="language-bash hljs">sudo nmcli connection modify enp0s3 ipv4.dns 172.16.0.1</code></pre>

        <p>Change the addressing from DHCP to static.</p>

        <pre><code class="language-bash hljs">sudo nmcli connection modify enp0s3 ipv4.method manual</code></pre>

        <p>To make changes into the effect, disable and enable the connection:</p>

        <pre><code class="language-bash hljs">sudo nmcli connection down enp0s3</code></pre>

        <pre><code class="language-bash hljs">sudo nmcli connection up enp0s3</code></pre>     

        <h4 class="post-subtitle">Setting hostname on Worker Node</h4>

        <p>Set the hostname using the following command:</p>

        <pre><code class="language-bash hljs">sudo echo "kubeworker1" > /etc/hostname</code></pre>

        <p>Add the following line to <code>/etc/hosts</code> to map the hostname to a IP address:</p>

        <pre><code class="language-bash hljs">172.16.0.21    kubemaster
172.16.0.22    kubeworker1</code></pre>

        <h4 class="post-subtitle">Disable systemd-resolved service on Worker Node</h4>

        <p>The <code>systemd-resolved</code> service can be disabled with:</p>

        <pre><code class="language-bash hljs">sudo systemctl stop systemd-resolved
sudo systemctl disable systemd-resolved
sudo systemctl mask systemd-resolved</code></pre>
        
        <p>Delete the symlink <code>/etc/resolv.conf</code>:</p>

        <pre><code class="language-bash hljs">sudo rm /etc/resolv.conf</code></pre>

        <p>Create a new <code>/etc/resolv.conf</code> file:</p>

        <pre><code class="language-bash hljs">sudo touch /etc/resolv.conf</code></pre>

        <p>Add the following line to <code>/etc/resolv.conf</code>:</p>

        <pre><code class="language-bash hljs">nameserver 172.16.0.1</code></pre>

        <h4 class="post-subtitle">Setting Machine ID on Worker Node</h4>

        <p>First, remove the <code>/etc/machine-id</code> file:</p>
        
        <pre><code class="language-bash hljs">sudo rm /etc/machine-id</code></pre>        

        <p>Then generate the new one using this command:</p>

        <pre><code class="language-bash hljs">systemd-machine-id-setup</code></pre>

        <h4 class="post-subtitle">Install containerd (container runtime) on Worker Node</h4>

        <p>First, install the required packages to run kubernetes:</p>

        <pre><code class="language-bash hljs">sudo dnf -y install ethtool
sudo dnf -y install socat
sudo dnf -y install iproute-tc
sudo dnf -y install conntrack
sudo dnf -y install openssl
sudo dnf -y install tar
sudo dnf -y install net-tools</code></pre>

        <p>Before install containerd, you need to enable <code>overlay</code> and <code>br_netfilter</code> kernel modules, and <code>net.bridge.bridge-nf-call-iptables</code>, <code>net.bridge.bridge-nf-call-ip6tables</code>, and <code>net.ipv4.ip_forward</code> kernel parameter.</p>

        <p>Create a file in the <code>/etc/modules-load.d/</code> directory which contains kernel module names to be loaded at boot time.</p>

        <pre><code class="language-bash hljs">cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF</code></pre>

        <p>Load <code>overlay</code> and <code>br_netfilter</code> kernel modules now using the <code>modprobe</code> command.</p>  

        <pre><code class="language-bash hljs">sudo modprobe overlay
sudo modprobe br_netfilter</code></pre>

        <p>Create a file in the <code>/etc/sysctl.d/</code> directory which contains <code>net.bridge.bridge-nf-call-iptables</code>, <code>net.bridge.bridge-nf-call-ip6tables</code>, and <code>net.ipv4.ip_forward</code> kernel parameters to be set at boot time.</p>

        <pre><code class="language-bash hljs">cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF</code></pre>        

        <p>Apply the kernel parameters without reboot using <code>sysctl</code> command.</p>

        <pre><code class="language-bash hljs">sudo sysctl --system</code></pre>

        <p>Verify that the <code>br_netfilter</code>, <code>overlay</code> modules are loaded by running the following commands:</p>

        <pre><code class="language-bash hljs">lsmod | grep br_netfilter
lsmod | grep overlay</code></pre>
        
        <p>Verify that the <code>net.bridge.bridge-nf-call-iptables</code>, <code>net.bridge.bridge-nf-call-ip6tables</code>, and <code>net.ipv4.ip_forward</code> system variables are set to <code>1</code> in your <code>sysctl</code> config by running the following command:</p>

        <pre><code class="language-bash hljs">sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward</code></pre>

        <p>Download the containerd from <a href="https://github.com/containerd/containerd/releases">https://github.com/containerd/containerd/releases</a>.</p>

        <pre><code class="language-bash hljs">curl -OJL https://github.com/containerd/containerd/releases/download/v1.7.2/containerd-1.7.2-linux-amd64.tar.gz</code></pre>

        <p>Extract the containerd under <code>/usr/local</code></p>

        <pre><code class="language-bash hljs">sudo tar Cxzvf /usr/local containerd-1.7.2-linux-amd64.tar.gz</code></pre>
        
        <p>Download the <code>containerd.service</code> unit file from <a href="https://raw.githubusercontent.com/containerd/containerd/main/containerd.service">https://raw.githubusercontent.com/containerd/containerd/main/containerd.service</a> into <code>/usr/lib/systemd/system/</code> directory</p>

        <pre><code class="language-bash hljs">curl -o /usr/lib/systemd/system/containerd.service https://raw.githubusercontent.com/containerd/containerd/main/containerd.service</code></pre>

        <p>Reload systemd manager configuration and enable the containerd service.</p>
        
        <pre><code class="language-bash hljs">sudo systemctl daemon-reload
sudo systemctl enable --now containerd</code></pre>
        
        <p>Download the <code>runc</code> binary from <a href="https://github.com/opencontainers/runc/releases">https://github.com/opencontainers/runc/releases</a> and install it as <code>/usr/local/sbin/runc</code>.</p>
        
        <pre><code class="language-bash hljs">curl -OJL https://github.com/opencontainers/runc/releases/download/v1.1.7/runc.amd64
sudo install -m 755 runc.amd64 /usr/local/sbin/runc</code></pre>

        <p>Download the cni-plugins from <a href="https://github.com/containernetworking/plugins/releases">https://github.com/containernetworking/plugins/releases</a> and extract it under <code>/opt/cni/bin</code></p>

        <pre><code class="language-bash hljs">curl -OJL https://github.com/containernetworking/plugins/releases/download/v1.3.0/cni-plugins-linux-amd64-v1.3.0.tgz
sudo mkdir -p /opt/cni/bin
sudo tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.3.0.tgz</code></pre>
        
        <p>Once, we have successfully installed the Containerd. We need to load the Containerd configurations.</p>

        <p>Create <code>/etc/containerd</code> directory and generate the confguration file using <code>containerd</code> command.</p>

        <pre><code class="language-bash hljs">sudo mkdir -p /etc/containerd
sudo containerd config default > /etc/containerd/config.toml</code></pre>

        <p>Open the configuration file <code>/etc/containerd/config.toml</code> and set <code>SystemdCgroup = true</code></p>

        <pre><code class="language-bash hljs">[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
  ...
  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
    SystemdCgroup = true</code></pre> 

        <p>To apply the changes, restart the <code>containerd</code> service.</p>

        <pre><code class="language-bash hljs">sudo systemctl restart containerd</code></pre>

        <h4 class="post-subtitle">Install kubectl, kubeadm, and kubelet on Worker Node</h4>

        <p>The ports required for worker node are:</p>

        <table class="table table-bordered border-primary">
            <thead>
                <th>Protocol</th>
                <th>Port Range</th>
                <th>Used By</th>
            </thead>
            <tbody>
            <tr>
                <td>TCP</td>
                <td>10250</td>
                <td>Kubelet API</td>
            </tr>
            <tr>
                <td>TCP</td>
                <td>30000-32767</td>
                <td>NodePort Services</td>
            </tr>
            </tbody>
        </table>

        <p>The ports are required to be open in the firewall.</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --add-port=10250/tcp
sudo firewall-cmd --permanent --add-port=30000-32767/tcp
sudo firewall-cmd --reload</code></pre>
        
        <p>Swap must be disabled in order for the kubelet to work properly. To permanently disable swap:</p>

        <ul>
            <li>Open the <code>/etc/fstab</code> file, search for a swap line and add a # (hashtag) sign in front of the line to comment on the entire line</li>
            <li>The swap-on-zram feature can be disabled with:<br>
                <code>sudo systemctl stop systemd-zram-setup@zram0</code><br>
                <code>sudo systemctl disable systemd-zram-setup@zram0</code><br>
                <code>sudo systemctl mask systemd-zram-setup@zram0</code>
            </li>
            <li>Run <code>swapoff -va</code> to disable memory swapping</li>
        </ul>

        <p>Download the latest release <code>kubectl</code> with the command:</p>

        <pre><code class="language-bash hljs">curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"</code></pre>

        <p>Download the <code>kubectl</code> checksum file:</p>

        <pre><code class="language-bash hljs">curl -LO "https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"</code></pre>   
        
        <p>Validate the <code>kubectl</code> binary against the checksum file:</p>

        <pre><code class="language-bash hljs">echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check</code></pre>

        <p>If valid, the output is:</p>

        <pre><code class="language-bash hljs">kubectl: OK</code></pre>   
        
        <p>Install <code>kubectl</code>:</p>

        <pre><code class="language-bash hljs">sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl</code></pre>

        <p>Check the <code>kubectl</code> version you installed:</p> 

        <pre><code class="language-bash hljs">kubectl version --short</code></pre>

        <p>Install CNI plugins (required for most pod network):</p>

        <pre><code class="language-bash hljs">CNI_PLUGINS_VERSION="v1.3.0"
ARCH="amd64"
DEST="/opt/cni/bin"
sudo mkdir -p "$DEST"
curl -L "https://github.com/containernetworking/plugins/releases/download/${CNI_PLUGINS_VERSION}/cni-plugins-linux-${ARCH}-${CNI_PLUGINS_VERSION}.tgz" | sudo tar -C "$DEST" -xz</code></pre>

        <p>Define the directory to download command files:</p>      
        
        <pre><code class="language-bash hljs">DOWNLOAD_DIR="/usr/local/bin"
sudo mkdir -p "$DOWNLOAD_DIR"</code></pre>

        <p>Install <code>crictl</code> (required for <code>kubeadm</code>/<code>kubelet</code> Container Runtime Interface (CRI)).</p>

        <pre><code class="language-bash hljs">CRICTL_VERSION="v1.27.0"
ARCH="amd64"
curl -L "https://github.com/kubernetes-sigs/cri-tools/releases/download/${CRICTL_VERSION}/crictl-${CRICTL_VERSION}-linux-${ARCH}.tar.gz" | sudo tar -C $DOWNLOAD_DIR -xz</code></pre>
        
        <p>Install <code>kubeadm</code>, <code>kubelet</code>, <code>kubectl</code> and add a <code>kubelet</code> systemd service:</p>

        <pre><code class="language-bash hljs">RELEASE="$(curl -sSL https://dl.k8s.io/release/stable.txt)"
ARCH="amd64"
cd $DOWNLOAD_DIR
sudo curl -L --remote-name-all https://dl.k8s.io/release/${RELEASE}/bin/linux/${ARCH}/{kubeadm,kubelet}
sudo chmod +x {kubeadm,kubelet}
cd -</code></pre>

        <pre><code class="language-bash hljs">RELEASE_VERSION="v0.15.1"
curl -sSL "https://raw.githubusercontent.com/kubernetes/release/${RELEASE_VERSION}/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service" | sed "s:/usr/bin:${DOWNLOAD_DIR}:g" | sudo tee /etc/systemd/system/kubelet.service
sudo mkdir -p /etc/systemd/system/kubelet.service.d
curl -sSL "https://raw.githubusercontent.com/kubernetes/release/${RELEASE_VERSION}/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf" | sed "s:/usr/bin:${DOWNLOAD_DIR}:g" | sudo tee /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code></pre>
        
        <p>Create <code>/etc/kubernetes/manifests</code> directory where kubelet should look for static pod manifests.</p>

        <pre><code class="language-bash hljs">mkdir -p /etc/kubernetes/manifests</code></pre>

        <p>Enable and start <code>kubelet</code>:</p>

        <pre><code class="language-bash hljs">sudo systemctl enable --now kubelet</code></pre>
            
        <p>If you have lost the <code>kubeadm join</code> command with the token id then you can generate a new one by running the following command on the <b>master node</b></p>

        <pre><code class="language-bash hljs">kubeadm token create --print-join-command</code></pre>

        <p>Now let's connect worker node to the master node using <code>kubeadm join</code> command:</p>
        
        <pre><code class="language-bash hljs">kubeadm join kubemaster:6443 --token 2rzeso.snqfpw46z7zaszb1 --discovery-token-ca-cert-hash sha256:bd7fffd83e3fd971df50acb6950c483e7145227ea4eca388a07967e3627fba96</code></pre>
        
        <p>If the join succeeds then it will print output similar to follow:</p>

        <pre><code class="language-bash hljs">[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.</code></pre>
        
        <p>To start using your cluster, you need to run the following as a <b>regular user</b>:</p>

        <pre><code class="language-bash hljs">sudo mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/kubelet.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config</code></pre>

        <p>Alternatively, if you are the root user, you can run:</p>

        <pre><code class="language-bash hljs">export KUBECONFIG=/etc/kubernetes/kubelet.conf</code></pre>
        
        <p>Check if kubectl is working:</p>

        <pre><code class="language-bash hljs">kubectl get nodes</code></pre>

        <p>If kubectl is working then it will show output similar to:</p>

        <pre><code class="language-bash hljs">NAME            STATUS   ROLES           AGE     VERSION
kubemaster    Ready    control-plane   3h17m   v1.27.3
kubeworker1   NotReady    &lt;none&gt;          29m     v1.27.3</code></pre>

        <p>The status will be <b>NotReady</b> as the master node is still setting up the networking.</p> 

        <p>On the master node, it will create a new calico node pod. Check the pod state using the command:</p>

        <pre><code class="language-bash hljs">kubectl get all -A</code></pre>
        
        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">NAMESPACE          NAME                                           READY   STATUS              RESTARTS        AGE
calico-apiserver   pod/calico-apiserver-746fbc54c9-m7sgk          1/1     Running             2 (6m32s ago)   22h
calico-apiserver   pod/calico-apiserver-746fbc54c9-xgsdz          1/1     Running             2 (6m32s ago)   22h
calico-system      pod/calico-kube-controllers-6cb95b98f5-bpx9g   1/1     Running             2 (6m32s ago)   22h
calico-system      pod/calico-node-lmgvk                          0/1     Init:0/2            0               63s
calico-system      pod/calico-node-phxjc                          1/1     Running             2 (6m32s ago)   22h
calico-system      pod/calico-typha-69856d85d5-frh2j              1/1     Running             2 (6m32s ago)   22h
calico-system      pod/csi-node-driver-8k2x6                      2/2     Running             4 (6m32s ago)   22h
calico-system      pod/csi-node-driver-hz2jd                      0/2     ContainerCreating   0               63s
kube-system        pod/coredns-5d78c9869d-ngzq8                   1/1     Running             2 (6m32s ago)   23h
kube-system        pod/coredns-5d78c9869d-x7pbv                   1/1     Running             2 (6m32s ago)   23h
kube-system        pod/etcd-kubemaster                            1/1     Running             2 (6m33s ago)   23h
kube-system        pod/kube-apiserver-kubemaster                  1/1     Running             2 (6m32s ago)   23h
kube-system        pod/kube-controller-manager-kubemaster         1/1     Running             2 (6m32s ago)   23h
kube-system        pod/kube-proxy-5frjt                           1/1     Running             2 (6m32s ago)   23h
kube-system        pod/kube-proxy-6wthc                           0/1     ContainerCreating   0               63s
kube-system        pod/kube-scheduler-kubemaster                  1/1     Running             2 (6m32s ago)   23h
tigera-operator    pod/tigera-operator-5f4668786-mchkg            1/1     Running             4 (6m2s ago)    22h

NAMESPACE          NAME                                      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                  AGE
calico-apiserver   service/calico-api                        ClusterIP   10.105.92.196    &lt;none&gt;        443/TCP                  22h
calico-system      service/calico-kube-controllers-metrics   ClusterIP   None             &lt;none&gt;        9094/TCP                 22h
calico-system      service/calico-typha                      ClusterIP   10.100.194.232   &lt;none&gt;        5473/TCP                 22h
default            service/kubernetes                        ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP                  23h
kube-system        service/kube-dns                          ClusterIP   10.96.0.10       &lt;none&gt;        53/UDP,53/TCP,9153/TCP   23h

NAMESPACE       NAME                             DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
calico-system   daemonset.apps/calico-node       2         2         1       2            1           kubernetes.io/os=linux   22h
calico-system   daemonset.apps/csi-node-driver   2         2         1       2            1           kubernetes.io/os=linux   22h
kube-system     daemonset.apps/kube-proxy        2         2         1       2            1           kubernetes.io/os=linux   23h

NAMESPACE          NAME                                      READY   UP-TO-DATE   AVAILABLE   AGE
calico-apiserver   deployment.apps/calico-apiserver          2/2     2            2           22h
calico-system      deployment.apps/calico-kube-controllers   1/1     1            1           22h
calico-system      deployment.apps/calico-typha              1/1     1            1           22h
kube-system        deployment.apps/coredns                   2/2     2            2           23h
tigera-operator    deployment.apps/tigera-operator           1/1     1            1           22h

NAMESPACE          NAME                                                 DESIRED   CURRENT   READY   AGE
calico-apiserver   replicaset.apps/calico-apiserver-746fbc54c9          2         2         2       22h
calico-system      replicaset.apps/calico-kube-controllers-6cb95b98f5   1         1         1       22h
calico-system      replicaset.apps/calico-typha-69856d85d5              1         1         1       22h
kube-system        replicaset.apps/coredns-5d78c9869d                   2         2         2       23h
tigera-operator    replicaset.apps/tigera-operator-5f4668786            1         1         1       22h</code></pre>

        <p>To see more info about the pod, use the command:</p>

        <pre><code class="language-bash hljs">kubectl describe pod/calico-node-grfnr -n calico-system</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">Name:                 calico-node-lmgvk
Namespace:            calico-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Service Account:      calico-node
Node:                 kubeworker1/172.16.0.22
Start Time:           Thu, 20 Jul 2023 21:11:50 +0700
Labels:               app.kubernetes.io/name=calico-node
                      controller-revision-hash=57d9485f79
                      k8s-app=calico-node
                      pod-template-generation=1
Annotations:          hash.operator.tigera.io/cni-config: 9f0a12e03c58671de56ed3876cb88f1c43cef5dc
                      hash.operator.tigera.io/system: bb4746872201725da2dea19756c475aa67d9c1e9
                      hash.operator.tigera.io/tigera-ca-private: 7675b87693668c11faf4b4f275c014fd575b0c6b
Status:               Pending
IP:                   172.16.0.22
IPs:
  IP:           172.16.0.22
Controlled By:  DaemonSet/calico-node
Init Containers:
  flexvol-driver:
    Container ID:    containerd://3bc13d8131404c3f8134efd48a7cc8a169d1613ffc5358ba300ad12bcdeabae1
    Image:           docker.io/calico/pod2daemon-flexvol:v3.26.1
    Image ID:        docker.io/calico/pod2daemon-flexvol@sha256:2aefd77a4f8289c88cfe24c0db38822de3132292d1ea4ac9192abc9583e4b54c
    Port:            &lt;none&gt;
    Host Port:       &lt;none&gt;
    SeccompProfile:  RuntimeDefault
    State:           Terminated
      Reason:        Completed
      Exit Code:     0
      Started:       Thu, 20 Jul 2023 21:14:01 +0700
      Finished:      Thu, 20 Jul 2023 21:14:01 +0700
    Ready:           True
    Restart Count:   0
    Environment:     &lt;none&gt;
    Mounts:
      /host/driver from flexvol-driver-host (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-txnjv (ro)
  install-cni:
    Container ID:    
    Image:           docker.io/calico/cni:v3.26.1
    Image ID:        
    Port:            &lt;none&gt;
    Host Port:       &lt;none&gt;
    SeccompProfile:  RuntimeDefault
    Command:
      /opt/cni/bin/install
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Environment:
      CNI_CONF_NAME:            10-calico.conflist
      SLEEP:                    false
      CNI_NET_DIR:              /etc/cni/net.d
      CNI_NETWORK_CONFIG:       &lt;set to the key 'config' of config map 'cni-config'&gt;  Optional: false
      KUBERNETES_SERVICE_HOST:  10.96.0.1
      KUBERNETES_SERVICE_PORT:  443
    Mounts:
      /host/etc/cni/net.d from cni-net-dir (rw)
      /host/opt/cni/bin from cni-bin-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-txnjv (ro)
Containers:
  calico-node:
    Container ID:    
    Image:           docker.io/calico/node:v3.26.1
    Image ID:        
    Port:            &lt;none&gt;
    Host Port:       &lt;none&gt;
    SeccompProfile:  RuntimeDefault
    State:           Waiting
      Reason:        PodInitializing
    Ready:           False
    Restart Count:   0
    Liveness:        http-get http://localhost:9099/liveness delay=0s timeout=10s period=10s #success=1 #failure=3
    Readiness:       exec [/bin/calico-node -bird-ready -felix-ready] delay=0s timeout=5s period=10s #success=1 #failure=3
    Environment:
      DATASTORE_TYPE:                      kubernetes
      WAIT_FOR_DATASTORE:                  true
      CLUSTER_TYPE:                        k8s,operator,bgp
      CALICO_DISABLE_FILE_LOGGING:         false
      FELIX_DEFAULTENDPOINTTOHOSTACTION:   ACCEPT
      FELIX_HEALTHENABLED:                 true
      FELIX_HEALTHPORT:                    9099
      NODENAME:                             (v1:spec.nodeName)
      NAMESPACE:                           calico-system (v1:metadata.namespace)
      FELIX_TYPHAK8SNAMESPACE:             calico-system
      FELIX_TYPHAK8SSERVICENAME:           calico-typha
      FELIX_TYPHACAFILE:                   /etc/pki/tls/certs/tigera-ca-bundle.crt
      FELIX_TYPHACERTFILE:                 /node-certs/tls.crt
      FELIX_TYPHAKEYFILE:                  /node-certs/tls.key
      FIPS_MODE_ENABLED:                   false
      FELIX_TYPHACN:                       typha-server
      CALICO_MANAGE_CNI:                   true
      CALICO_IPV4POOL_CIDR:                192.168.0.0/16
      CALICO_IPV4POOL_VXLAN:               CrossSubnet
      CALICO_IPV4POOL_BLOCK_SIZE:          26
      CALICO_IPV4POOL_NODE_SELECTOR:       all()
      CALICO_IPV4POOL_DISABLE_BGP_EXPORT:  false
      CALICO_NETWORKING_BACKEND:           bird
      IP:                                  autodetect
      IP_AUTODETECTION_METHOD:             first-found
      IP6:                                 none
      FELIX_IPV6SUPPORT:                   false
      KUBERNETES_SERVICE_HOST:             10.96.0.1
      KUBERNETES_SERVICE_PORT:             443
    Mounts:
      /etc/pki/tls/cert.pem from tigera-ca-bundle (ro,path="ca-bundle.crt")
      /etc/pki/tls/certs from tigera-ca-bundle (ro)
      /host/etc/cni/net.d from cni-net-dir (rw)
      /lib/modules from lib-modules (ro)
      /node-certs from node-certs (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/calico from var-lib-calico (rw)
      /var/log/calico/cni from cni-log-dir (rw)
      /var/run/calico from var-run-calico (rw)
      /var/run/nodeagent from policysync (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-txnjv (ro)
Conditions:
  Type              Status
  Initialized       False 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  policysync:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/nodeagent
    HostPathType:  DirectoryOrCreate
  tigera-ca-bundle:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      tigera-ca-bundle
    Optional:  false
  node-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  node-certs
    Optional:    false
  var-run-calico:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/calico
    HostPathType:  
  var-lib-calico:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/calico
    HostPathType:  
  cni-bin-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /opt/cni/bin
    HostPathType:  
  cni-net-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  cni-log-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /var/log/calico/cni
    HostPathType:  
  flexvol-driver-host:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds
    HostPathType:  DirectoryOrCreate
  kube-api-access-txnjv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 :NoSchedule op=Exists
                             :NoExecute op=Exists
                             CriticalAddonsOnly op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  8m5s   default-scheduler  Successfully assigned calico-system/calico-node-lmgvk to kubeworker1
  Normal  Pulling    8m     kubelet            Pulling image "docker.io/calico/pod2daemon-flexvol:v3.26.1"
  Normal  Pulled     5m54s  kubelet            Successfully pulled image "docker.io/calico/pod2daemon-flexvol:v3.26.1" in 47.864589232s (2m5.390789865s including waiting)
  Normal  Created    5m54s  kubelet            Created container flexvol-driver
  Normal  Started    5m54s  kubelet            Started container flexvol-driver
  Normal  Pulling    5m53s  kubelet            Pulling image "docker.io/calico/cni:v3.26.1"</code></pre>

        <p><b>Wait for the pod to be in running state</b>.</p>

        <p>Then check the status of a Calico instance on the master node:</p>

        <pre><code class="language-bash hljs">calicoctl node status</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">Calico process is running.

IPv4 BGP status
+--------------+-------------------+-------+----------+-------------+
| PEER ADDRESS |     PEER TYPE     | STATE |  SINCE   |    INFO     |
+--------------+-------------------+-------+----------+-------------+
| 172.16.0.22  | node-to-node mesh | up    | 14:43:05 | Established |
+--------------+-------------------+-------+----------+-------------+

IPv6 BGP status
No IPv6 peers found.</code></pre>

        <p>Check the status of a Calico instance on the worker node:</p>

        <pre><code class="language-bash hljs">calicoctl node status</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">Calico process is running.

IPv4 BGP status
+--------------+-------------------+-------+----------+-------------+
| PEER ADDRESS |     PEER TYPE     | STATE |  SINCE   |    INFO     |
+--------------+-------------------+-------+----------+-------------+
| 172.16.0.21  | node-to-node mesh | up    | 14:43:05 | Established |
+--------------+-------------------+-------+----------+-------------+

IPv6 BGP status
No IPv6 peers found.</code></pre>

        <h4 class="post-subtitle">Install MetalLB (Load Balancer) on Master Node</h4>

        <p>Download the MetalLB manifest for the Kubernetes clusters.</p>

        <pre><code class="language-bash hljs">curl -OJL https://raw.githubusercontent.com/metallb/metallb/v0.13.10/config/manifests/metallb-native.yaml</code></pre>

        <p>Change the value of <code>failurePolicy</code> from <code>Fail</code> to <code>Ignore</code> in the <code>metallb-native.yaml</code> file.</p>

        <pre><code class="language-bash hljs">sed -i 's/failurePolicy: Fail/failurePolicy: Ignore/' metallb-native.yaml</code></pre>

        <p>To install MetalLB, apply the manifest:</p>

        <pre><code class="language-bash hljs">kubectl create -f metallb-native.yaml</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">namespace/metallb-system created
customresourcedefinition.apiextensions.k8s.io/addresspools.metallb.io created
customresourcedefinition.apiextensions.k8s.io/bfdprofiles.metallb.io created
customresourcedefinition.apiextensions.k8s.io/bgpadvertisements.metallb.io created
customresourcedefinition.apiextensions.k8s.io/bgppeers.metallb.io created
customresourcedefinition.apiextensions.k8s.io/communities.metallb.io created
customresourcedefinition.apiextensions.k8s.io/ipaddresspools.metallb.io created
customresourcedefinition.apiextensions.k8s.io/l2advertisements.metallb.io created
serviceaccount/controller created
serviceaccount/speaker created
role.rbac.authorization.k8s.io/controller created
role.rbac.authorization.k8s.io/pod-lister created
clusterrole.rbac.authorization.k8s.io/metallb-system:controller created
clusterrole.rbac.authorization.k8s.io/metallb-system:speaker created
rolebinding.rbac.authorization.k8s.io/controller created
rolebinding.rbac.authorization.k8s.io/pod-lister created
clusterrolebinding.rbac.authorization.k8s.io/metallb-system:controller created
clusterrolebinding.rbac.authorization.k8s.io/metallb-system:speaker created
configmap/metallb-excludel2 created
secret/webhook-server-cert created
service/webhook-service created
deployment.apps/controller created
daemonset.apps/speaker created
validatingwebhookconfiguration.admissionregistration.k8s.io/metallb-webhook-configuration created</code></pre>
        
        <p>This will deploy MetalLB to your cluster, under the <code>metallb-system</code> namespace. The components in the manifest are:</p>

        <ul>
            <li>The <code>metallb-system/controller</code> deployment. This is the cluster-wide controller that handles IP address assignments</li>
            <li>The <code>metallb-system/speaker</code> daemonset. This is the component that speaks the protocol(s) of your choice to make the services reachable</li>
            <li>Service accounts for the controller and speaker, along with the RBAC permissions that the components need to function</li>
        </ul>

        <p>The installation manifest does not include a configuration file. MetalLB's components will still start, but will remain idle until you start deploying resources.</p>

        <p>To list all resources in <code>metallb-system</code> namespace, use the command:</p>

        <pre><code class="language-bash hljs">kubectl get all -n metallb-system</code></pre>
        
        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">NAME                              READY   STATUS              RESTARTS   AGE
pod/controller-595f88d88f-gk98r   0/1     ContainerCreating   0          77s
pod/speaker-2rg52                 0/1     ContainerCreating   0          77s
pod/speaker-r8p6g                 0/1     ContainerCreating   0          77s

NAME                      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
service/webhook-service   ClusterIP   10.106.144.178   &lt;none&gt;        443/TCP   77s

NAME                     DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/speaker   2         2         0       2            0           kubernetes.io/os=linux   77s

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/controller   0/1     1            0           77s

NAME                                    DESIRED   CURRENT   READY   AGE
replicaset.apps/controller-595f88d88f   1         1         0       77s</code></pre>
        
        <p>You can see that all the resources are still not in running state. <b>Wait for everything to be in running state</b>.</p>        
        
        <p>If the error message is <code>ErrImagePull</code> or <code>ImagePullBackOff</code> then delete the pod using the command:</p>

        <pre><code class="language-bash hljs">kubectl delete pod/controller-595f88d88f-gk98r -n metallb-system</code></pre>

        <p><code>Deployment/StatefulSet/ReplicaSet/DaemonSet</code> will reschedule a new one in its place automatically.</p>

        <!--<p>If the error message is <code>Error: secret "memberlist" not found</code> then delete the pod and create a secret object named memberlist using the command:</p>

        <pre><code class="language-bash hljs">kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey="$(openssl rand -base64 128)"</code></pre>-->

        <p>When everything is in running state, the response should look similar to this:</p>

        <pre><code class="language-bash hljs">NAME                              READY   STATUS    RESTARTS   AGE
pod/controller-595f88d88f-gk98r   1/1     Running   0          12m
pod/speaker-2rg52                 1/1     Running   0          12m
pod/speaker-r8p6g                 1/1     Running   0          12m

NAME                      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
service/webhook-service   ClusterIP   10.106.144.178   &lt;none&gt;        443/TCP   12m

NAME                     DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/speaker   2         2         2       2            2           kubernetes.io/os=linux   12m

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/controller   1/1     1            1           12m

NAME                                    DESIRED   CURRENT   READY   AGE
replicaset.apps/controller-595f88d88f   1         1         1       12m</code></pre>


        <p>Create a <code>metallb-addresspool.yaml</code> configuration file to enable the layer 2 mode as below:</p>

        <pre><code class="language-bash hljs">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ipaddresspool
  namespace: metallb-system
spec:
  addresses:
  - 192.170.0.0/24
  avoidBuggyIPs: true
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: l2advertisement
  namespace: metallb-system
spec:
  ipAddressPools:
  - ipaddresspool</code></pre>        

        <p>Apply the configuration file:</p>

        <pre><code class="language-bash hljs">kubectl create -f metallb-addresspool.yaml</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">ipaddresspool.metallb.io/ipaddresspool created
l2advertisement.metallb.io/l2advertisement created</code></pre>
        
        <p>The ports required by MetalLB In Layer 2 Mode for master node and worker node are:</p>

        <table class="table table-bordered border-primary">
            <thead>
                <th>Protocol</th>
                <th>Port Range</th>
                <th>Used By</th>
            </thead>
            <tbody>
            <tr>
                <td>TCP</td>
                <td>7946</td>
                <td>metallb speaker</td>
            </tr>
            <tr>
                <td>UDP</td>
                <td>7946</td>
                <td>metallb speaker</td>
            </tr>
            </tbody>
        </table>

        <p>The ports are required to be open in the firewall.</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --add-port=7946/tcp
sudo firewall-cmd --permanent --add-port=7946/udp
sudo firewall-cmd --reload</code></pre>

        <p>Add label to kubeworker1 node, so we can assign pods to that node:</p>
        
        <pre><code class="language-bash hljs">kubectl label nodes kubeworker1 nodelabel=kubeworker1</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">node/kubeworker1 labeled</code></pre>        

        <p>Let's verify that the new label is added by running:</p>

        <pre><code class="language-bash hljs">kubectl get nodes --show-labels</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">NAME          STATUS   ROLES           AGE    VERSION   LABELS
kubemaster    Ready    control-plane   2d1h   v1.27.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=kubemaster,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=
kubeworker1   Ready    &lt;none&gt;          26h    v1.27.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=kubeworker1,kubernetes.io/os=linux,nodelabel=kubeworker1</code></pre>                        
        
        <h4 class="post-subtitle">Install Nginx Ingress Controller on Master Node</h4>

        <p>Nginx Ingress controller is an Ingress controller for Kubernetes using NGINX as a reverse proxy and load balancer.</p>

        <p>Download NGINX Ingress Controller manifest:</p>

        <pre><code class="language-bash hljs">curl -o ingress-nginx.yaml https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/baremetal/deploy.yaml</code></pre>

        <p>Change the value of <code>failurePolicy</code> from <code>Fail</code> to <code>Ignore</code> in the <code>ingress-nginx.yaml</code> file.</p>

        <pre><code class="language-bash hljs">sed -i 's/failurePolicy: Fail/failurePolicy: Ignore/' ingress-nginx.yaml</code></pre>

        <pre><code class="language-bash hljs">sed -i 's/--patch-failure-policy=Fail/--patch-failure-policy=Ignore/' ingress-nginx.yaml</code></pre>

        <p>Apply Nginx ingress controller manifest deployment file:</p>

        <pre><code class="language-bash hljs">kubectl create -f ingress-nginx.yaml</code></pre>        

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">namespace/ingress-nginx created
serviceaccount/ingress-nginx created
serviceaccount/ingress-nginx-admission created
role.rbac.authorization.k8s.io/ingress-nginx created
role.rbac.authorization.k8s.io/ingress-nginx-admission created
clusterrole.rbac.authorization.k8s.io/ingress-nginx created
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission created
rolebinding.rbac.authorization.k8s.io/ingress-nginx created
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx created
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created
configmap/ingress-nginx-controller created
service/ingress-nginx-controller created
service/ingress-nginx-controller-admission created
deployment.apps/ingress-nginx-controller created
job.batch/ingress-nginx-admission-create created
job.batch/ingress-nginx-admission-patch created
ingressclass.networking.k8s.io/nginx created
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission created</code></pre>
        
        <p>To list all resources in <code>ingress-nginx</code> namespace, use the command:</p>

        <pre><code class="language-bash hljs">kubectl get all -n ingress-nginx</code></pre>
        
        <p>The response should look similar to this:</p>
        
        <pre><code class="language-bash hljs">NAME                                            READY   STATUS              RESTARTS   AGE
pod/ingress-nginx-admission-create-7fkfc        0/1     ContainerCreating   0          15s
pod/ingress-nginx-admission-patch-vnbwf         0/1     ContainerCreating   0          15s
pod/ingress-nginx-controller-5c778bffff-4bb6x   0/1     ContainerCreating   0          15s

NAME                                         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
service/ingress-nginx-controller             NodePort    10.107.65.165    &lt;none&gt;        80:30609/TCP,443:30996/TCP   15s
service/ingress-nginx-controller-admission   ClusterIP   10.104.236.122   &lt;none&gt;        443/TCP                      15s

NAME                                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/ingress-nginx-controller   0/1     1            0           15s

NAME                                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/ingress-nginx-controller-5c778bffff   1         1         0       15s

NAME                                       COMPLETIONS   DURATION   AGE
job.batch/ingress-nginx-admission-create   0/1           15s        15s
job.batch/ingress-nginx-admission-patch    0/1           15s        15s</code></pre>

        <p>You can see that all the resources are still not in running state. <b>Wait for everything to be in running state</b>.</p>

        <p>When all the resources are ready. The response should look similar to this:</p>        

        <pre><code class="language-bash hljs">NAME                                            READY   STATUS      RESTARTS   AGE
pod/ingress-nginx-admission-create-7fkfc        0/1     Completed   0          116s
pod/ingress-nginx-admission-patch-vnbwf         0/1     Completed   0          116s
pod/ingress-nginx-controller-5c778bffff-4bb6x   1/1     Running     0          116s

NAME                                         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
service/ingress-nginx-controller             NodePort    10.107.65.165    &lt;none&gt;        80:30609/TCP,443:30996/TCP   116s
service/ingress-nginx-controller-admission   ClusterIP   10.104.236.122   &lt;none&gt;        443/TCP                      116s

NAME                                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/ingress-nginx-controller   1/1     1            1           116s

NAME                                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/ingress-nginx-controller-5c778bffff   1         1         1       116s

NAME                                       COMPLETIONS   DURATION   AGE
job.batch/ingress-nginx-admission-create   1/1           20s        116s
job.batch/ingress-nginx-admission-patch    1/1           20s        116s</code></pre>

        <p>Edit <code>ingress-nginx-controller</code> service.</p>

        <pre><code class="language-bash hljs">kubectl edit service/ingress-nginx-controller -n ingress-nginx</code></pre>

        <p>Change the value of <code>type</code> from <code>NodePort</code> to <code>LoadBalancer</code>.</p>

        <pre><code class="language-bash hljs">apiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2023-07-25T05:22:21Z"
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.8.1
  name: ingress-nginx-controller
  namespace: ingress-nginx
  resourceVersion: "90209"
  uid: 67865f69-35af-4e2d-aa61-897556d7ae1b
spec:
  clusterIP: 10.107.65.165
  clusterIPs:
  - 10.107.65.165
  externalTrafficPolicy: Cluster
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - appProtocol: http
    name: http
    nodePort: 30609
    port: 80
    protocol: TCP
    targetPort: http
  - appProtocol: https
    name: https
    nodePort: 30996
    port: 443
    protocol: TCP
    targetPort: https
  selector:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
  sessionAffinity: None
  type: LoadBalancer
status:
  loadBalancer: {}</code></pre>        
        
        <p>Confirm successful edited of the service.</p>

        <pre><code class="language-bash hljs">service/ingress-nginx-controller edited</code></pre>

        <p>Nginx Controller Service will be assigned an IP address automatically from Address Pool as configured in MetalLB and assigned ports which are required to be open.</p>

        <pre><code class="language-bash hljs">kubectl get service ingress-nginx-controller --namespace=ingress-nginx</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">NAME                       TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx-controller   LoadBalancer   10.107.65.165   192.170.0.1   80:30609/TCP,443:30996/TCP   4m6s</code></pre>

        <p>The ports required for ingress nginx controller are:</p>

        <table class="table table-bordered border-primary">
            <thead>
                <th>Protocol</th>
                <th>Port Range</th>
                <th>Used By</th>
            </thead>
            <tbody>
            <tr>
                <td>TCP</td>
                <td>80</td>
                <td>cluster ip, external ip</td>
            </tr>
            <tr>
                <td>TCP</td>
                <td>443</td>
                <td>cluster ip, external ip</td>
            </tr>
            <tr>
                <td>TCP</td>
                <td>30609</td>
                <td>node ip</td>
            </tr>
            <tr>
                <td>TCP</td>
                <td>30996</td>
                <td>node ip</td>
            </tr>
            </tbody>
        </table>

        <p>The ports are required to be open in the firewall on master node.</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --add-port=80/tcp
sudo firewall-cmd --permanent --add-port=443/tcp
sudo firewall-cmd --reload</code></pre>
        
        <p>The ports are required to be open in the firewall on worker node.</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --add-port=80/tcp
sudo firewall-cmd --permanent --add-port=443/tcp
sudo firewall-cmd --permanent --add-port=30609/tcp
sudo firewall-cmd --permanent --add-port=30996/tcp
sudo firewall-cmd --reload</code></pre>
        
        <p>If you have multiple worker nodes, to know which worker node the <code>ingress-nginx-controller</code> pod is running on, run the command on the master node:</p>

        <pre><code class="language-bash hljs">kubectl describe pod/ingress-nginx-controller-5c778bffff-4bb6x -n ingress-nginx | grep Node:</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">Node:             kubeworker1/172.16.0.22</code></pre>

        <p>On worker node where <code>ingress-nginx-controller</code> pod is running on, use <code>curl</code> to test the cluster IP address of <code>ingress-nginx-controller</code> service:</p>

        <pre><code class="language-bash hljs">curl http://10.107.65.165:80</code></pre>

        <pre><code class="language-bash hljs">curl -k https://10.107.65.165:443</code></pre>

        <p>Since there isn't any ingress resource configured, the response will return "404 Not Found" similar to follow:</p>

        <pre><code class="language-bash hljs">&lt;html&gt;
&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;
&lt;body&gt;
&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;
&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;
&lt;/body&gt;
&lt;/html&gt;</code></pre>

        <p>On worker node where <code>ingress-nginx-controller</code> pod is running on, use <code>curl</code> to test the external IP address of <code>ingress-nginx-controller</code> service:</p>

        <pre><code class="language-bash hljs">curl http://192.170.0.1:80</code></pre>

        <pre><code class="language-bash hljs">curl -k https://192.170.0.1:443</code></pre>

        <p>Since there isn't any ingress resource configured, the response will return "404 Not Found" similar to follow:</p>

        <pre><code class="language-bash hljs">&lt;html&gt;
&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;
&lt;body&gt;
&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;
&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;
&lt;/body&gt;
&lt;/html&gt;</code></pre>
        
        <p>On worker node where <code>ingress-nginx-controller</code> pod is running on, you also can use the worker node IP address:</p>

        <pre><code class="language-bash hljs">curl http://172.16.0.22:30609</code></pre>

        <pre><code class="language-bash hljs">curl -k https://172.16.0.22:30996</code></pre>

        <p>Since there isn't any ingress resource configured, the response will return "404 Not Found" similar to follow:</p>

        <pre><code class="language-bash hljs">&lt;html&gt;
&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;
&lt;body&gt;
&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;
&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;
&lt;/body&gt;
&lt;/html&gt;</code></pre>

        <p>On master node and other worker nodes, use <code>curl</code> to test tcp connection on cluster IP and on external IP address of <code>ingress-nginx-controller</code> service and on the worker node IP address where <code>ingress-nginx-controller</code> pod is running on:</p>

        <pre><code class="language-bash hljs">curl http://10.107.65.165:80</code></pre>

        <pre><code class="language-bash hljs">curl -k https://10.107.65.165:443</code></pre>

        <pre><code class="language-bash hljs">curl http://192.170.0.1:80</code></pre>

        <pre><code class="language-bash hljs">curl -k https://192.170.0.1:443</code></pre>

        <pre><code class="language-bash hljs">curl http://172.16.0.22:30609</code></pre>

        <pre><code class="language-bash hljs">curl -k https://172.16.0.22:30996</code></pre>

        <p>The curl response when try to test tcp connection on cluster IP external IP address of <code>ingress-nginx-controller</code> service will fail since the firewall is blocking the request:</p>

        <pre><code class="language-bash hljs">curl: (7) Failed to connect to 10.107.65.165 port 80 after 1 ms: Couldn't connect to server</code></pre>

        <pre><code class="language-bash hljs">curl: (7) Failed to connect to 10.107.65.165 port 443 after 1 ms: Couldn't connect to server</code></pre>

        <pre><code class="language-bash hljs">curl: (7) Failed to connect to 192.170.0.1 port 80 after 1 ms: Couldn't connect to server</code></pre>

        <pre><code class="language-bash hljs">curl: (7) Failed to connect to 192.170.0.1 port 443 after 1 ms: Couldn't connect to server</code></pre>

        <p>But the curl response when try to test tcp connection on the worker node IP address where <code>ingress-nginx-controller</code> pod is running on will return "404 Not Found" since there isn't any ingress resource configured:</p>

        <pre><code class="language-bash hljs">&lt;html&gt;
&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;
&lt;body&gt;
&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;
&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;
&lt;/body&gt;
&lt;/html&gt;</code></pre>

        <p>Let's inspect why the firewall blocked the request.</p>

        <p>Enable <code>firewalld</code> logging for denied packets on <b>the worker node</b> where <code>ingress-nginx-controller</code> pod is running on. Edit the <code>/etc/firewalld/firewalld.conf</code>. Set the value of <code>LogDenied</code> option from <code>off</code> to <code>all</code>.</p>

        <p>Restart the <code>firewalld</code> service:</p>

        <pre><code class="language-bash hljs">sudo systemctl restart firewalld</code></pre>

        <p><b>On master node and other worker nodes</b>, run the <code>curl</code> command again:</p>

        <pre><code class="language-bash hljs">curl http://10.107.65.165:80</code></pre>

        <pre><code class="language-bash hljs">curl -k https://10.107.65.165:443</code></pre>

        <pre><code class="language-bash hljs">curl http://192.170.0.1:80</code></pre>

        <pre><code class="language-bash hljs">curl -k https://192.170.0.1:443</code></pre>

        <p>On worker node where <code>ingress-nginx-controller</code> pod is running on, to view the denied packets, use <code>dmesg</code> command:</p>
        
        <pre><code class="language-bash hljs">dmesg | grep -i REJECT</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">[ 8484.826734] filter_FWD_public_REJECT: IN=enp0s3 OUT=cali7368eccef27 MAC=08:00:27:10:a0:5f:08:00:27:fe:60:d3:08:00 SRC=172.16.0.21 DST=192.168.90.8 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=47217 DF PROTO=TCP SPT=59916 DPT=80 WINDOW=64240 RES=0x00 SYN URGP=0 MARK=0x10000 
[ 8484.842467] filter_FWD_public_REJECT: IN=enp0s3 OUT=cali7368eccef27 MAC=08:00:27:10:a0:5f:08:00:27:fe:60:d3:08:00 SRC=172.16.0.21 DST=192.168.90.8 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=38798 DF PROTO=TCP SPT=5163 DPT=443 WINDOW=64240 RES=0x00 SYN URGP=0 MARK=0x10000 
[ 8484.863771] filter_FWD_public_REJECT: IN=enp0s3 OUT=cali7368eccef27 MAC=08:00:27:10:a0:5f:08:00:27:fe:60:d3:08:00 SRC=172.16.0.21 DST=192.168.90.8 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=15091 DF PROTO=TCP SPT=58803 DPT=80 WINDOW=64240 RES=0x00 SYN URGP=0 MARK=0x10000 
[ 8484.881376] filter_FWD_public_REJECT: IN=enp0s3 OUT=cali7368eccef27 MAC=08:00:27:10:a0:5f:08:00:27:fe:60:d3:08:00 SRC=172.16.0.21 DST=192.168.90.8 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=28939 DF PROTO=TCP SPT=19211 DPT=443 WINDOW=64240 RES=0x00 SYN URGP=0 MARK=0x10000</code></pre>        
        <p>Create an new zone using calico network interface name (ie. <code>cali7368eccef27</code>). This is the zone for <code>ingress-nginx-controller</code> calico pod network.</p>
        
        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --new-zone cali7368eccef27</code></pre>

        <p>Then assign <code>ingress-nginx-controller</code> calico pod network interface <code>cali7368eccef27</code> to this zone.</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --change-interface=cali7368eccef27 --zone=cali7368eccef27</code></pre>

        <p>Next, create a policy:</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --new-policy cali7368eccef27pol</code></pre>

        <p>Set the policies to traffic forwarded from lan interface <code>enp0s3</code> to calico network interface <code>cali7368eccef27</code>:</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --policy cali7368eccef27pol --add-ingress-zone public
sudo firewall-cmd --permanent --policy cali7368eccef27pol --add-egress-zone cali7368eccef27</code></pre>        
        <p>To finish up the policy settings, set it to accept new connections by default:</p>
        
        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --policy cali7368eccef27pol --set-target ACCEPT</code></pre>

        <p>Then load the new zone and policy into the active runtime state:</p>
        
        <pre><code class="language-bash hljs">sudo firewall-cmd --reload</code></pre>

        <p><b>On master node and other worker nodes</b>, test to run the <code>curl</code> command again:</p>

        <pre><code class="language-bash hljs">curl http://10.107.65.165:80</code></pre>

        <pre><code class="language-bash hljs">curl -k https://10.107.65.165:443</code></pre>

        <pre><code class="language-bash hljs">curl http://192.170.0.1:80</code></pre>

        <pre><code class="language-bash hljs">curl -k https://192.170.0.1:443</code></pre>

        <p>Since there isn't any ingress resource configured, the response will return "404 Not Found" similar to follow:</p>

        <pre><code class="language-bash hljs">&lt;html&gt;
&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;
&lt;body&gt;
&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;
&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;
&lt;/body&gt;
&lt;/html&gt;</code></pre>

        <h4 class="post-subtitle">Deploy a web-application using Nginx server to Worker Node from Master Node</h4>

        <p>First generate a self signed rsa key and certificate that the server can use for TLS:</p>

        <pre><code class="language-bash hljs">mkdir ~/nginx
openssl req -x509 -nodes -days 365 -newkey rsa:4096 -keyout ~/nginx/nginx.key -out ~/nginx/nginx.crt</code></pre>
        
        <p>Fill out the certificate form:</p>

        <pre><code class="language-bash hljs">...+........+.+...+..+......+.........+...+.+......+...+..+...+.+.....+..........+......+...+......+......+.....+.+..+...+...+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*.............+.+.....+.+...........+....+...+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*......+.............+......+....................+.......+..................+...............+...........+.+........+......+.............+.....+.+..............+...+.......+...............+...+..+.+........................+............+.....+.............+...+..+.+......+.........+.................+....+.....+.+.............................+.+..................+..+.......+..+.............+...........+.......+...+...........+...+.......+.....+..................+...+....+...........+.........+..................+.+.................+...+....+............+.........+.....+....+...............+......+...............+..+............+.+.....+.+...+............+...+..+.+..+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
.....+...+......+...+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*.....+...+...+.......+...+...+.....+....+............+.....+...+...+...............+...+....+...........+.+......+..+.+......+...+..+...+...+....+..............+......+.+..+.+.........+......+.....+............+.......+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*........+..+.............+..+.......+...........+.......+.....+..............................+...+.......+......+......+...+...+...+..+....+...+...+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
-----
You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter '.', the field will be left blank.
-----
Country Name (2 letter code) [AU]:ID
State or Province Name (full name) [Some-State]:Jakarta
Locality Name (eg, city) []:Jakarta
Organization Name (eg, company) [Internet Widgits Pty Ltd]:hemimorphite
Organizational Unit Name (eg, section) []:     
Common Name (e.g. server FQDN or YOUR name) []:kubeworker1
Email Address []:yangsamuel91@gmail.com</code></pre>
        
        <p>Encode the key and certificate data in base64</p>

        <pre><code class="language-bash hljs">cat ~/nginx/nginx.crt | base64</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">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</code></pre>

        <pre><code class="language-bash hljs">cat ~/nginx/nginx.key | base64</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">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</code></pre>

        <p>Then create your NGINX server configuration file <code>default.conf</code>:</p>

        <pre><code class="language-bash hljs">server {
    listen 80 default_server;
    listen [::]:80 default_server ipv6only=on;

    listen 443 ssl;

    root /usr/share/nginx/html;
    index index.html;

    server_name localhost;
    ssl_certificate /etc/nginx/ssl/tls.crt;
    ssl_certificate_key /etc/nginx/ssl/tls.key;

    location / {
            try_files $uri $uri/ =404;
    }
}</code></pre>
        

        <p>Create a kubernetes yaml tls secret file for creating a secret named <code>nginx-secret</code></p>

        <pre><code class="language-bash hljs">apiVersion: v1
data: 
  tls.crt: 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
  tls.key: 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
kind: Secret
metadata: 
  name: nginx-secret
  namespace: default
type: kubernetes.io/tls</code></pre>
        
        <p>Apply the tls secret manifest file:</p>

        <pre><code class="language-bash hljs">kubectl create -f nginx-secret.yaml</code></pre>        

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">secret/nginx-secret created</code></pre>

        <p>Create a <code>ConfigMap</code> that stores Nginx configuration:</p>

        <pre><code class="language-bash hljs">kubectl create configmap nginx-configmap --from-file=default.conf</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">configmap/nginx-configmap created</code></pre>

        <p>Create a kubernetes yaml deployment file for deploying a pod named <code>nginx-server</code> that running Nginx image:</p>
        
        <pre><code class="language-bash hljs">apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-server
  namespace: default
  labels:
    app: web
spec:
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      volumes:
      - name: secret-volume
        secret:
          secretName: nginx-secret
      - name: configmap-volume
        configMap:
          name: nginx-configmap
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
        - containerPort: 443
        volumeMounts:
        - mountPath: /etc/nginx/ssl
          name: secret-volume
        - mountPath: /etc/nginx/conf.d
          name: configmap-volume
      nodeSelector:
        nodelabel: kubeworker1</code></pre>        

        <p>Apply the nginx server manifest deployment file:</p>

        <pre><code class="language-bash hljs">kubectl create -f nginx-server.yaml</code></pre>        

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">deployment.apps/nginx-server created</code></pre>

        <p>Create a kubernetes yaml load balancer service file for exposing the pod with the label <code>app=web</code> to a service named <code>nginx-server-service</code> on a TCP port of 80:</code></p>        
        <pre><code class="language-bash hljs">apiVersion: v1
kind: Service
metadata:
  name: nginx-server-service
  namespace: default
spec:
  selector:
    app: web
  ports:
    - name: web-http
      protocol: TCP
      appProtocol: http
      targetPort: http
      port: 80
      targetPort: 80
    - name: web-https
      protocol: TCP
      appProtocol: https
      targetPort: https
      port: 443
      targetPort: 443
  type: LoadBalancer</code></pre>

        <p>Apply the nginx server service manifest file:</p>

        <pre><code class="language-bash hljs">kubectl create -f nginx-server-service.yaml</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">service/nginx-server-service created</code></pre>

        <p>Nginx Controller Service will be assigned an IP address automatically from Address Pool as configured in MetalLB and assigned ports which are required to be open.</p>

        <pre><code class="language-bash hljs">kubectl get service nginx-server-service --namespace=default</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">NAME                   TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
nginx-server-service   LoadBalancer   10.107.131.88   192.170.0.2   80:31653/TCP,443:30552/TCP   5m23s</code></pre>

        <p>The ports required by nginx server service are:</p>

        <table class="table table-bordered border-primary">
            <thead>
                <th>Protocol</th>
                <th>Port Range</th>
                <th>Used By</th>
            </thead>
            <tbody>
            <tr>
                <td>TCP</td>
                <td>80</td>
                <td>cluster ip, external ip</td>
            </tr>
            <tr>
                <td>TCP</td>
                <td>443</td>
                <td>cluster ip, external ip</td>
            </tr>
            <tr>
                <td>TCP</td>
                <td>31653</td>
                <td>node ip</td>
            </tr>
            <tr>
                <td>TCP</td>
                <td>30552</td>
                <td>node ip</td>
            </tr>
            </tbody>
        </table>

        <p>The ports are required to be open in the firewall on master node.</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --add-port=80/tcp
sudo firewall-cmd --permanent --add-port=443/tcp
sudo firewall-cmd --reload</code></pre>
        
        <p>The ports are required to be open in the firewall on worker node.</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --add-port=80/tcp
sudo firewall-cmd --permanent --add-port=443/tcp
sudo firewall-cmd --permanent --add-port=31025/tcp
sudo firewall-cmd --permanent --add-port=30625/tcp
sudo firewall-cmd --reload</code></pre>
    
        <p>If you have multiple worker nodes, you need to know which worker node the <code>nginx-server</code> pod is running on. To know which worker node the pod is running on, run the command:</p>

        <pre><code class="language-bash hljs">kubectl describe pod/ingress-nginx-controller-5c778bffff-4bb6x -n ingress-nginx | grep Node:</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">Node:             kubeworker1/172.16.0.22</code></pre>

        <p>On worker node where the <code>nginx-server</code> pod is running on, try to access your application using the cluster IP and the external IP address of <code>nginx-server-service</code> and the worker node IP which the <code>nginx-server</code> pod is running on:</p>

        <pre><code class="language-bash hljs">curl http://10.107.131.88:80</code></pre>

        <pre><code class="language-bash hljs">curl -k https://10.107.131.88:443</code></pre>

        <pre><code class="language-bash hljs">curl http://192.170.0.2:80</code></pre>

        <pre><code class="language-bash hljs">curl -k https://192.170.0.2:443</code></pre>

        <pre><code class="language-bash hljs">curl http://172.16.0.22:31653</code></pre>

        <pre><code class="language-bash hljs">curl -k https://172.16.0.22:30552</code></pre>

        <p>The response to a successful request is similar to:</p>

        <pre><code class="language-bash hljs">&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href="http://nginx.org/"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href="http://nginx.com/"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;</code></pre>

        <p>On master node and other worker nodes, try to access your application using the cluster IP, external IP and node IP address of <code>nginx-server-service</code>:</p>

        <pre><code class="language-bash hljs">curl http://10.107.131.88:80</code></pre>

        <pre><code class="language-bash hljs">curl -k https://10.107.131.88:443</code></pre>

        <pre><code class="language-bash hljs">curl http://192.170.0.2:80</code></pre>

        <pre><code class="language-bash hljs">curl -k https://192.170.0.2:443</code></pre>

        <p>The curl response when try to test tcp connection on cluster IP external IP address of <code>nginx-server-service</code> will fail since the firewall is blocking the request:</p>
        
        <pre><code class="language-bash hljs">curl: (7) Failed to connect to 10.107.131.88 port 80 after 1 ms: Couldn't connect to server</code></pre>

        <pre><code class="language-bash hljs">curl: (7) Failed to connect to 10.107.131.88 port 443 after 1 ms: Couldn't connect to server</code></pre>

        <pre><code class="language-bash hljs">curl: (7) Failed to connect to 192.170.0.2 port 80 after 1 ms: Couldn't connect to server</code></pre>

        <pre><code class="language-bash hljs">curl: (7) Failed to connect to 192.170.0.2 port 443 after 1 ms: Couldn't connect to server</code></pre>

        <p>But otherwise the curl response when try to test tcp connection on the worker node IP address where <code>ingress-nginx-controller</code> pod is running on returns:</p>

        <pre><code class="language-bash hljs">&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href="http://nginx.org/"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href="http://nginx.com/"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;</code></pre>

        <p>On worker node where the <code>nginx-server</code> pod is running on, use <code>dmesg</code> command to view the denied packets:</p>
        
        <pre><code class="language-bash hljs">dmesg | grep -i REJECT</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">[29039.265405] filter_FWD_public_REJECT: IN=enp0s3 OUT=cali9aba158b53d MAC=08:00:27:10:a0:5f:08:00:27:fe:60:d3:08:00 SRC=172.16.0.21 DST=192.168.90.11 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=15179 DF PROTO=TCP SPT=35290 DPT=80 WINDOW=64240 RES=0x00 SYN URGP=0 MARK=0x10000 
[29039.291039] filter_FWD_public_REJECT: IN=enp0s3 OUT=cali9aba158b53d MAC=08:00:27:10:a0:5f:08:00:27:fe:60:d3:08:00 SRC=172.16.0.21 DST=192.168.90.11 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=21020 DF PROTO=TCP SPT=4065 DPT=443 WINDOW=64240 RES=0x00 SYN URGP=0 MARK=0x10000 
[29039.302519] filter_FWD_public_REJECT: IN=enp0s3 OUT=cali9aba158b53d MAC=08:00:27:10:a0:5f:08:00:27:fe:60:d3:08:00 SRC=172.16.0.21 DST=192.168.90.11 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=26229 DF PROTO=TCP SPT=7973 DPT=80 WINDOW=64240 RES=0x00 SYN URGP=0 MARK=0x10000 
[29039.310790] filter_FWD_public_REJECT: IN=enp0s3 OUT=cali9aba158b53d MAC=08:00:27:10:a0:5f:08:00:27:fe:60:d3:08:00 SRC=172.16.0.21 DST=192.168.90.11 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=58856 DF PROTO=TCP SPT=9648 DPT=443 WINDOW=64240 RES=0x00 SYN URGP=0 MARK=0x10000</code></pre>       
        <p>Create an new zone using calico network interface name (ie. <code>cali9aba158b53d</code>). <code>cali9aba158b53d</code> is the zone for <code>nginx-server</code> calico pod network.</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --new-zone cali9aba158b53d</code></pre>

        <p>Then assign <code>nginx-server</code> calico pod network interface <code>cali9aba158b53d</code> to this zone.</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --change-interface=cali9aba158b53d --zone=cali9aba158b53d</code></pre>

        <p>Next, create a policy:</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --new-policy cali9aba158b53dpol</code></pre>

        <p>Set the policies to traffic forwarded from lan interface <code>enp0s3</code> to calico network interface <code>cali7368eccef27</code>:</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --policy cali9aba158b53dpol --add-ingress-zone public
sudo firewall-cmd --permanent --policy cali9aba158b53dpol --add-egress-zone cali9aba158b53d</code></pre>        
        
        <p>To finish up the policy settings, set it to accept new connections by default:</p>
        
        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --policy cali9aba158b53dpol --set-target ACCEPT</code></pre>

        <p>Then load the new zone and policy into the active runtime state:</p>
        
        <pre><code class="language-bash hljs">sudo firewall-cmd --reload</code></pre>

        <p><b>On master node and other worker nodes</b>, test to run the <code>curl</code> command again:</p>

        <pre><code class="language-bash hljs">curl http://10.107.131.88:80</code></pre>

        <pre><code class="language-bash hljs">curl -k https://10.107.131.88:443</code></pre>

        <pre><code class="language-bash hljs">curl http://192.170.0.2:80</code></pre>

        <pre><code class="language-bash hljs">curl -k https://192.170.0.2:443</code></pre>

        <pre><code class="language-bash hljs">curl http://172.16.0.22:31653</code></pre>

        <pre><code class="language-bash hljs">curl -k https://172.16.0.22:30552</code></pre>

        <p>The request should be successful now.</p>

        <pre><code class="language-bash hljs">&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href="http://nginx.org/"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href="http://nginx.com/"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;</code></pre>
        
        <p>Generate a self signed rsa key and certificate that later the ingress resource can use for TLS:</p>

        <pre><code class="language-bash hljs">openssl req -x509 -nodes -days 365 -newkey rsa:4096 -keyout ~/nginx/ingress.key -out ~/nginx/ingress.crt</code></pre>
        
        <p>Fill out the certificate form:</p>

        <pre><code class="language-bash hljs">..+.........+...+.............+.........+..+...+..........+........+....+...+........+...+.+...+...+...+.....+.+.....+....+...........+.+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*.+...+..+.+...........+.........+.+..............+....+..+.+..+......+.+........+.+.....+.......+.....+.........+....+.....+....+.....................+..+....+......+..+...+..........+...+...........+...+..........+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*.........+.....+....+.....+................+......+..............+................+.....+...+......+....+...........+..........+.........+...+..................+............+........+.......+...............+...+.....+............+.........+.............+......+......+.....+...+.+............+.........+...+.....+......+..........+..+.........+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
....+.+...+......+...........+.+...+......+...+..................+..............+.+..+...+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*.................+....+...+......+..+....+......+.....+...+......+...+............+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*........+......+........+.......+...+......+.....+......+........................+....+......+...........+...+...............+......+....+.......................+.......+..+...................+..+..........+..+.+.....+.......+.........+..............+...............+.....................+....+...........+...+...+...............+...+..........+......+......+.........+........+......+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
-----
You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter '.', the field will be left blank.
-----
Country Name (2 letter code) [XX]:ID
State or Province Name (full name) []:Jakarta
Locality Name (eg, city) [Default City]:Jakarta
Organization Name (eg, company) [Default Company Ltd]:hemimorphite
Organizational Unit Name (eg, section) []:
Common Name (eg, your name or your server's hostname) []:hemimorphite.app
Email Address []:yangsamuel91@gmail.com</code></pre>
        
        <p>Encode the key and certificate data in base64</p>

        <pre><code class="language-bash hljs">cat ~/nginx/ingress.crt | base64</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">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</code></pre>

        <pre><code class="language-bash hljs">cat ~/nginx/ingress.key | base64</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">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</code></pre>

        <p>Create a kubernetes yaml tls secret file for creating a secret named <code>ingress-secret</code></p>

        <pre><code class="language-bash hljs">apiVersion: v1
data: 
  tls.crt: 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
  tls.key: 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
kind: Secret
metadata: 
  name: ingress-secret
  namespace: default
type: kubernetes.io/tls</code></pre>
        
        <p>Apply the tls secret manifest file:</p>

        <pre><code class="language-bash hljs">kubectl create -f ingress-secret.yaml</code></pre>        

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">secret/ingress-secret created</code></pre>

        <p>Create a ingress resource that contains rules with a rewrite annotation for redirecting the request path <code>/something</code> to <code>nginx-server-service</code> on a TCP port of 80 and 443:</code></p>

        <pre><code class="language-bash hljs">apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx-server-ingress
  namespace: default
  annotations:
    nginx.ingress.kubernetes.io/use-regex: "true"
    nginx.ingress.kubernetes.io/rewrite-target: /$2
spec:
  ingressClassName: nginx
  tls:
  - secretName: ingress-secret
  rules:
  - http:
      paths:
      - path: /something(/|$)(.*)
        pathType: ImplementationSpecific
        backend:
          service:
            name: nginx-server-service
            port:
              number: 80</code></pre>

        <p><code>$2</code> means the second regex group of <code>/something(/|$)(.*)</code>, that is <code>(.*)</code>. Any characters captured by <code>(.*)</code> will be assigned to <code>$2</code> in the rewrite-target annotation (eg. <code>/something</code> rewrites to <code>/</code>, <code>/something/new</code> rewrites to <code>/new</code>).</p>

        <p>Apply the ingress resource manifest file:</p>

        <pre><code class="language-bash hljs">kubectl create -f nginx-server-ingress.yaml</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">ingress.networking.k8s.io/nginx-server-ingress created</code></pre>

        <p>To list all ingress resources, use the command:</p>

        <pre><code class="language-bash hljs">kubectl get ingresses</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">NAME                   CLASS   HOSTS   ADDRESS       PORTS     AGE
nginx-server-ingress   nginx   *       172.16.0.22   80, 443   5m51s</code></pre>

        <p>On master node and worker nodes, try to access your application using the cluster IP and the external IP address of <code>ingress-nginx-controller</code> service and the worker node IP address where <code>ingress-nginx-controller</code> pod is running on:</p>

        <pre><code class="language-bash hljs">curl http://10.107.65.165:80/something</code></pre>

        <pre><code class="language-bash hljs">curl -k https://10.107.65.165:443/something</code></pre>

        <pre><code class="language-bash hljs">curl http://192.170.0.1:80/something</code></pre>

        <pre><code class="language-bash hljs">curl -k https://192.170.0.1:443/something</code></pre>

        <pre><code class="language-bash hljs">curl http://172.16.0.22:30609/something</code></pre>

        <pre><code class="language-bash hljs">curl -k https://172.16.0.22:30996/something</code></pre>

        <p>The curl response will return "502 Bad Gateway" since the firewall is blocking the curl request.</p>

        <pre><code class="language-bash hljs">&lt;html&gt;
&lt;head&gt;&lt;title&gt;502 Bad Gateway&lt;/title&gt;&lt;/head&gt;
&lt;body&gt;
&lt;center&gt;&lt;h1&gt;502 Bad Gateway&lt;/h1&gt;&lt;/center&gt;
&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;
&lt;/body&gt;
&lt;/html&gt;</code></pre>

        <p>On worker node, to view the denied packets, use <code>dmesg</code> command:</p>
        
        <pre><code class="language-bash hljs">dmesg | grep -i REJECT</code></pre>

        <p>The response should look similar to this:</p>

        <pre><code class="language-bash hljs">[27430.987420] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=6053 DF PROTO=TCP SPT=49274 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27430.988258] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=46895 DF PROTO=TCP SPT=49280 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27430.992201] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=21590 DF PROTO=TCP SPT=49284 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27431.041655] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=50033 DF PROTO=TCP SPT=49290 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27431.042189] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=20178 DF PROTO=TCP SPT=49298 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27431.043061] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=23163 DF PROTO=TCP SPT=49310 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27431.052636] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=23192 DF PROTO=TCP SPT=49320 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27431.979329] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=23193 DF PROTO=TCP SPT=49320 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27431.980157] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=55915 DF PROTO=TCP SPT=49330 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27432.924389] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=55916 DF PROTO=TCP SPT=49330 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27432.925277] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=15452 DF PROTO=TCP SPT=49342 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27433.869141] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=15453 DF PROTO=TCP SPT=49342 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27433.961525] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=43556 DF PROTO=TCP SPT=49348 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27434.932766] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=43557 DF PROTO=TCP SPT=49348 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27434.934358] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=12923 DF PROTO=TCP SPT=49354 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27435.877665] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=12924 DF PROTO=TCP SPT=49354 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27435.878217] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=5169 DF PROTO=TCP SPT=49364 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27436.823613] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=5170 DF PROTO=TCP SPT=49364 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27436.922655] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=9553 DF PROTO=TCP SPT=49380 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27437.886286] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=9554 DF PROTO=TCP SPT=49380 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27437.887678] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=47980 DF PROTO=TCP SPT=49390 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27438.831341] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=47981 DF PROTO=TCP SPT=49390 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27438.831926] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=27766 DF PROTO=TCP SPT=49398 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000 
[27439.776698] filter_FWD_cali7368eccef27_REJECT: IN=cali7368eccef27 OUT=cali9aba158b53d MAC=ee:ee:ee:ee:ee:ee:26:4a:27:49:ea:e9:08:00 SRC=192.168.90.13 DST=192.168.90.12 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=27767 DF PROTO=TCP SPT=49398 DPT=80 WINDOW=64860 RES=0x00 SYN URGP=0 MARK=0x10000</code></pre>        
        <p>Create two new zones using calico network interface names (ie. <code>cali7368eccef27</code> and <code>cali9aba158b53d</code>). <code>cali7368eccef27</code> is the zone for <code>ingress-nginx-controller</code> calico pod network and <code>cali9aba158b53d</code> is the zone for <code>nginx-server</code> calico pod network.</p>
        
        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --new-zone cali7368eccef27
sudo firewall-cmd --permanent --new-zone cali9aba158b53d</code></pre>

        <p>Then assign <code>cali7368eccef27</code> zone with <code>cali7368eccef27</code> network interface and assign <code>cali9aba158b53d</code> zone with <code>cali9aba158b53d</code> network interface.</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --change-interface=cali7368eccef27 --zone=cali7368eccef27
sudo firewall-cmd --permanent --change-interface=cali9aba158b53d --zone=cali9aba158b53d</code></pre>

        <p>Create a policy:</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --new-policy cali736cali9abpol</code></pre>

        <p>Set the policies to traffic forwarded from calico network interface <code>cali7368eccef27</code> to calico network interface <code>cali9aba158b53d</code>:</p>

        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --policy cali736cali9abpol --add-ingress-zone cali7368eccef27
sudo firewall-cmd --permanent --policy cali736cali9abpol --add-egress-zone cali9aba158b53d</code></pre>        
        <p>To finish up the policy settings, set it to accept new connections by default:</p>
        
        <pre><code class="language-bash hljs">sudo firewall-cmd --permanent --policy cali736cali9abpol --set-target ACCEPT</code></pre>

        <p>Then load the new zone and policy into the active runtime state:</p>
        
        <pre><code class="language-bash hljs">sudo firewall-cmd --reload</code></pre>

        <p><b>On master node and worker nodes</b>, test to run the <code>curl</code> command again:</p>

        <pre><code class="language-bash hljs">curl http://10.107.65.165:80/something</code></pre>

        <pre><code class="language-bash hljs">curl -k https://10.107.65.165:443/something</code></pre>

        <pre><code class="language-bash hljs">curl http://192.170.0.1:80/something</code></pre>

        <pre><code class="language-bash hljs">curl -k https://192.170.0.1:443/something</code></pre>

        <pre><code class="language-bash hljs">curl http://172.16.0.22:30609/something</code></pre>

        <pre><code class="language-bash hljs">curl -k https://172.16.0.22:30996/something</code></pre>

        <p>The request should be successful now.</p>

        <pre><code class="language-bash hljs">&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href="http://nginx.org/"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href="http://nginx.com/"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;</code></pre>
        
        <p>Create a port forward from the remote port on the <code>ingress-nginx-controller</code> service to the local port on local address 114.122.37.100 on your machine:</p>

        <pre><code class="language-bash hljs">kubectl port-forward --address 114.122.37.100 service/ingress-nginx-controller -n ingress-nginx 80:80 443:443</code></pre>

        <p>If the port forwarding is successful, the response should look similar to this:</p>

        <pre><code class="language-bash hljs">Forwarding from 114.122.37.100:80 -> 80
Forwarding from 114.122.37.100:443 -> 443</code></pre>

        <p>Now, you can access your application on the local clients LAN by</p>

        <pre><code class="language-bash hljs">curl http://114.122.37.100/something
curl https://114.122.37.100/something</code></pre>        
        
        <p>Opening a web browser and navigating to <code>http://114.122.37.100/something</code> or <code>https://114.122.37.100/something</code>.</p>

        <figure class="post-figure">
            <img src="/assets/images/fedoraserver38-34.jpg" alt="Fedora Server installation">
        </figure>        
    </article>

    <div class="post-tags">
	<div class="title">Tags</div>
	<ul class="tags">
		
		<li><a href="https://hemimorphite.github.io/zh/tag/virtualbox" class="tag">virtualbox</a></li>
		
		<li><a href="https://hemimorphite.github.io/zh/tag/fedora" class="tag">fedora</a></li>
		
		<li><a href="https://hemimorphite.github.io/zh/tag/linux" class="tag">linux</a></li>
		
		<li><a href="https://hemimorphite.github.io/zh/tag/kubernetes" class="tag">kubernetes</a></li>
		
		<li><a href="https://hemimorphite.github.io/zh/tag/kubeadm" class="tag">kubeadm</a></li>
		
		<li><a href="https://hemimorphite.github.io/zh/tag/containerd" class="tag">containerd</a></li>
		
		<li><a href="https://hemimorphite.github.io/zh/tag/project-calico" class="tag">project calico</a></li>
		
		<li><a href="https://hemimorphite.github.io/zh/tag/metallb" class="tag">metallb</a></li>
		
		<li><a href="https://hemimorphite.github.io/zh/tag/nginx" class="tag">nginx</a></li>
		
		<li><a href="https://hemimorphite.github.io/zh/tag/ingress-controller" class="tag">ingress controller</a></li>
		
		<li><a href="https://hemimorphite.github.io/zh/tag/firewalld" class="tag">firewalld</a></li>
		
	</ul>
</div>


    <div class="post-share">
	
    <div class="title">Share this post</div>
    <ul class="rounded-social-buttons">
        <li><a href="https://www.facebook.com/sharer/sharer.php?u=https://hemimorphite.github.io/zh/2023/07/27/create-a-kubernetes-cluster-with-kubeadm-containerd-calico-metallb-and-nginx-ingress-controller-on-bare-metal-machine-fedora-38/" class="social-button facebook"><i class="fab fa-facebook-f"></i></a></li>
        <li><a href="http://twitter.com/share?text=Hey+guys%2c+check+this+out!&amp;url=https://hemimorphite.github.io/zh/2023/07/27/create-a-kubernetes-cluster-with-kubeadm-containerd-calico-metallb-and-nginx-ingress-controller-on-bare-metal-machine-fedora-38/" class="social-button twitter"><i class="fab fa-twitter"></i></a></li>
        <li><a href="https://www.linkedin.com/sharing/share-offsite/?url=https://hemimorphite.github.io/zh/2023/07/27/create-a-kubernetes-cluster-with-kubeadm-containerd-calico-metallb-and-nginx-ingress-controller-on-bare-metal-machine-fedora-38/" class="social-button linkedin"><i class="fab fa-linkedin"></i></a></li>
    </ul>
</div>
</div>


				<div class="container">
					<div class="row">
						<div class="col-md-6">
							<script type="text/javascript">
								atOptions = {
									'key' : '24dfaf1e2babad270db4c0eae11415cc',
									'format' : 'iframe',
									'height' : 60,
									'width' : 468,
									'params' : {}
								};
							</script>
							<script type="text/javascript" src="//www.topcreativeformat.com/24dfaf1e2babad270db4c0eae11415cc/invoke.js"></script>
						</div>
						<div class="col-md-6">
							<script type="text/javascript">
								atOptions = {
									'key' : '24dfaf1e2babad270db4c0eae11415cc',
									'format' : 'iframe',
									'height' : 60,
									'width' : 468,
									'params' : {}
								};
							</script>
							<script type="text/javascript" src="//www.topcreativeformat.com/24dfaf1e2babad270db4c0eae11415cc/invoke.js"></script>
						</div>
						<div class="col-md-6">
							<script type="text/javascript">
								atOptions = {
									'key' : '08760f2487e830ed5039902de4007bec',
									'format' : 'iframe',
									'height' : 250,
									'width' : 300,
									'params' : {}
								};
							</script>
							<script type="text/javascript" src="//www.topcreativeformat.com/08760f2487e830ed5039902de4007bec/invoke.js"></script>
						</div>
						<div class="col-md-6">
							<script type="text/javascript">
								atOptions = {
									'key' : '08760f2487e830ed5039902de4007bec',
									'format' : 'iframe',
									'height' : 250,
									'width' : 300,
									'params' : {}
								};
							</script>
							<script type="text/javascript" src="//www.topcreativeformat.com/08760f2487e830ed5039902de4007bec/invoke.js"></script>
						</div>
					</div>
				</div>
				<div id="disqus_thread"></div>
			</div>

			<div class="col-12" style="border-bottom:0.1rem solid #eee;margin-top:2rem;margin-bottom:5rem;"></div>
		</div>
	</div>
</section>

<section>
	<div class="container">
		<div class="row">
			<div class="col-12">
				<div class="section-heading d-flex align-items-center">  
					<h2>Related Posts</h2>
					<a href="https://hemimorphite.github.io/zh/" type="button" class="btn btn-all ms-auto">All posts <i class="fa-solid fa-arrow-right"></i></a>
				</div>
				<div class="related-posts">
					<div class="row">
						
						
						
						
						
						
						
						
						<div class="col-md-6 col-lg-4">
							<div class="blog-post">
								<div class="blog-thumb">
									<a href="https://hemimorphite.github.io/zh/2023/06/30/virtualbox-guest-additions-kernel-headers-not-found-for-target-kernel/"><img src="https://hemimorphite.github.io/assets/images/vguest.jpg" alt=""></a>
								</div>
								<div class="blog-eyebrow">Troubleshooting</div>
								<h4 class="blog-title"><a href="https://hemimorphite.github.io/zh/2023/06/30/virtualbox-guest-additions-kernel-headers-not-found-for-target-kernel/">Virtualbox Guest Additions: Kernel headers not found for target kernel (Fedora)</a></h4>
							</div>
						</div>
						
						
						
						
						<div class="col-md-6 col-lg-4">
							<div class="blog-post">
								<div class="blog-thumb">
									<a href="https://hemimorphite.github.io/zh/2023/06/27/install-virtualbox-on-ubuntu-2204-by-using-the-deb-file/"><img src="https://hemimorphite.github.io/assets/images/ubuntuvirtualbox.jpg" alt=""></a>
								</div>
								<div class="blog-eyebrow">Tutorial</div>
								<h4 class="blog-title"><a href="https://hemimorphite.github.io/zh/2023/06/27/install-virtualbox-on-ubuntu-2204-by-using-the-deb-file/">Install Virtualbox on Ubuntu 22.04 with UEFI Secure Boot Enable by Using the Deb File</a></h4>
							</div>
						</div>
						
						
						
						
						<div class="col-md-6 col-lg-4">
							<div class="blog-post">
								<div class="blog-thumb">
									<a href="https://hemimorphite.github.io/zh/2023/06/21/enable-nested-vtx-amdv-on-virtualbox/"><img src="https://hemimorphite.github.io/assets/images/virtualbox.jpg" alt=""></a>
								</div>
								<div class="blog-eyebrow">Tutorial</div>
								<h4 class="blog-title"><a href="https://hemimorphite.github.io/zh/2023/06/21/enable-nested-vtx-amdv-on-virtualbox/">Enable Nested VT-x/AMD-V on Virtualbox</a></h4>
							</div>
						</div>
						
						
						
					</div>
				</div>
			</div>
		</div>
	</div>
</section>

		<section>
			<div class="container">
				<div class="row">
					<div class="col-md-6">
						<script async="async" data-cfasync="false" src="//pl23622823.highrevenuenetwork.com/82849ac901fcdf553b99a113d233ce38/invoke.js"></script>
						<div id="container-82849ac901fcdf553b99a113d233ce38"></div>
					</div>
					<div class="col-md-6">
						<script type="text/javascript">
							atOptions = {
								'key' : '08760f2487e830ed5039902de4007bec',
								'format' : 'iframe',
								'height' : 250,
								'width' : 300,
								'params' : {}
							};
						</script>
						<script type="text/javascript" src="//www.topcreativeformat.com/08760f2487e830ed5039902de4007bec/invoke.js"></script>
					</div>
				</div>
			</div>
		</section>

	    <footer>
			<div class="container">
				<div class="row">
					<div class="col-lg-12">
						<div class="copyright-text">
						<p>&copy; 2025 Copyright hemimorphite. All Rights Reserved</p>
						</div>
					</div>
				</div>
			</div>
		</footer>

		<script src="https://hemimorphite.github.io/assets/vendor/jquery/jquery.min.js"></script>
	    <script src="https://hemimorphite.github.io/assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
	    <script src="https://hemimorphite.github.io/assets/vendor/highlight.js/js/highlight.min.js"></script>
		<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
		<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>
	    <script>
			//<![CDATA[
			/* highlight.js | https://unpkg.com/highlightjs-badge@0.1.8/highlightjs-badge.min.js */
			"use strict";!function(e,o){"object"==typeof module&&"object"==typeof module.exports?module.exports=e.document?o(e,!0):function(e){if(!e.document)throw new Error("A window with a document is required");return o(e)}:o(e)}("undefined"!=typeof window?window:this,function(y,e){if("boolean"!=typeof o)var o=!1;function t(e){var o,m={templateSelector:"#CodeBadgeTemplate",contentSelector:"body",loadDelay:0,copyIconClass:"fa fa-copy",copyIconContent:"",checkIconClass:"fa fa-check text-success",checkIconContent:"",onBeforeCodeCopied:null};function t(){m.loadDelay?setTimeout(n,loadDelay):n()}function n(){if(!document.querySelector(m.templateSelector)){var e=document.createElement("div");e.innerHTML=function(){for(var e=["<style>","@media print {","   .code-badge { display: none; }","}","    .code-badge-pre {","        position: relative;","    }","    .code-badge {","        display: flex;","        flex-direction: row;","        white-space: normal;","        background: transparent;","        background: #fff;","        color: #333;","        font-size: 0.875em;","        opacity: 0.5;","        transition: opacity linear 0.5s;","        border-radius: 0 0 0 7px;","        padding: 5px 8px 5px 8px;","        position: absolute;","        right: 0;","        top: 0;","    }","    .code-badge.active {","        opacity: 0.8;","    }","","    .code-badge:hover {","        opacity: .95;","    }","","    .code-badge a,","    .code-badge a:hover {","        text-decoration: none;","    }","","    .code-badge-language {","        margin-right: 10px;","        font-weight: 600;","        color: goldenrod;","    }","    .code-badge-copy-icon {","        font-size: 1.2em;","        cursor: pointer;","        padding: 0 7px;","    }","    .fa.text-success:{ color: limegreen !important }","</style>",'<div id="CodeBadgeTemplate" style="display:none">','    <div class="code-badge">','        <div class="code-badge-language" ></div>','        <div  title="Copy to clipboard">','            <i class=" code-badge-copy-icon"></i></i></a>',"        </div>","     </div>","</div>"],o="",t=0;t<e.length;t++)o+=e[t]+"\n";return o}();var o=e.querySelector("style"),t=e.querySelector(m.templateSelector);document.body.appendChild(o),document.body.appendChild(t)}for(var n=document.querySelector(m.templateSelector).innerHTML,c=document.querySelectorAll("pre>code.hljs"),a=0;a<c.length;a++){var r=c[a];if(!r.querySelector(".code-badge")){for(var d="",l=0;l<r.classList.length;l++){var i=r.classList[l];if("language-"===i.substr(0,9)){d=r.classList[l].replace("language-","");break}if("lang-"===i.substr(0,5)){d=r.classList[l].replace("lang-","");break}if(!d)for(var s=0;s<r.classList.length;s++)if("hljs"!=r.classList[s]){d=r.classList[s];break}}"ps"==(d=d?d.toLowerCase():"text")?d="powershell":"cs"==d?d="csharp":"js"==d?d="javascript":"ts"==d?d="typescript":"fox"==d&&(d="foxpro");var p=n.replace("",d).replace("",m.copyIconClass).trim(),u=document.createElement("div");u.innerHTML=p,u=u.querySelector(".code-badge");var g=r.parentElement;g.classList.add("code-badge-pre"),m.copyIconContent&&(u.querySelector(".code-badge-copy-icon").innerText=m.copyIconContent),g.insertBefore(u,r)}}document.querySelector(m.contentSelector).addEventListener("click",function(e){return e.srcElement.classList.contains("code-badge-copy-icon")&&(e.preventDefault(),e.cancelBubble=!0,function(e){var o=e.srcElement.parentElement.parentElement.parentElement,t=o.querySelector("pre>code"),n=t.textContent||t.innerText;m.onBeforeCodeCopied&&(n=m.onBeforeCodeCopied(n,t));var c=document.createElement("textarea");c.value=n.trim(),document.body.appendChild(c),c.style.display="block",y.document.documentMode?c.setSelectionRange(0,c.value.length):c.select();document.execCommand("copy"),document.body.removeChild(c),function(e){var o=m.copyIconClass.split(" "),t=m.checkIconClass.split(" "),n=e.querySelector(".code-badge-copy-icon");n.innerText=m.checkIconContent;for(var c=0;c<o.length;c++)n.classList.remove(o[c]);for(c=0;c<t.length;c++)n.classList.add(t[c]);setTimeout(function(){n.innerText=m.copyIconContent;for(var e=0;e<t.length;e++)n.classList.remove(t[e]);for(e=0;e<o.length;e++)n.classList.add(o[e])},2e3)}(o)}(e)),!1})}o=e,Object.assign(m,o),"loading"==document.readyState?document.addEventListener("DOMContentLoaded",t):t()}y.highlightJsBadge=t,y.module&&y.module.exports&&(y.module.exports.highlightJsBadge=t),o&&t()});   
			//]]>
		</script>

	    <!-- Additional Scripts -->
	    <script src="https://hemimorphite.github.io/assets/js/custom.js"></script>
	    <script>
	    	let url = new URL("https://hemimorphite.github.io/zh/2023/07/27/create-a-kubernetes-cluster-with-kubeadm-containerd-calico-metallb-and-nginx-ingress-controller-on-bare-metal-machine-fedora-38/");
			let paths = url.pathname.split('/').slice(1);

		    /**
		    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
		    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
		    if(typeof paths[3] !== 'undefined') {
		    	var disqus_config = function () {
				    this.page.url = "https://hemimorphite.github.io/zh/2023/07/27/create-a-kubernetes-cluster-with-kubeadm-containerd-calico-metallb-and-nginx-ingress-controller-on-bare-metal-machine-fedora-38/";  // Replace PAGE_URL with your page's canonical URL variable
				    this.page.identifier = btoa(paths[3]); // Replace PAGE_IDENTIFIER with your page's unique identifier variable
			    };
			    
			    (function() { // DON'T EDIT BELOW THIS LINE
			    var d = document, s = d.createElement('script');
			    s.src = 'https://hemimorphite-github-io.disqus.com/embed.js';
			    s.setAttribute('data-timestamp', +new Date());
			    (d.head || d.body).appendChild(s);
			    })();	
		    }
		    
		</script>
	    <script id="dsq-count-scr" src="https://hemimorphite-github-io.disqus.com/count.js" async></script>
		
  	</body>
</html>